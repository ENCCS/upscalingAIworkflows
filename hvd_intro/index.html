

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Intoduction to Horovod &mdash; Upscaling AI workflows  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_lesson.css?v=e9df6548" />
      <link rel="stylesheet" type="text/css" href="../_static/term_role_formatting.css?v=4194e21c" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_rtd_theme_ext_color_contrast.css?v=8e8ea19f" />
      <link rel="stylesheet" type="text/css" href="../_static/overrides.css?v=0572569b" />

  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=187304be"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
      <script src="../_static/minipres.js?v=a0d29692"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script data-domain="enccs.github.io/upscalingAIworkflows" defer="defer" src="https://plausible.io/js/script.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Training Neural Networks using Containers" href="../train_contain/" />
    <link rel="prev" title="Distributed training in TensorFlow" href="../tf_mltgpus/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../" class="icon icon-home">
            Upscaling AI workflows
              <img src="../_static/ENCCS.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../setup/">Access to Vega</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">The lesson</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro-container/">Introduction to Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_docker/">Introduction to Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mang_contain/">Cleaning Up Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../create_contain/">Creating your own container images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compx_contain/">Creating More Complex Container Images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../singlrty_start/">What is Singularity?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../work_contain/">Working with Singularity containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_contain/">Building Singularity images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tf_intro/">TensorFlow on a single GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tf_mltgpus/">Distributed training in TensorFlow</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Intoduction to Horovod</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#why-horovod">Why Horovod</a></li>
<li class="toctree-l2"><a class="reference internal" href="#main-concept">Main concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-use-horovod">How to use Horovod</a></li>
<li class="toctree-l2"><a class="reference internal" href="#base-model">Base model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-with-model-fit">Training with <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#analysis-of-performance">Analysis of Performance</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../train_contain/">Training Neural Networks using Containers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Optional</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../containers/content/mpi_contain/">Running MPI parallel jobs using Singularity containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../containers/content/rep_gran/">Containers in research workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../containers/content/pwd_exmps/">PWD exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../upscalingAIcontainer/content/namespc-cgroup/">Namespaces and cgroups</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick-reference/">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/">Instructor’s guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../">Upscaling AI workflows</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Intoduction to Horovod</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/upscalingAIworkflows/blob/main/content/hvd_intro.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="intoduction-to-horovod">
<span id="hvd-intro"></span><h1>Intoduction to Horovod<a class="headerlink" href="#intoduction-to-horovod" title="Link to this heading"></a></h1>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="https://horovod.readthedocs.io/en/stable/_static/logo.png"><img alt="https://horovod.readthedocs.io/en/stable/_static/logo.png" src="https://horovod.readthedocs.io/en/stable/_static/logo.png" style="width: 40%;" />
</a>
<figcaption>
<p><span class="caption-text"><a class="reference external" href="https://horovod.ai">(Image Source)</a></span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="why-horovod">
<h2>Why Horovod<a class="headerlink" href="#why-horovod" title="Link to this heading"></a></h2>
<p>Horovod was developed at Uber with the primary motivation of making it easy to
take a single-GPU training script and successfully scale it to train across many
GPUs in parallel. This has two aspects:</p>
<ul class="simple">
<li><p>How much modification does one have to make to a program to make it distributed,
and how easy is it to run it?</p></li>
<li><p>How much faster would it run in distributed mode?</p></li>
</ul>
<p>What researchers at Uber discovered was that the MPI model to be much more straightforward
and require far less code changes than previous solutions such as Distributed TensorFlow with
parameter servers. Once a training script has been written for scale with Horovod, it can run
on a single-GPU, multiple-GPUs, or even multiple hosts without any further code changes.</p>
<p>In addition to being easy to use, Horovod is fast. Below is a chart representing the benchmark
that was done on 128 servers with 4 Pascal GPUs each connected by RoCE-capable 25 Gbit/s network:</p>
<img alt="scaling" src="https://user-images.githubusercontent.com/16640218/38965607-bf5c46ca-4332-11e8-895a-b9c137e86013.png" />
<p>Horovod achieves 90% scaling efficiency for both Inception V3 and ResNet-101, and
68% scaling efficiency for VGG-16. While installing MPI and NCCL itself may seem like an extra hassle,
it only needs to be done once by the team dealing with infrastructure, while everyone else in the company
who builds the models can enjoy the simplicity of training them at scale. Plus, in modern clusters where
GPUs are available, MPI and NCCL are readily installed. Installation of Horovod is not as difficult.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1802.05799.pdf">Alex Sergeev and Mike Del Balso</a>,
the researchers behind the development of Horovod at Uber,
published an excellent history and review of <a class="reference external" href="https://eng.uber.com/horovod/">the Hovorod</a>.
Here are some points they mentioned in their article:</p>
<ul class="simple">
<li><p>The first try for distributed training was based on TensorFlow distributed method.
However, they experienced two major difficulties: 1. Difficulty of following instructions
given by TensorFlow. In particular, they found the newly introduced concepts by TensorFlow
for distributed training causes <em>hard-to-diagnose bugs that slowed training</em>.</p></li>
<li><p>The second issue dealt with the challenge of computing at Uber’s scale.
After running a few benchmarks, they found that they could not get the standard
distributed TensorFlow to scale as well as our services required.
For example, about half of their resources was lost due to scaling inefficiencies
when training on 128 GPUs.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/image4-1-768x330.png"><img alt="https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/image4-1-768x330.png" src="https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/image4-1-768x330.png" style="width: 90%;" />
</a>
</figure>
<ul class="simple">
<li><p>An article by Facebook researchers entitled  “<a class="reference external" href="https://scontent-arn2-1.xx.fbcdn.net/v/t39.8562-6/240818965_455586748763065_8609026679315857149_n.pdf?_nc_cat=111&amp;ccb=1-5&amp;_nc_sid=ad8a9d&amp;_nc_ohc=CtM02FZ33KwAX8zcuRy&amp;_nc_ht=scontent-arn2-1.xx&amp;oh=00_AT_dczJ90lEGzFc3ugwhrl3vI3fnIvBVhWsxpQrWaamVTQ&amp;oe=62647A23">Accurate, Large Minibatch SGD:
Training ImageNet in 1 Hour,</a>
demonstrating their training of a ResNet-50 network in one hour on 256
GPUs by combining principles of data parallelism peaked their interests.</p></li>
<li><p>A paper published by Baidu researchers in early 2017, “Bringing HPC Techniques to
Deep Learning,” evangelizing a different algorithm for averaging gradients
and communicating those gradients to all nodes, called <strong>ring-allreduce</strong>.
The algorithm was based on the approach introduced in the 2009 paper
“<a class="reference external" href="http://www.cs.fsu.edu/~xyuan/paper/09jpdc.pdf">Bandwidth Optimal All-reduce Algorithms for Clusters of Workstations</a>”
by Patarasuk and Yuan.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/figure4-7564694e76d08e091ce453f681515e59.png"><img alt="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/figure4-7564694e76d08e091ce453f681515e59.png" src="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/figure4-7564694e76d08e091ce453f681515e59.png" style="width: 80%;" />
</a>
</figure>
<p><a class="reference external" href="https://www.oreilly.com/content/distributed-tensorflow/">(Image Source)</a></p>
<ul class="simple">
<li><p>The realization that a ring-allreduce approach can improve both usability
and performance motivated them to work on our own implementation to address
Uber’s TensorFlow needs.</p></li>
<li><p>Horovod (Khorovod) is named after a traditional Russian folk dance in which
performers dance with linked arms in a circle, much like how distributed
TensorFlow processes use Horovod to communicate with each other.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="https://cdni.russiatoday.com/rbthmedia/images/all/2016/11/10/khorovod_rian_02920393_b.jpg"><img alt="https://cdni.russiatoday.com/rbthmedia/images/all/2016/11/10/khorovod_rian_02920393_b.jpg" src="https://cdni.russiatoday.com/rbthmedia/images/all/2016/11/10/khorovod_rian_02920393_b.jpg" style="width: 75%;" />
</a>
</figure>
<p><a class="reference external" href="https://www.rbth.com/arts/2016/12/07/8-facts-about-the-khorovod-russias-oldest-dance_654295">(Image Source)</a></p>
<ul class="simple">
<li><p>They replaced the Baidu ring-allreduce implementation with NCCL.
NCCL provides a highly optimized version of ring-allreduce.
NCCL 2 introduced the ability to run ring-allreduce across multiple machines,
enabling us to take advantage of its many performance boosting optimizations.</p></li>
</ul>
</section>
<section id="main-concept">
<h2>Main concept<a class="headerlink" href="#main-concept" title="Link to this heading"></a></h2>
<p>Horovod’s connection to MPI is deep, and for those familiar with MPI programming,
much of what you program to distribute model training with Horovod will feel familiar.</p>
<p>Four core principles that Horovod is based on are the MPI concepts: <em>size</em>, <em>rank</em>, <em>local rank</em>,
<em>allreduce</em>, <em>allgather</em>, <em>broadcast</em>, and <em>alltoall</em>.
These are best explained by example.
Say we launched a training script on 4 servers, each having 4 GPUs.
If we launched one copy of the script per GPU:</p>
<ul>
<li><p><strong>Size</strong> would be the number of processes, in this case, 16.</p></li>
<li><p><strong>Rank</strong> would be the unique process ID from 0 to 15 (size - 1).</p></li>
<li><p><strong>Local rank</strong> would be the unique process ID within the server from 0 to 3.</p></li>
<li><p><strong>Allreduce</strong> is an operation that aggregates data among multiple processes and
distributes results back to them. Allreduce is used to average dense tensors.</p>
<img alt="Allreduce" src="http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_allreduce_1.png" />
</li>
<li><p><strong>Allgather</strong> is an operation that gathers data from all processes on every process.
Allgather is used to collect values of sparse tensors.</p>
<img alt="allgather" src="http://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/allgather.png" />
</li>
<li><p><strong>Broadcast</strong> is an operation that broadcasts data from one process, identified by
root rank, onto every other process.</p>
<img alt="broadcast" src="http://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/broadcast_pattern.png" />
</li>
<li><p><strong>Alltoall</strong> is an operation to exchange data between all processes.
Alltoall may be useful to implement neural networks with advanced architectures that span multiple devices.</p></li>
</ul>
<p><a class="reference external" href="http://mpitutorial.com/tutorials">(Images Source)</a></p>
<p>Horovod, as with MPI, strictly follows the Single-Program Multiple-Data (SPMD)
paradigm where we implement the instruction flow of multiple processes in the
same file/program. Because multiple processes are executing code in parallel,
we have to take care about race conditions and also the synchronization of participating
processes.</p>
<p>Horovod assigns a unique numerical ID or rank (an MPI concept) to each process executing
the program. This rank can be accessed programmatically. As you will see below when writing Horovod code, by identifying a process’s rank programmatically in the code we can take steps such as:</p>
<ul class="simple">
<li><p>Pin that process to its own exclusive GPU.</p></li>
<li><p>Utilize a single rank for broadcasting values that need to be used uniformly by all ranks.</p></li>
<li><p>Utilize a single rank for collecting and/or reducing values produced by all ranks.</p></li>
<li><p>Utilize a single rank for logging or writing to disk.</p></li>
</ul>
</section>
<section id="how-to-use-horovod">
<h2>How to use Horovod<a class="headerlink" href="#how-to-use-horovod" title="Link to this heading"></a></h2>
<p>To use Horovod, we should add the following to the program:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">hvd.init()</span></code> to initialize Horovod.</p></li>
</ol>
<p>2. Pin each GPU to a single process. This is to avoid resource contention. With the typical
setup of one GPU per process, set this to local rank.
The first process on the server will be allocated the first GPU, the second process
will be allocated the second GPU, and so forth.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Horovod: pin GPU to be used to process local rank (one GPU per process)</span>
<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>3. Print Verbose Logs Only on the First Worker. When running on several N processors,
all N TensorFlow processes printed their progress to stdout (standard output).
We only want to see the state of the output once at any given time.
To accomplish this, we can arbitrarily select a single
rank to display the training progress. By convention, we typically call rank
0 the “root” rank and use it for logistical work such as I/O when only one
rank is required.</p>
<p>4. Add Distributed Optimizer. In the previous two sections we ran with multiple processes,
but each process was running completely independently – this is not data parallel training,
it is just multiple processes running serial training at the same time. The key step to
make the training data parallel is to average out gradients across all workers, so that
all workers are updating with the same gradients and thus moving in the same direction.
Horovod implements an operation that averages gradients across workers. Deploying this in
your code is very straightforward and just requires wrapping an existing
optimizer (<code class="docutils literal notranslate"><span class="pre">keras.optimizers.Optimizer</span></code>) with a Horovod distributed
optimizer (<code class="docutils literal notranslate"><span class="pre">horovod.keras.DistributedOptimizer</span></code>).</p>
<p>5. Initialize Random Weights on Only One Processor. Data parallel stochastic gradient
descent, at least in its traditionally defined sequential algorithm, requires weights to
be synchronized between all processors. We already know that this is accomplished for
backpropagation by averaging out the gradients among all processors prior to the weight
updates. Then the only other required step is for the weights to be synchronized initially.
Assuming we start from the beginning of the training (we’ll handle checkpoint/restart
later), this means that every processor needs to have the same random weights.
In a previous section, we mentioned that the first worker would broadcast parameters to
the rest of the workers. We will use
<code class="docutils literal notranslate"><span class="pre">horovod.keras.callbacks.BroadcastGlobalVariablesCallback</span></code> to make this happen.</p>
<p>6. Modify Training Loop to Execute Fewer Steps Per Epoch. As it stands, we are running
the same number of steps per epoch for the serial training implementation. But since we
have increased the number of workers by a factor of <code class="docutils literal notranslate"><span class="pre">N</span></code>, that means we’re doing <code class="docutils literal notranslate"><span class="pre">N</span></code> times
more work (when we sum the amount of work done over all processes). Our target was to
get the same answer in less time (that is, to speed up the training), so we want to keep
the total amount of work done the same (that is, to process the same number of examples
in the dataset). This means we need to do a factor of <code class="docutils literal notranslate"><span class="pre">N</span></code> fewer steps per epoch, so the
number of steps goes to <code class="docutils literal notranslate"><span class="pre">steps_per_epoch</span> <span class="pre">/</span> <span class="pre">number_of_workers</span></code>.
We will also speed up validation by validating <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">*</span> <span class="pre">num_test_iterations</span> <span class="pre">/</span> <span class="pre">number_of_workers</span></code>
steps on each worker. While we could just do num_test_iterations / number_of_workers on each
worker to get a linear speedup in the validation, the multiplier 3 provides over-sampling of
the validation data and helps to increase the probability that every validation example will be evaluated.</p>
<p>7. Average Validation Results Among Workers. Since we are not validating the full dataset
on each worker anymore, each worker will have different validation results. To improve
validation metric quality and reduce variance, we will average validation results among
all workers. To do so, we can use <code class="docutils literal notranslate"><span class="pre">horovod.keras.callbacks.MetricAverageCallback</span></code>.</p>
<p>8. Do Checkpointing Logic Only Using the Root Worker. The most important issue is that
there can be a race condition while writing the checkpoint to a file. If every rank
finishes the epoch at the same time, they might be writing to the same filename, and
this could result in corrupted data. But more to the point, we don’t even need to do
this: by construction in synchronous data parallel SGD, every rank has the same copy
of the weights at all times, so only one worker needs to write the checkpoint. As usual,
our convention will be that the root worker (rank 0) handles this.</p>
<p>9. Increase the learning rate. Given a fixed batch size per GPU, the effective batch size
for training increases when you use more GPUs, since we average out the gradients among
all processors. Standard practice is to scale the learning rate by the same factor that
you have scaled the batch size – that is, by the number of workers present. This can be
done so that the training script does not change for single-process runs, since in that
case you just multiply by 1.</p>
<p>The reason we do this is that the error of a mean of <em>n</em> samples (random variables) with
finite variance <span class="math notranslate nohighlight">\(\sigma\)</span> is approximately <span class="math notranslate nohighlight">\(\sigma/\sqrt(n)\)</span> when <span class="math notranslate nohighlight">\(n\)</span> is large (see the
<a class="reference external" href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>).
Hence, learning rates should be scaled at least with <span class="math notranslate nohighlight">\(\sqrt(k)\)</span> when using <span class="math notranslate nohighlight">\(k\)</span> times
bigger batch sizes in order to preserve the variance of the batch-averaged gradient.
In practice we use linear scaling, often out of convenience, although in different
circumstances one or the other may be superior in practice.</p>
<p>10. (Optional) Add learning rate warmup. Many models are sensitive to using a large learning
rate immediately after initialization and can benefit from learning rate warmup.
We saw earlier that we typically scale the learning rate linear with batch sizes.
But if the batch size gets large enough, then the learning rate will be very high,
and the network tends to diverge, especially in the very first few iterations.
We counteract this by gently ramping the learning rate to the target learning rate.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">lr_schedule</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">25</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1e-1</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">35</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1e-2</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">return</span> <span class="mf">1e-3</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>

<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateScheduler</span><span class="p">(</span><span class="n">lr_schedule</span><span class="p">))</span>
</pre></div>
</div>
<p>In practice, the idea is to start training with a lower learning rate and gradually raise it
to a target learning rate over a few epochs. Horovod has the convenient
<code class="docutils literal notranslate"><span class="pre">horovod.keras.callbacks.LearningRateWarmupCallback</span></code> for the Keras API that implements that logic.
By default it will, over the first 5 epochs, gradually increase the learning rate from
<code class="docutils literal notranslate"><span class="pre">initial</span> <span class="pre">learning</span> <span class="pre">rate</span> <span class="pre">/</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">workers</span></code> up to initial learning rate.</p>
</div></blockquote>
<p>Once the script is transformed to a proper form, it can be launched using <code class="docutils literal notranslate"><span class="pre">horovodrun</span></code>
command. Here are some general examples for how to run the train a model on a machine with 4 GPUs.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>horovodrun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span>-H<span class="w"> </span>localhost:4<span class="w"> </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<p>And for running on 4 machines with 4 GPUs each, we use</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>horovodrun<span class="w"> </span>-np<span class="w"> </span><span class="m">16</span><span class="w"> </span>-H<span class="w"> </span>server1:4,server2:4,server3:4,server4:4<span class="w"> </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<p>It is also possible to run the script using Open MPI without the horovodrun wrapper.
The launch command for the first example using <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> would be</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-bind-to<span class="w"> </span>none<span class="w"> </span>-map-by<span class="w"> </span>slot<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-x<span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO<span class="w"> </span>-x<span class="w"> </span>LD_LIBRARY_PATH<span class="w"> </span>-x<span class="w"> </span>PATH<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-mca<span class="w"> </span>pml<span class="w"> </span>ob1<span class="w"> </span>-mca<span class="w"> </span>btl<span class="w"> </span>^openib<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<p>And for the second example</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span>server1:4,server2:4,server3:4,server4:4<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-bind-to<span class="w"> </span>none<span class="w"> </span>-map-by<span class="w"> </span>slot<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-x<span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO<span class="w"> </span>-x<span class="w"> </span>LD_LIBRARY_PATH<span class="w"> </span>-x<span class="w"> </span>PATH<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-mca<span class="w"> </span>pml<span class="w"> </span>ob1<span class="w"> </span>-mca<span class="w"> </span>btl<span class="w"> </span>^openib<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>python<span class="w"> </span>train.py
</pre></div>
</div>
</section>
<section id="base-model">
<h2>Base model<a class="headerlink" href="#base-model" title="Link to this heading"></a></h2>
<p>The base model is the same as <a class="reference internal" href="../tf_intro/"><span class="doc">TensorFlow on a single GPU</span></a> section, an NLP model, given below.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>

<span class="c1"># Suppress tensorflow logging</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CPP_MIN_LOG_LEVEL&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow_hub</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hub</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Version: &quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hub version: &quot;</span><span class="p">,</span> <span class="n">hub</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU is&quot;</span><span class="p">,</span> <span class="s2">&quot;available&quot;</span> <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;NOT AVAILABLE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of GPUs :&#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)))</span>

<span class="n">logdir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span><span class="o">/</span><span class="s2">&quot;tensorboard_logs&quot;</span>
<span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">logdir</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Parse input arguments</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Transfer Learning Example&#39;</span><span class="p">,</span>
                                 <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--log-dir&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">logdir</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;tensorboard log directory&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch-size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;input batch size for training&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;number of epochs to train&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--base-lr&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;learning rate for a single GPU&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--patience&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Number of epochs that meet target before stopping&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--use-checkpointing&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># Steps</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip&#39;</span><span class="p">,</span>
             <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;zip&#39;</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">)</span>

<span class="n">train_df</span><span class="p">,</span> <span class="n">remaining</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">valid_df</span><span class="p">,</span> <span class="n">_</span>  <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">remaining</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">remaining</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The shape of training </span><span class="si">{}</span><span class="s2"> and validation </span><span class="si">{}</span><span class="s2"> datasets.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##-------------------------##&quot;</span><span class="p">)</span>

<span class="n">buffer_size</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">size</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">train_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">valid_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">module_url</span> <span class="o">=</span> <span class="s2">&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1&quot;</span>
<span class="n">embeding_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">name_of_model</span> <span class="o">=</span> <span class="s1">&#39;nnlm-en-dim128&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch size :&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_checkpointing</span><span class="p">:</span>
    <span class="c1"># callbacks.append(tfdocs.modeling.EpochDots()),</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">patience</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">)),</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">logdir</span><span class="o">/</span><span class="n">name</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_and_evaluate_model_ds</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">hub_layer</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">[</span><span class="n">embed_size</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
                                      <span class="n">hub_layer</span><span class="p">,</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="p">])</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">base_lr</span><span class="p">),</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(),</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)])</span>

    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                      <span class="n">epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
                      <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
                      <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                      <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span>
                      <span class="p">)</span>
    <span class="k">return</span> <span class="n">history</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;GPU:0&quot;</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">##-------------------------##&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training starts ...&quot;</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">train_and_evaluate_model_ds</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="o">=</span><span class="n">embeding_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name_of_model</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">endt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsed Time: </span><span class="si">{}</span><span class="s2"> ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="n">endt</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##-------------------------##&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Save above script as <code class="docutils literal notranslate"><span class="pre">Transfer_Learning_NLP.py</span></code> (or directly download <a class="reference download internal" download="" href="../_downloads/113108d50e2dc0302b11ce92b88d2a9a/Transfer_Learning_NLP.py"><code class="xref download docutils literal notranslate"><span class="pre">Transfer_Learning_NLP.py</span></code></a> )
and follow the instructions given in <a class="reference internal" href="../setup/"><span class="doc">Access to Vega</span></a> to start a notebook. Once the Jupyter notebook started, you can open a terminal from drop down on
the right side of the notebook and watch the usage of the GPUs using <code class="docutils literal notranslate"><span class="pre">watch</span> <span class="pre">-n</span> <span class="pre">0.5</span> <span class="pre">nvidia-smi</span></code>.
In the Jupyter notebook, we need to install TensorFlow Hub first</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow_hub</span>
</pre></div>
</div>
<p>And suppress the standard outputs of TensorFlow</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="c1"># Suppress tensorflow logging outputs</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CPP_MIN_LOG_LEVEL&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>
</pre></div>
</div>
<p>Now, we can run the base model using for 10 epochs and with the default batch size of 32.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">python</span> <span class="n">Transfer_Learning_NLP</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">epochs</span> <span class="mi">10</span>
</pre></div>
</div>
<div class="admonition-elapsed-time-as-a-function-of-batch-size exercise important admonition" id="exercise-0">
<p class="admonition-title">Elapsed time as a function of batch size</p>
<p>As you perhaps noticed, it took a rather long time to finish the job.
Do you know any parameter that can be tuned to make the calculations faster?
How does the elapsed time scale with the batch size?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Increasing the batch size reduces the training time. The reduction must be
almost linear.</p>
</div>
</div>
</section>
<section id="training-with-model-fit">
<h2>Training with <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code><a class="headerlink" href="#training-with-model-fit" title="Link to this heading"></a></h2>
<p>Applying the 10-step processors mentioned above to the <code class="docutils literal notranslate"><span class="pre">Transfer_Learning_NLP.py</span></code>, we will have</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>

<span class="c1"># Suppress tensorflow logging outputs</span>
<span class="c1"># os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &quot;2&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow_hub</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hub</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">logdir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span><span class="o">/</span><span class="s2">&quot;tensorboard_logs&quot;</span>
<span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">logdir</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Parse input arguments</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Transfer Learning Example&#39;</span><span class="p">,</span>
                                 <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--log-dir&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">logdir</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;tensorboard log directory&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num-worker&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;number of workers for training part&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch-size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;input batch size for training&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--base-lr&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;learning rate for a single GPU&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;number of epochs to train&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--momentum&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;SGD momentum&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--target-accuracy&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">.96</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Target accuracy to stop training&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--patience&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Number of epochs that meet target before stopping&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--use-checkpointing&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">)</span>

<span class="c1"># Step 10: register `--warmup-epochs`</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--warmup-epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;number of warmup epochs&#39;</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># Define a function for a simple learning rate decay over time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">lr_schedule</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">25</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1e-1</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">35</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1e-2</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">return</span> <span class="mf">1e-3</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>

<span class="c1">##### Steps</span>
<span class="c1"># Step 1: import Horovod</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">horovod.tensorflow.keras</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hvd</span>

<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="c1"># Nomrally Step 2: pin to a GPU</span>
<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>

<span class="c1"># Step 2: but in our case</span>
<span class="c1"># gpus = tf.config.list_physical_devices(&#39;GPU&#39;)</span>
<span class="c1"># if gpus:</span>
<span class="c1">#    tf.config.experimental.set_memory_growth(gpus[0], True)</span>

<span class="c1"># Step 3: only set `verbose` to `1` if this is the root worker.</span>
<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Version: &quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hub version: &quot;</span><span class="p">,</span> <span class="n">hub</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU is&quot;</span><span class="p">,</span> <span class="s2">&quot;available&quot;</span> <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;NOT AVAILABLE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of GPUs :&#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)))</span>
    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1">#####</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip&#39;</span><span class="p">,</span>
             <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;zip&#39;</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">)</span>

<span class="n">train_df</span><span class="p">,</span> <span class="n">remaining</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">valid_df</span><span class="p">,</span> <span class="n">_</span>  <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">remaining</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.09</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">remaining</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The shape of training </span><span class="si">{}</span><span class="s2"> and validation </span><span class="si">{}</span><span class="s2"> datasets.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##-------------------------##&quot;</span><span class="p">)</span>

<span class="n">buffer_size</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#train_dataset = tf.data.Dataset.from_tensor_slices((train_df.question_text.values, train_df.target.values)).repeat(args.epochs*2).shuffle(buffer_size).batch(args.batch_size)</span>
<span class="c1">#valid_dataset = tf.data.Dataset.from_tensor_slices((valid_df.question_text.values, valid_df.target.values)).repeat(args.epochs*2).batch(args.batch_size)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">train_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">valid_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">module_url</span> <span class="o">=</span> <span class="s2">&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1&quot;</span>
<span class="n">embeding_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">name_of_model</span> <span class="o">=</span> <span class="s1">&#39;nnlm-en-dim128&#39;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_model</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">hub_layer</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">[</span><span class="n">embed_size</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">hub_layer</span><span class="p">,</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)])</span>

    <span class="c1"># Step 9: Scale the learning rate by the number of workers.</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">base_lr</span> <span class="o">*</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">momentum</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span>
    <span class="c1"># opt = tf.optimizers.Adam(learning_rate=args.base_lr * hvd.size())</span>

    <span class="c1">#Step 4: Wrap the optimizer in a Horovod distributed optimizer</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span>
                                   <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                   <span class="n">average_aggregated_gradients</span><span class="o">=</span><span class="kc">True</span>
                                   <span class="p">)</span>

    <span class="c1"># For Horovod: We specify `experimental_run_tf_function=False` to ensure TensorFlow</span>
    <span class="c1"># uses hvd.DistributedOptimizer() to compute gradients.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(),</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)],</span>
                <span class="n">experimental_run_tf_function</span> <span class="o">=</span> <span class="kc">False</span>
                 <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Step 5: broadcast initial variable states from the first worker to</span>
<span class="c1"># all others by adding the broadcast global variables callback.</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># Step 7: average the metrics among workers at the end of every epoch</span>
<span class="c1"># by adding the metric average callback.</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">MetricAverageCallback</span><span class="p">())</span>

<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_checkpointing</span><span class="p">:</span>
    <span class="c1"># TensorFlow normal callbacks</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">apped</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">))</span>

    <span class="c1"># Step 8: checkpointing should only be done on the root worker.</span>
    <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">callbacks</span><span class="o">.</span><span class="n">apped</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">logdir</span><span class="o">/</span><span class="n">name_of_model</span><span class="p">))</span>

<span class="c1"># Step 10: implement a LR warmup over `args.warmup_epochs`</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateWarmupCallback</span><span class="p">(</span><span class="n">initial_lr</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span><span class="p">,</span> <span class="n">warmup_epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">warmup_epochs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">))</span>

<span class="c1"># Step 10: replace with the Horovod learning rate scheduler,</span>
<span class="c1"># taking care not to start until after warmup is complete</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateScheduleCallback</span><span class="p">(</span><span class="n">initial_lr</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span><span class="p">,</span> <span class="n">start_epoch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">warmup_epochs</span><span class="p">,</span> <span class="n">multiplier</span><span class="o">=</span><span class="n">lr_schedule</span><span class="p">))</span>


<span class="c1"># Creating model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="o">=</span><span class="n">embeding_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name_of_model</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">##-------------------------##&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training starts ...&quot;</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                    <span class="c1"># Step 6: keep the total number of steps the same despite of an increased number of workers</span>
                    <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="p">)</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                    <span class="c1"># steps_per_epoch = ( 5000 ) // hvd.size(),</span>
                    <span class="n">workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_worker</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
                    <span class="c1">#Step 6: set this value to be 3 * num_test_iterations / number_of_workers</span>
                    <span class="n">validation_steps</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="n">valid_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="p">)</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                    <span class="c1"># validation_steps = ( 5000 ) // hvd.size(),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
                    <span class="c1"># use_multiprocessing = True,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>

<span class="n">endt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span>

<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsed Time: </span><span class="si">{}</span><span class="s2"> ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="n">endt</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##-------------------------##&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We can also download <a class="reference download internal" download="" href="../_downloads/23c91e6565c3bc46858a123c488ec6b5/Transfer_Learning_NLP_Horovod.py"><code class="xref download docutils literal notranslate"><span class="pre">the</span> <span class="pre">python</span> <span class="pre">script</span></code></a>
from the Github repository.</p>
<div class="admonition-does-the-training-scale exercise important admonition" id="exercise-1">
<p class="admonition-title">Does the training scale?</p>
<p>Now you can launch the Horovod training on the number of GPUs you booked in
your Jupyter notebook using</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="err">$</span><span class="n">np</span> <span class="o">-</span><span class="n">H</span> <span class="n">localhost</span><span class="p">:</span><span class="err">$</span><span class="n">np</span> <span class="o">-</span><span class="n">bind</span><span class="o">-</span><span class="n">to</span> <span class="n">none</span> <span class="o">-</span><span class="nb">map</span><span class="o">-</span><span class="n">by</span> <span class="n">slot</span> \
<span class="n">python</span> <span class="n">Transfer_Learning_NLP_Horovod</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">epochs</span> <span class="mi">10</span> <span class="o">--</span><span class="n">batch</span><span class="o">-</span><span class="n">size</span> <span class="mi">64</span>
</pre></div>
</div>
<p>Does the total time scale with the number of processers <code class="docutils literal notranslate"><span class="pre">$np</span></code>? Can you
explain the reason?</p>
<p>What does happen when you increase the batch size?</p>
</div>
<div class="admonition-horovodize-a-cnn-model exercise important admonition" id="exercise-2">
<p class="admonition-title">Horovodize a CNN model.</p>
<p>You can find a CNN model for an MNIST dataset <a class="reference download internal" download="" href="../_downloads/97408403ab2b1c4500a5d230517b90be/SVHN_class.py"><code class="xref download docutils literal notranslate"><span class="pre">here</span></code></a>. Apply
the steps mentioned above and test your script.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>The solution can be found <a class="reference download internal" download="" href="../_downloads/e900458a6cd35ddf94bb0dbb5e704712/SVHN_class_Horovod.py"><code class="xref download docutils literal notranslate"><span class="pre">here</span></code></a>.</p>
</div>
</div>
<div class="admonition-advanced-custom-training exercise important admonition" id="exercise-3">
<p class="admonition-title">Advanced - Custom training</p>
<p>Instead of using <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code>, write a custom training loop within the framework of Horovod.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>Two main differences that should be made are:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Definig the loss function using Horovod</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">first_batch</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
          <span class="n">probs</span> <span class="o">=</span> <span class="n">mnist_model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

    <span class="c1"># Horovod: add Horovod Distributed GradientTape.</span>
    <span class="n">tape</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedGradientTape</span><span class="p">(</span><span class="n">tape</span><span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss_value</span><span class="p">,</span> <span class="n">mnist_model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">mnist_model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="c1"># Horovod: broadcast initial variable states from rank 0 to all other processes.</span>
    <span class="c1"># This is necessary to ensure consistent initialization of all workers when</span>
    <span class="c1"># training is started with random weights or restored from a checkpoint.</span>
    <span class="c1"># Please see `the documentation &lt;https://horovod.readthedocs.io/en/stable/api.html#horovod.tensorflow.broadcast_variables&gt;`_.</span>
    <span class="c1"># Note: broadcast should be done after the first gradient step to ensure optimizer</span>
    <span class="c1"># initialization.</span>

    <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">mnist_model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss_value</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Looping over the dataset</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10000</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">())):</span>
    <span class="n">loss_value</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Step #</span><span class="si">%d</span><span class="se">\t</span><span class="s1">Loss: </span><span class="si">%.6f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">loss_value</span><span class="p">))</span>
</pre></div>
</div>
</div></blockquote>
</div>
</div>
</section>
<section id="analysis-of-performance">
<h2>Analysis of Performance<a class="headerlink" href="#analysis-of-performance" title="Link to this heading"></a></h2>
<p>Horovod has the ability to record the timeline of its activity, called Horovod Timeline.
To record a Horovod Timeline, set the <code class="docutils literal notranslate"><span class="pre">--timeline-filename</span></code> command line argument to the
location of the timeline file to be created. This file is only recorded on rank 0,
but it contains information about activity of all workers.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>horovodrun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span>--timeline-filename<span class="w"> </span>/path/to/timeline.json<span class="w"> </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<p>You can then open the timeline file using the <code class="docutils literal notranslate"><span class="pre">chrome://tracing</span></code> facility of
the Chrome-based browsers. Nonetheless, we do not employ the profiler in this workshop.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../tf_mltgpus/" class="btn btn-neutral float-left" title="Distributed training in TensorFlow" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../train_contain/" class="btn btn-neutral float-right" title="Training Neural Networks using Containers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, ENCCS, and individual contributors..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>