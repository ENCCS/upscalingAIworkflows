

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Upscaling AI workflows documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
      <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx_lesson.css" />
      <link rel="stylesheet" type="text/css" href="_static/term_role_formatting.css" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx_rtd_theme_ext_color_contrast.css" />
      <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
      <link rel="stylesheet" type="text/css" href="_static/overrides.css" />

  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js"></script>
      <script src="_static/doctools.js"></script>
      <script src="_static/sphinx_highlight.js"></script>
      <script src="_static/clipboard.min.js"></script>
      <script src="_static/copybutton.js"></script>
      <script src="_static/minipres.js"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="_static/togglebutton.js"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="_static/tabs.js"></script>
      <script data-domain="enccs.github.io/upscalingAIworkflows" defer="defer" src="https://plausible.io/js/script.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            Upscaling AI workflows
              <img src="_static/ENCCS.jpg" class="logo" alt="Logo"/>
          </a>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="#document-setup">Access to Vega</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">The lesson</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-intro-container">Introduction to Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-intro_docker">Introduction to Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-mang_contain">Cleaning Up Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-create_contain">Creating your own container images</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-compx_contain">Creating More Complex Container Images</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-singlrty_start">What is Singularity?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-work_contain">Working with Singularity containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-build_contain">Building Singularity images</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-tf_intro">TensorFlow on a single GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-tf_mltgpus">Distributed training in TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-hvd_intro">Intoduction to Horovod</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-train_contain">Training Neural Networks using Containers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Optional</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-containers/content/mpi_contain">Running MPI parallel jobs using Singularity containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-containers/content/rep_gran">Containers in research workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-containers/content/pwd_exmps">PWD exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-upscalingAIcontainer/content/namespc-cgroup">Namespaces and cgroups</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-guide">Instructor’s guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Upscaling AI workflows</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Upscaling AI workflows  documentation</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/upscalingAIworkflows/blob/main/content/index" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="upscaling-ai-workflows">
<h1>Upscaling AI Workflows<a class="headerlink" href="#upscaling-ai-workflows" title="Link to this heading"></a></h1>
<p>Artificial Intelligence (AI) has become a foundational building block
of our modern world.  Accordingly, a vast effort has been put into
bringing AI to researchers and practitioners of a wide range of
fields.  Nonetheless, the computationally intensive task of training
an AI increasingly requires more computational power than what our
laptops and PCs can offer. Therefore, the ability to develop and train
a neural network on large clusters seems imperative. This workshop
teaches us how to scale an AI-powered application in large clusters,
i.e., supercomputers.</p>
<div class="admonition-prerequisites prerequisites admonition" id="prerequisites-0">
<p class="admonition-title">Prerequisites</p>
<p>Working knowledge of Unix OS is required. In addition, a basic
understanding of Neural Networks (NNs) is desirable.  Please follow
the <a class="reference external" href="https://docs.docker.com/get-docker/">link</a> to install docker
locally on your laptop as we need to use it for some part of the
begininng of this workshop. Details of using and accessing to
the Vega cluster are given in <a class="reference internal" href="#document-setup"><span class="doc">Access to Vega</span></a> section.</p>
</div>
<div class="toctree-wrapper compound">
<span id="document-setup"></span><section id="access-to-vega">
<span id="setup"></span><h2>Access to Vega<a class="headerlink" href="#access-to-vega" title="Link to this heading"></a></h2>
<p>You should have received your username from the Vega support with the help of our colleagues at
ENCCS. One important step to login to the system is to generate an SSH key and upload your public
to the server. You can read it in <a class="reference external" href="https://doc.vega.izum.si/ssh/">the Vega documentation</a>.</p>
<p>Once the setup is completed, you can login to the system. While the <code class="docutils literal notranslate"><span class="pre">Singularity</span></code>
module is readily available in the login node, we should book a compute node to run
our examples. You can book a CPU-node for 1 hour using this command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>salloc<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-t<span class="w"> </span><span class="m">1</span>:00:00
</pre></div>
</div>
<p>And for a GPU-node with <em>X</em> number of GPUs per node (there are 4 GPUs available per node in Vega)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>salloc<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>--gres<span class="o">=</span>gpu:X<span class="w"> </span>--partition<span class="o">=</span>gpu<span class="w"> </span>--mem-per-gpu<span class="o">=</span>40GB<span class="w"> </span>--reservation<span class="w"> </span>enccs<span class="w"> </span><span class="se">\</span>
--ntasks<span class="w"> </span><span class="m">4</span><span class="w"> </span>--cpus-per-task<span class="w"> </span><span class="m">1</span><span class="w">  </span>-t<span class="w"> </span><span class="m">02</span>:00:00
</pre></div>
</div>
<p>Once the allocation is granted you will receive a message similar to</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>salloc: Pending job allocation 24122556
salloc: job 24122556 queued and waiting for resources
salloc: job 24122556 has been allocated resources
salloc: Granted job allocation 24122556
salloc: Waiting for resource configuration
salloc: Nodes cn0381 are ready for job
</pre></div>
</div>
<p>The granted CPU compute node here is <cite>cn0381</cite>. The general form for the CPU compute node likes
like <cite>cn0XXX</cite> for GPU compute node <cite>gnXXX</cite>. Now you should SSH to the compute node to run interactively
our job using the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>cn0381
</pre></div>
</div>
<p>You might get a warning regarding the authenticity of the host, similar to the
output below.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>The authenticity of host &#39;cn0381 (&lt;no hostip for proxy command&gt;)&#39; can&#39;t be established.
ECDSA key fingerprint is SHA256:0BOlvbjVPLytjYEium04uNTCACCQN/Rr7NMJhje30aw.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added &#39;cn0381&#39; (ECDSA) to the list of known hosts.
</pre></div>
</div>
<p>Please enter <code class="docutils literal notranslate"><span class="pre">yes</span></code> to the question and ignore it as it is a self-assigned useless check
that doesn’t understand what is the purpose of our login. Now, we are able to run
our jobs interactively.</p>
<p>To run Jupyter Notebooks, we need to load Anaconda module.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>Anaconda3/2020.11
module<span class="w"> </span>load<span class="w"> </span>scikit-learn
</pre></div>
</div>
<p>Afterwards, you can run a Jupyter kernel by specifying the port number and ip address.
The ip address here is the name of compute node, in the example given above is <cite>cn0381</cite>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>jupyter-notebook<span class="w"> </span>--no-browser<span class="w"> </span>--port<span class="o">=</span><span class="m">8888</span><span class="w"> </span>--ip<span class="o">=</span>cn0381
</pre></div>
</div>
<p>The result should look like</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[I 13:21:26.105 NotebookApp] JupyterLab extension loaded from /cvmfs/sling.si/modules/el7/software/Anaconda3/2020.11/lib/python3.8/site-packages/jupyterlab
[I 13:21:26.105 NotebookApp] JupyterLab application directory is /cvmfs/sling.si/modules/el7/software/Anaconda3/2020.11/share/jupyter/lab
[I 13:21:26.107 NotebookApp] Serving notebooks from local directory: /ceph/hpc/home/euhosseine
[I 13:21:26.107 NotebookApp] Jupyter Notebook 6.1.4 is running at:
[I 13:21:26.107 NotebookApp] http://cn0381:8888/?token=80d695595aa333c6d97dc6f868f96b36f4812622a5008090
[I 13:21:26.107 NotebookApp]  or http://127.0.0.1:8888/?token=80d695595aa333c6d97dc6f868f96b36f4812622a5008090
[I 13:21:26.107 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 13:21:26.122 NotebookApp]

  To access the notebook, open this file in a browser:
      file:///ceph/hpc/home/euhosseine/.local/share/jupyter/runtime/nbserver-339349-open.html
  Or copy and paste one of these URLs:
      http://cn0381:8888/?token=80d695595aa333c6d97dc6f868f96b36f4812622a5008090
   or http://127.0.0.1:8888/?token=80d695595aa333c6d97dc6f868f96b36f4812622a5008090
</pre></div>
</div>
<p>In your local machine (PC/laptop), open a terminal and use this command to tunnel to the running kernel
on Vega.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>-N<span class="w"> </span>-f<span class="w"> </span>-L<span class="w"> </span><span class="m">8888</span>:cn0381:8888<span class="w"> </span>username@vglogin0005.vega.izum.si
</pre></div>
</div>
<p>The first port number is for your local machine and the second port number is what
you specified above running a Jupyter Notebook. Open a browser, and enter <code class="docutils literal notranslate"><span class="pre">http://localhost:8888</span></code>.
You should see a prompt to enter the password or the token. The token in this run is the number after
the <code class="docutils literal notranslate"><span class="pre">token</span></code>. Entering the token, you will be shown the tree of structure of home folder.</p>
<p>To use TensorFlow or Horovod in this course, we can simply load them through module system.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>TensorFlow/2.5.0-fosscuda-2020b
</pre></div>
</div>
<p>Or</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>Horovod/0.22.1-fosscuda-2020b-TensorFlow-2.5.0
</pre></div>
</div>
</section>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>5 min</p></td>
<td><p><a class="reference internal" href="#document-intro-container"><span class="doc">Introduction to Containers</span></a></p></td>
</tr>
<tr class="row-even"><td><p>10 min</p></td>
<td><p><a class="reference internal" href="#document-intro_docker"><span class="doc">Introduction to Docker</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>10 min</p></td>
<td><p><a class="reference internal" href="#document-mang_contain"><span class="doc">Cleaning Up Containers</span></a></p></td>
</tr>
<tr class="row-even"><td><p>10 min</p></td>
<td><p><a class="reference internal" href="#document-create_contain"><span class="doc">Creating your own container images</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>10 min</p></td>
<td><p><a class="reference internal" href="#document-compx_contain"><span class="doc">Creating More Complex Container Images</span></a></p></td>
</tr>
<tr class="row-even"><td><p>10 min</p></td>
<td><p><a class="reference internal" href="#document-singlrty_start"><span class="doc">What is Singularity?</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>10 min</p></td>
<td><p><a class="reference internal" href="#document-work_contain"><span class="doc">Working with Singularity containers</span></a></p></td>
</tr>
<tr class="row-even"><td><p>10 min</p></td>
<td><p><a class="reference internal" href="#document-build_contain"><span class="doc">Building Singularity images</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>25 min</p></td>
<td><p><a class="reference internal" href="#document-tf_intro"><span class="doc">TensorFlow on a single GPU</span></a></p></td>
</tr>
<tr class="row-even"><td><p>25 min</p></td>
<td><p><a class="reference internal" href="#document-tf_mltgpus"><span class="doc">Distributed training in TensorFlow</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>25 min</p></td>
<td><p><a class="reference internal" href="#document-hvd_intro"><span class="doc">Intoduction to Horovod</span></a></p></td>
</tr>
<tr class="row-even"><td><p>25 min</p></td>
<td><p><a class="reference internal" href="#document-train_contain"><span class="doc">Training Neural Networks using Containers</span></a></p></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper compound">
<span id="document-intro-container"></span><section id="introduction-to-containers">
<span id="intro-container"></span><h2>Introduction to Containers<a class="headerlink" href="#introduction-to-containers" title="Link to this heading"></a></h2>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="https://upload.wikimedia.org/wikipedia/commons/1/1c/MAERSK_MC_KINNEY_MÖLLER_%26_MARSEILLE_MAERSK_%2848694054418%29.jpg"><img alt="https://upload.wikimedia.org/wikipedia/commons/1/1c/MAERSK_MC_KINNEY_MÖLLER_%26_MARSEILLE_MAERSK_%2848694054418%29.jpg" src="https://upload.wikimedia.org/wikipedia/commons/1/1c/MAERSK_MC_KINNEY_MÖLLER_%26_MARSEILLE_MAERSK_%2848694054418%29.jpg" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-text"><a class="reference external" href="https://upload.wikimedia.org/wikipedia/commons/1/1c/MAERSK_MC_KINNEY_MÖLLER_%26_MARSEILLE_MAERSK_%2848694054418%29.jpg">(Image Source)</a></span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>When software is deployed, a set of libraries and configuration files
is used in a runtime environment.  Typically, we have several
applications running in the runtime environment.  Therefore, if a
system update changes a lib to fix an issue, it might break other apps
that use the same library.  We all have experienced this at some
point.</p>
<p>As a developer, you need to control the version of libraries within
the runtime environment.  Two technologies that can help you achieve
this goal are Containers and Virtual Machines (VMs).  Managing the
environment of the apps becomes possible with the help of
“virtualization.”  The system resources, e.g., RAM, CPU, storage,
networking, can be “virtually” delivered as multiple resources in the
virtualization process.</p>
<p>Containers are executable units of software that encapsulate
everything to run. In principle, one can run containers anywhere.
Containers use the operating system (OS) level virtualization,
isolating processes from the rest of the OS environment and
controlling the allocation of (hardware) resources. The isolation is
enabled via kernel namespaces and cgroups (as we will discuss them in
detail at <span class="xref std std-doc">namespc-cgroup</span> section), which have been in Linux for
a long time.</p>
<div class="admonition-virtualization callout admonition" id="callout-0">
<p class="admonition-title">Virtualization</p>
<p>Containers are an example of what’s called <strong>virtualization</strong> –
having a second “virtual” computer running and accessible from a main
or <strong>host</strong> computer. A VM typically contains a whole copy
of an operating system in addition to its own file system and has to
get booted up in the same way a computer would. A container is
considered a lightweight version of a virtual machine; underneath,
the container is using the Linux kernel and simply has some flavor of
Linux + the file system inside.</p>
</div>
<p>The key differentiator between containers and VMs is that VMs
virtualize an entire machine down to the hardware layers and
containers only virtualize software layers above the operating system
level.</p>
<img alt="_images/conts_vms.jpeg" src="_images/conts_vms.jpeg" />
<section id="cons-and-pros-of-containers">
<h3>Cons and Pros of Containers<a class="headerlink" href="#cons-and-pros-of-containers" title="Link to this heading"></a></h3>
<section id="pros">
<h4>Pros<a class="headerlink" href="#pros" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>Containers are lightweight software packages that contain all the
dependencies.</p></li>
<li><p>Because of their lightweight, it is easy and very fast to
iteratively modify them.</p></li>
</ul>
</section>
<section id="cons">
<h4>Cons<a class="headerlink" href="#cons" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>Since containers share the same underlying hardware system, it is
possible that an exploit in one container could break out of the
container and affect the shared hardware.</p></li>
</ul>
</section>
</section>
<section id="cons-and-pros-of-vms">
<h3>Cons and Pros of VMs<a class="headerlink" href="#cons-and-pros-of-vms" title="Link to this heading"></a></h3>
<section id="id1">
<h4>Pros<a class="headerlink" href="#id1" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>VMs are immune to any exploits or interference from other VMs on a
shared host due run in isolation as because of a fully standalone
system.</p></li>
<li><p>Since VMs are full-flegded OS, they are more dynamic and can be
interactively developed. Once the basic hardware definition is
specified for a VM, the VM can then be treated as a bare bones
computer.</p></li>
</ul>
</section>
<section id="id2">
<h4>Cons<a class="headerlink" href="#id2" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>It is time consuming to build and regenerate VMs, because they
encompass a full stack system. Any modifications to a VM snapshot
can take significant time to regenerate and validate they behave as
expected.</p></li>
<li><p>VMs can take up a lot of storage space. They can quickly grow to
several Gigabytes in size. This can lead to disk space shortage
issues on the VMs host machine.</p></li>
</ul>
<p>One final term: if the <em>container</em> is an alternative file system layer
that you can access and run from your computer, the <strong>container image</strong>
is like a template for that container. The container image has all the
needed information to start up a running copy of the container. A
running container tends to be transient and can be started and shut
down. The image is more long-lived, as a source file for the container.
You could think of the container image like a cookie cutter – it can be
used to create multiple copies of the same shape (or container) and is
relatively unchanging, where cookies come and go. If you want a
different type of container (cookie) you need a different image (cookie
cutter).</p>
<section id="putting-the-pieces-together">
<h5>Putting the Pieces Together<a class="headerlink" href="#putting-the-pieces-together" title="Link to this heading"></a></h5>
<p>Think back to some of the challenges we described at the beginning. The
many layers of scientific software installations make it hard to install
and re-install scientific software – which ultimately, hinders
reliability and reproducibility.</p>
<p>But now, think about what a container is - a self-contained, complete,
separate computer file system. What if you put your scientific software
tools into a container?</p>
<p>This solves several of our problems:</p>
<ul class="simple">
<li><p>There is a clear record of what software and software dependencies
were used, from bottom to top.</p></li>
<li><p>The container can be used on any computer that has Docker installed</p></li>
<li><p>It doesn’t matter whether the computer is Mac, Windows or Linux-based.</p></li>
<li><p>The container ensures that you can use the exact same software and
environment on your computer and on other resources (like a
large-scale computing cluster).</p></li>
</ul>
<p>The rest of this workshop will show you how to download and run
pre-existing containers on your own computer, and how to create and
share your own containers.</p>
</section>
</section>
</section>
</section>
<span id="document-intro_docker"></span><section id="introduction-to-docker">
<span id="intro-docker"></span><h2>Introduction to Docker<a class="headerlink" href="#introduction-to-docker" title="Link to this heading"></a></h2>
<p>There are many container runtime available out there. One of popular runtimes is Docker.
Docker has proven to an extraordinary tool for developers and researchers alike.
Most of today’s workshop will be spent of Docker command line (CLI) utility.</p>
<section id="downloading-docker-images">
<h3>Downloading Docker images<a class="headerlink" href="#downloading-docker-images" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span></code> command is used to list and modify Docker images.
You can find out what container images you have on your computer by
using the following command (“ls” is short for “list”):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>ls
</pre></div>
</div>
<p>If you’ve just installed Docker, you won’t see any images listed.</p>
<p>To get a copy of the <code class="docutils literal notranslate"><span class="pre">hello-world</span></code> Docker image from the internet, run
this command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>pull<span class="w"> </span>hello-world
</pre></div>
</div>
<p>You should see output like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Using default tag: latest
latest: Pulling from library/hello-world
93288797bd35: Pull complete
Digest: sha256:975f4b14f326b05db86e16de00144f9c12257553bba9484fed41f9b6f2257800
Status: Downloaded newer image for hello-world:latest
docker.io/library/hello-world:latest
</pre></div>
</div>
<div class="admonition-dockerhub callout admonition" id="callout-0">
<p class="admonition-title">DockerHub</p>
<p>Where did the <code class="docutils literal notranslate"><span class="pre">hello-world</span></code> image come from? It came from the
DockerHub website, which is a place to share Docker images with other
people. More on that in a later episode.</p>
</div>
<div class="admonition-exercise-check-on-your-images exercise important admonition" id="exercise-0">
<p class="admonition-title">Exercise: Check on Your Images</p>
<p>What command would you use to see if the <code class="docutils literal notranslate"><span class="pre">hello-world</span></code> Docker image
had downloaded successfully and was on your computer? Give it a try
before checking the solution.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>To see if the <code class="docutils literal notranslate"><span class="pre">hello-world</span></code> image is now on your computer, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>ls
</pre></div>
</div>
</div>
</div>
<p>Note that the downloaded <code class="docutils literal notranslate"><span class="pre">hello-world</span></code> image is not in the folder
where you are in the terminal! (Run <code class="docutils literal notranslate"><span class="pre">ls</span></code> by itself to check.) The
image is not a file like our normal programs and files; Docker stores it
in a specific location that isn’t commonly accessed, so it’s necessary
to use the special <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span></code> command to see what Docker images
you have on your computer.</p>
</section>
<section id="running-the-hello-world-container">
<h3>Running the <code class="docutils literal notranslate"><span class="pre">hello-world</span></code> container<a class="headerlink" href="#running-the-hello-world-container" title="Link to this heading"></a></h3>
<p>To create and run containers from named Docker images you use the
<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command. Try the following <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> invocation.
Note that it does not matter what your current working directory is.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>hello-world

Hello<span class="w"> </span>from<span class="w"> </span>Docker!
This<span class="w"> </span>message<span class="w"> </span>shows<span class="w"> </span>that<span class="w"> </span>your<span class="w"> </span>installation<span class="w"> </span>appears<span class="w"> </span>to<span class="w"> </span>be<span class="w"> </span>working<span class="w"> </span>correctly.
</pre></div>
</div>
<p>To generate this message, Docker took the following steps:</p>
<ol class="arabic simple">
<li><p>The Docker client contacted the Docker daemon.</p></li>
<li><p>The Docker daemon pulled the “hello-world” image from the Docker Hub.
(arm64v8)</p></li>
<li><p>The Docker daemon created a new container from that image which runs the
executable that produces the output you are currently reading.</p></li>
<li><p>The Docker daemon streamed that output to the Docker client,
which sent it to your terminal.</p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">hello-world</span></code> container is set up to run an action by default -
namely to print this message.</p>
<div class="admonition-using-docker-run-to-get-the-image callout admonition" id="callout-1">
<p class="admonition-title">Using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> to get the image</p>
<p>We could have skipped the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">pull</span></code> step; if you use the
<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command and you don’t already have a copy of the
Docker image, Docker will automatically pull the image first and then
run it.</p>
</div>
</section>
<section id="running-a-container-with-a-chosen-command">
<h3>Running a container with a chosen command<a class="headerlink" href="#running-a-container-with-a-chosen-command" title="Link to this heading"></a></h3>
<p>But what if we wanted to do something different with the container? The
output just gave us a suggestion of what to do – let’s use a different
Docker image to explore what else we can do with the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code>
command. The suggestion above is to use <code class="docutils literal notranslate"><span class="pre">ubuntu</span></code>, but we’re going to
run a different type of Linux, <code class="docutils literal notranslate"><span class="pre">alpine</span></code> instead because it’s quicker
to download.</p>
<div class="admonition-run-the-alpine-docker-container callout admonition" id="callout-2">
<p class="admonition-title">Run the Alpine Docker container</p>
<p>Try downloading and running the <code class="docutils literal notranslate"><span class="pre">alpine</span></code> Docker container. You can
do it in two steps, or one. What are they?</p>
</div>
<p>What happened when you ran the Alpine Docker container?</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>alpine
</pre></div>
</div>
<p>If you never used the <em>alpine</em> docker image on your computer, docker
probably printed a message that it couldn’t find the image and had to
download it. If you used the alpine image before, the command will
probably show no output. That’s because this particular container is
designed for you to provide commands yourself. Try running this instead:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>alpine<span class="w"> </span>cat<span class="w"> </span>/etc/os-release
</pre></div>
</div>
<p>You should see the output of the <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/etc/os-release</span></code> command, which
prints out the version of Alpine Linux that this container is using and
a few additional bits of information.</p>
<div class="admonition-exercise-hello-world-part-2 exercise important admonition" id="exercise-1">
<p class="admonition-title">Exercise: Hello World, Part 2</p>
<p>Can you run the container and make it print a “hello world” message?
Give it a try before checking the solution.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>Use the same command as above, but with the <code class="docutils literal notranslate"><span class="pre">echo</span></code> command to
print a message.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>alpine<span class="w"> </span><span class="nb">echo</span><span class="w"> </span>‘Hello<span class="w"> </span>World’
</pre></div>
</div>
</div>
</div>
<p>So here, we see another option – we can provide commands at the end of
the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command and they will execute inside the running
container.</p>
</section>
<section id="running-containers-interactively">
<h3>Running containers interactively<a class="headerlink" href="#running-containers-interactively" title="Link to this heading"></a></h3>
<p>In all the examples above, Docker has started the container, run a
command, and then immediately shut down the container. But what if we
wanted to keep the container running so we could log into it and test
drive more commands? The way to do this is by adding the interactive
flag <code class="docutils literal notranslate"><span class="pre">-it</span></code> to the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command and by providing a shell
(usually <code class="docutils literal notranslate"><span class="pre">bash</span></code> or <code class="docutils literal notranslate"><span class="pre">sh</span></code>) as our command. The alpine docker image
doesn’t include <code class="docutils literal notranslate"><span class="pre">bash</span></code> so we need to use <code class="docutils literal notranslate"><span class="pre">sh</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>alpine<span class="w"> </span>sh
</pre></div>
</div>
<p>Your prompt should change significantly to look like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/<span class="w"> </span><span class="c1">#</span>
</pre></div>
</div>
<p>That’s because you’re now inside the running container! Try these
commands:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pwd</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ls</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">whoami</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">$PATH</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/etc/os-release</span></code></p></li>
</ul>
<p>All of these are being run from inside the running container, so you’ll
get information about the container itself, instead of your computer. To
finish using the container, just type <code class="docutils literal notranslate"><span class="pre">exit</span></code>.</p>
<div class="admonition-reminder-of-terminology-images-and-containers callout admonition" id="callout-3">
<p class="admonition-title">Reminder of terminology: images and containers</p>
<p>Recall that a container “image” is the template from which particular
instances of containers will be created.</p>
</div>
<p>Let’s explore our first Docker container. The Docker team provides a
simple container image online called <code class="docutils literal notranslate"><span class="pre">hello-world</span></code>. We’ll start with
that one.</p>
</section>
</section>
<span id="document-mang_contain"></span><section id="cleaning-up-containers">
<span id="mang-contain"></span><h2>Cleaning Up Containers<a class="headerlink" href="#cleaning-up-containers" title="Link to this heading"></a></h2>
<section id="removing-images">
<h3>Removing images<a class="headerlink" href="#removing-images" title="Link to this heading"></a></h3>
<p>The images and their corresponding containers can start to take up a
lot of disk space if you don’t clean them up occasionally, so it’s a
good idea to periodically remove container images that you won’t be
using anymore.</p>
<p>In order to remove a specific image, you need to find out details
about the image, specifically, the “image ID”. For example say my
laptop contained the following image.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>ls
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>REPOSITORY       TAG         IMAGE ID       CREATED          SIZE
hello-world      latest      fce289e99eb9   15 months ago    1.84kB
</pre></div>
</div>
<p>You can remove the image with a <cite>docker image rm</cite> command that
includes the image ID, such as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>rm<span class="w"> </span>fce289e99eb9
</pre></div>
</div>
<p>or use the image name, like so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>rm<span class="w"> </span>hello-world
</pre></div>
</div>
<p>However, you may see this output:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Error response from daemon: conflict: unable to remove repository
reference &quot;hello-world&quot; (must force) - container e7d3b76b00f4 is
using its referenced image fce289e99eb9
</pre></div>
</div>
<p>This happens when Docker hasn’t cleaned up some of the times when a
container has been actually run. So before removing the container
image, we need to be able to see what containers are currently
running, or have been run recently, and how to remove these.</p>
<section id="what-containers-are-running">
<h4>What containers are running?<a class="headerlink" href="#what-containers-are-running" title="Link to this heading"></a></h4>
<p>Working with containers, we are going to shift to a new docker
command: <cite>docker container</cite>.  Similar to <cite>docker image</cite>, we can list
running containers by typing:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>ls
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
</pre></div>
</div>
<p>Notice that this command didn’t return any containers because our
containers all exited and thus stopped running after they completed
their work.</p>
<div class="admonition-docker-ps callout admonition" id="callout-0">
<p class="admonition-title"><cite>docker ps</cite></p>
<p>The command <cite>docker ps</cite> serves the same purpose as <cite>docker container
ls</cite>, and comes from the Unix shell command <cite>ps</cite> which describes
running processes.</p>
</div>
</section>
</section>
<section id="what-containers-have-run-recently">
<h3>What containers have run recently?<a class="headerlink" href="#what-containers-have-run-recently" title="Link to this heading"></a></h3>
<p>There is also a way to list running containers, and those that have
completed recently, which is to add the <cite>–all</cite>/<cite>-a</cite> flag to the
<cite>docker container ls</cite> command as shown below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>ls<span class="w"> </span>--all
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES
9c698655416a        hello-world         &quot;/hello&quot;            2 minutes ago       Exited (0) 2 minutes ago                       zen_dubinsky
6dd822cf6ca9        hello-world         &quot;/hello&quot;            3 minutes ago       Exited (0) 3 minutes ago                       eager_engelbart
</pre></div>
</div>
<div class="admonition-keeping-it-clean callout admonition" id="callout-1">
<p class="admonition-title">Keeping it clean</p>
<p>You might be surprised at the number of containers Docker is still
keeping track of.  One way to prevent this from happening is to add
the <cite>–rm</cite> flag to <cite>docker run</cite>. This will completely wipe out the
record of the run container when it exits. If you need a reference
to the running container for any reason, <strong>don’t</strong> use this flag.</p>
</div>
</section>
<section id="how-do-i-remove-an-exited-container">
<h3>How do I remove an exited container?<a class="headerlink" href="#how-do-i-remove-an-exited-container" title="Link to this heading"></a></h3>
<p>To delete an exited container you can run the following command,
inserting the <cite>CONTAINER ID</cite> for the container you wish to remove.  It
will repeat the <cite>CONTAINER ID</cite> back to you, if successful.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>rm<span class="w"> </span>9c698655416a
</pre></div>
</div>
<p>output:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>9c698655416a
</pre></div>
</div>
<p>If you want to remove all exited containers at once you can use the
<cite>docker containers prune</cite> command.  <strong>Be careful</strong> with this command.
If you have containers you may want to reconnect to, you should not
use this command.  It will ask you if to confirm you want to remove
these containers, see output below.  If successful it will print the
full <cite>CONTAINER ID</cite> back to you.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>prune
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>WARNING! This will remove all stopped containers.
Are you sure you want to continue? [y/N] y
Deleted Containers:
9c698655416a848278d16bb1352b97e72b7ea85884bff8f106877afe0210acfc
6dd822cf6ca92f3040eaecbd26ad2af63595f30bb7e7a20eacf4554f6ccc9b2b
</pre></div>
</div>
</section>
<section id="id1">
<h3>Removing images<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>Now that we’ve removed any potentially running or stopped containers,
we can try again to delete the <cite>hello-world</cite> <strong>image</strong>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>rm<span class="w"> </span>hello-world
</pre></div>
</div>
<p>output</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Untagged: hello-world:latest
Untagged: hello-world@sha256:5f179596a7335398b805f036f7e8561b6f0e32cd30a32f5e19d17a3cda6cc33d
Deleted: sha256:fce289e99eb9bca977dae136fbe2a82b6b7d4c372474c9235adc1741675f587e
Deleted: sha256:af0b15c8625bb1938f1d7b17081031f649fd14e6b233688eea3c5483994a66a3
</pre></div>
</div>
<p>The reason that there are a few lines of output, is that a given image
may have been formed by merging multiple underlying layers.  Any
layers that are used by multiple Docker images will only be stored
once.  Now the result of <cite>docker image ls</cite> should no longer include
the <cite>hello-world</cite> image.</p>
</section>
</section>
<span id="document-create_contain"></span><section id="creating-your-own-container-images">
<span id="create-contain"></span><h2>Creating your own container images<a class="headerlink" href="#creating-your-own-container-images" title="Link to this heading"></a></h2>
<p>There are lots of reasons why you might want to create your <strong>own</strong>
Docker image.</p>
<ul class="simple">
<li><p>You can’t find a container with all the tools you need on Docker
Hub.</p></li>
<li><p>You want to have a container to “archive” all the specific software
versions you ran for a project</p></li>
<li><p>You want to share your workflow with someone else.</p></li>
</ul>
<section id="interactive-installation">
<h3>Interactive installation<a class="headerlink" href="#interactive-installation" title="Link to this heading"></a></h3>
<p>Before creating a reproducible installation, let’s experiment with
installing software inside a container. Start the <cite>alpine</cite> container
from before, interactively:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>ubuntu<span class="w"> </span>sh
</pre></div>
</div>
<p>Because this is a basic container, there’s a lot of things not
installed – for example, <cite>python3</cite>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/#<span class="w"> </span>python3
</pre></div>
</div>
<p>Output</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>sh: 1: python3: not found
</pre></div>
</div>
<p>Inside the container, we can run commands to install Python 3. The
Ubuntu version of Linux has a installation tool called <cite>apt install</cite> that we
can use to install Python 3.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/#<span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3<span class="w"> </span>pip
</pre></div>
</div>
<p>We can test our installation by running a Python command:</p>
<div class="highlight-docker notranslate"><div class="highlight"><pre><span></span>/#<span class="w"> </span>python3<span class="w"> </span>--version
</pre></div>
</div>
<p>Once Python is installed, we can add Python packages using the pip
package installer:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/#<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>cython
</pre></div>
</div>
<p>Once we exit, these changes are not saved to a new container by
default. There is a command that will “snapshot” our changes, but
building containers this way is not very reproducible. Instead, we’re
going to take what we’ve learned from this interactive installation
and create our container from a reproducible recipe, known as a
<cite>Dockerfile</cite>.</p>
<p>If you haven’t already, exit out of the interactively running container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/#<span class="w"> </span><span class="nb">exit</span>
</pre></div>
</div>
</section>
<section id="put-installation-instructions-in-a-dockerfile">
<h3>Put installation instructions in a <cite>Dockerfile</cite><a class="headerlink" href="#put-installation-instructions-in-a-dockerfile" title="Link to this heading"></a></h3>
<p>A <cite>Dockerfile</cite> is a plain text file with keywords and commands that
can be used to create a new container image.</p>
<p>Every Dockerfile is composed of three main parts as shown below.</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">&lt;EXISTING</span><span class="w"> </span>IMAGE&gt;
<span class="k">RUN</span><span class="w"> </span>&lt;INSTALL<span class="w"> </span>CMDS<span class="w"> </span>FROM<span class="w"> </span>SHELL&gt;
<span class="k">RUN</span><span class="w"> </span>&lt;INSTALL<span class="w"> </span>CMDS<span class="w"> </span>FROM<span class="w"> </span>SHELL&gt;
<span class="k">CMD</span><span class="w"> </span>&lt;CMD<span class="w"> </span>TO<span class="w"> </span>RUN<span class="w"> </span>BY<span class="w"> </span>DEFAULT&gt;
</pre></div>
</div>
<p>Let’s break this file down:</p>
<ul class="simple">
<li><p>The first line, <cite>FROM</cite>, indicates which container we’re starting with.</p></li>
<li><p>The next two lines <cite>RUN</cite>, will indicate installation commands we
want to run. These are the same commands that we used interactively
above.</p></li>
<li><p>The last line, <cite>CMD</cite> indicates the default command we want the
container to run, if no other command is provided.</p></li>
</ul>
<div class="admonition-take-a-guess exercise important admonition" id="exercise-0">
<p class="admonition-title">Take a Guess</p>
<p>Do you have any ideas about what we should use to fill in the
sample Dockerfile to replicate the installation we did above?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Based on our experience above, edit the <cite>Dockerfile</cite> (in your
text editor of choice) to look like this:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">ubuntu:18.04</span>
<span class="k">RUN</span><span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3<span class="w"> </span>pip
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>cython
<span class="k">CMD</span><span class="w"> </span>python3<span class="w"> </span>--version
</pre></div>
</div>
</div>
</div>
<p>The recipe provided by this Dockerfile will use Ubuntu Linux as the
base container, add Python and the Cython library, and set a default
print command.</p>
</section>
<section id="create-a-new-docker-image">
<h3>Create a new Docker image<a class="headerlink" href="#create-a-new-docker-image" title="Link to this heading"></a></h3>
<p>So far, we just have a file. We want Docker to take this file, run the
install commands inside, and then save the resulting container as a
new container image. To do this we will use the <cite>docker build</cite>
command.</p>
<p>We have to provide <cite>docker build</cite> with two pieces of information:</p>
<ul class="simple">
<li><p>the location of the <cite>Dockerfile</cite></p></li>
<li><p>the name of the new image. Remember the naming scheme from before?
You should name your new image with your Docker Hub username and a
name for the container, like this: <code class="docutils literal notranslate"><span class="pre">USERNAME/CONTAINERNAME</span></code></p></li>
</ul>
<p>All together, the build command will look like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>USERNAME/CONTAINERNAME<span class="w"> </span>.
</pre></div>
</div>
<p>The <cite>-t</cite> option names the container; the final dot indicates that the
<cite>Dockerfile</cite> is in our current directory.</p>
<p>For example, if my user name was <cite>alice</cite> and I wanted to call my
image <cite>alpine-python</cite>, I would use this command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>alice/ubuntu-python<span class="w"> </span>.
</pre></div>
</div>
<div class="admonition-review exercise important admonition" id="exercise-1">
<p class="admonition-title">Review!</p>
<ol class="arabic simple">
<li><p>Think back to earlier. What command can you run to check if
your image was created successfully? (Hint: what command shows
the images on your computer?)</p></li>
<li><p>We didn’t specify a tag for our image name. What did
Docker automatically use?</p></li>
<li><p>What command will run the container you’ve created? What
should happen by default if you run the container? Can you make
it do something different, like print “hello world”?</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<ol class="arabic">
<li><p>To see your new image, run <cite>docker image ls</cite>. You should
see the name of your new image under the “REPOSITORY” heading.</p></li>
<li><p>In the output of <cite>docker image ls</cite>, you can see that
Docker has automatically used the <cite>latest</cite> tag for our new
image.</p></li>
<li><p>We want to use <cite>docker run</cite> to run the container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>alice/ubuntu-python
</pre></div>
</div>
<p>should run the container and print out our default
message, including the version of Linux and Python.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>alice/ubuntu-python<span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Hello World&quot;</span>
</pre></div>
</div>
<p>will run the container and print out “Hello world” instead.</p>
</li>
</ol>
</div>
</div>
<p>While it may not look like you have achieved much, you have already
effected the combination of a lightweight Linux operating system with
your specification to run a given command that can operate reliably on
macOS, Microsoft Windows, Linux and on the cloud!</p>
</section>
<section id="share-your-new-container-on-docker-hub">
<h3>Share your new container on Docker Hub<a class="headerlink" href="#share-your-new-container-on-docker-hub" title="Link to this heading"></a></h3>
<p>Images that you release publicly can be stored on the Docker Hub for
free.  If you name your image as described above, with your Docker Hub
username, all you need to do is run the opposite of <cite>docker pull</cite> –
<cite>docker push</cite>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>push<span class="w"> </span>alice/ubuntu-python
</pre></div>
</div>
<p>Make sure to substitute the full name of your container!</p>
<p>In a web browser, open &lt;<a class="reference external" href="https://hub.docker.com">https://hub.docker.com</a>&gt;, and on your user page
you should now see your container listed, for anyone to use or build
on.</p>
<div class="admonition-logging-in callout admonition" id="callout-0">
<p class="admonition-title">Logging In</p>
<p>Technically, you have to be logged into Docker on your computer for
this to work.  Usually it happens by default, but if <cite>docker push</cite>
doesn’t work for you, run <cite>docker login</cite> first, enter your Docker
Hub username and password, and then try <cite>docker push</cite> again.</p>
</div>
<p>You can rename images using the <cite>docker tag</cite> command. For example,
imagine someone named Alice has been working on a workflow container
and called it <cite>workflow-test</cite> on her own computer. She now wants to
share it in her <cite>alice</cite> Docker Hub account with the name
<cite>workflow-complete</cite> and a tag of <cite>v1</cite>. Her <cite>docker tag</cite> command would
look like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>tag<span class="w"> </span>workflow-test<span class="w"> </span>alice/workflow-complete:v1
</pre></div>
</div>
<p>She could then push the re-named container to Docker Hub, using
<cite>docker push alice/workflow-complete:v1</cite></p>
</section>
</section>
<span id="document-compx_contain"></span><section id="creating-more-complex-container-images">
<span id="compx-contain"></span><h2>Creating More Complex Container Images<a class="headerlink" href="#creating-more-complex-container-images" title="Link to this heading"></a></h2>
<p>In order to create and use your own containers, you may need more
information than our previous example. You may want to use files from
outside the container, copy those files into the container, and just
generally learn a little bit about software installation. This episode
will cover these. Note that the examples will get gradually more and
more complex - most day-to-day use of containers can be accomplished
using the first 1-2 sections on this page.</p>
<section id="using-scripts-and-files-from-outside-the-container">
<h3>Using scripts and files from outside the container<a class="headerlink" href="#using-scripts-and-files-from-outside-the-container" title="Link to this heading"></a></h3>
<p>Let’s create a file and folder called it <code class="docutils literal notranslate"><span class="pre">foo/dummy.py</span></code> in the root
folder.</p>
<p>Please copy the <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code> and place it in the <code class="docutils literal notranslate"><span class="pre">foo</span></code> directory.
Let’s say we wanted to try running the script using our recently
created <code class="docutils literal notranslate"><span class="pre">ubuntu-python</span></code> container.</p>
<div class="admonition-running-containers callout admonition" id="callout-0">
<p class="admonition-title">Running containers</p>
<p>What command would we use to run python from the <code class="docutils literal notranslate"><span class="pre">alpine-python</span></code>
container?</p>
</div>
<p>If we try running the container and Python script, what happens?</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>alice/ubuntu-python<span class="w"> </span>python3<span class="w"> </span>dummy.py
</pre></div>
</div>
<p>Output</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python3: can’t open file ‘dummy.py’: [Errno 2] No such file or directory
</pre></div>
</div>
<div class="admonition-no-such-file-or-directory callout admonition" id="callout-1">
<p class="admonition-title">No such file or directory</p>
<p>What does the error message mean? Why might the Python inside the
container not be able to find or open our script?</p>
</div>
<p>The problem here is that the container and its file system is separate
from our host computer’s file system. When the container runs, it can’t
see anything outside itself, including any of the files on our computer.
In order to use Python (inside the container) and our script (outside
the container, on our computer), we need to create a link between the
directory on our computer and the container.</p>
<p>This link is called a “mount” and is what happens automatically when a
USB drive or other external hard drive gets connected to a computer -
you can see the contents appear as if they were on your computer.</p>
<p>We can create a mount between our computer and the running container by
using an additional option to <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code>. We’ll also use the
variable <code class="docutils literal notranslate"><span class="pre">$PWD</span></code> which will substitute in our current working
directory. The option will look like this</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-v<span class="w"> </span><span class="nv">$PWD</span>:/temp
</pre></div>
</div>
<p>What this means is – link my current directory with the container, and
inside the container, name the directory <code class="docutils literal notranslate"><span class="pre">/temp</span></code></p>
<p>Let’s try running the command now:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-v<span class="w"> </span><span class="nv">$PWD</span>:/temp<span class="w"> </span>alice/ubuntu-python<span class="w"> </span>python3<span class="w"> </span>dummy.py
</pre></div>
</div>
<p>But we get the same error!</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python3: can&#39;t open file &#39;dummy.py&#39;: [Errno 2] No such file or directory
</pre></div>
</div>
<p>This final piece is a bit tricky – we really have to remember to put
ourselves inside the container. Where is the <cite>dummy.py</cite> file? It’s in
the directory that’s been mapped to <cite>/temp</cite> – so we need to include
that in the path to the script. This command should give us what we
need:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-v<span class="w"> </span><span class="nv">$PWD</span>:/temp<span class="w"> </span>alice/alpine-python<span class="w"> </span>python3<span class="w"> </span>/temp/dummy.py
</pre></div>
</div>
<p>Note that if we create any files in the <cite>/temp</cite> directory while the
container is running, these files will appear on our host filesystem
in the original directory and will stay there even when the container
stops.</p>
<div class="admonition-checking-the-options-interactive-jobs exercise important admonition" id="exercise-0">
<p class="admonition-title">Checking the options, Interactive jobs</p>
<ol class="arabic simple">
<li><p>Can you go through each piece of the Docker command above
the explain what it does? How would you characterize the
key components of a Docker command?</p></li>
<li><p>Try using the directory mount option but run the container
interactively. Can you find the folder that’s connected to
your computer? What’s inside?</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<ol class="arabic">
<li><p>Here’s a breakdown of each piece of the command above</p>
<ul class="simple">
<li><p><cite>docker run</cite>: use Docker to run a container</p></li>
<li><p><cite>-v $PWD:/temp</cite>: connect my current working directory
(<cite>$PWD</cite>) as a folder inside the container called <cite>/temp</cite></p></li>
<li><p><cite>alice/ubuntu-python</cite>: name of the container to run</p></li>
<li><p><cite>python3 /temp/dummy.py</cite>: what commands to run in the container</p></li>
</ul>
<p>More generally, every Docker command will have the form:
<cite>docker [action] [docker options] [docker image] [command
to run inside]</cite></p>
</li>
<li><p>The docker command to run the container interactively is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-v<span class="w"> </span><span class="nv">$PWD</span>:/temp<span class="w"> </span>-it<span class="w"> </span>alice/ubuntu-python<span class="w"> </span>sh
</pre></div>
</div>
<p>Once inside, you should be able to navigate to the <cite>/temp</cite>
folder and see that’s contents are the same as the files on your
computer:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/#<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/temp
/#<span class="w"> </span>ls
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<p>Mounting a folder can be very useful when you want to run the software
inside your container on many different input files. In other
situations, you may want to save or archive an authoritative version
of your data by adding it to the container permanently.  That’s what
we will cover next.</p>
</section>
<section id="including-personal-scripts-and-data-in-a-container">
<h3>Including personal scripts and data in a container<a class="headerlink" href="#including-personal-scripts-and-data-in-a-container" title="Link to this heading"></a></h3>
<p>Our next project will be to add our own files to a container -
something you might want to do if you’re sharing a finished analysis
or just want to have an archived copy of your entire analysis
including the data. Let’s as some that we’ve finished with our
<cite>dummy.py</cite> script and want to add it to the container itself.</p>
<p>In your shell, you should still be in the <cite>dummy</cite> folder in the
<cite>docker-intro</cite> folder.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">pwd</span>
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/Users/yourname/foo
</pre></div>
</div>
<p>We will modify our Dockerfile again to build an image based on Alpine
Linux with Python 3 installed (just as we did perviously). This time
we will add an additional line before the <cite>CMD</cite> line:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">COPY</span><span class="w"> </span>dummy.py<span class="w"> </span>/home
</pre></div>
</div>
<p>This line will cause Docker to copy the file from your computer into
the container’s file system <em>at build time</em>. Modify the Dockerfile as
before (or copy the version from the <cite>basic/</cite> subdirectory) and add
the extra copy line. Once you have done that, build the container like
before, but give it a different name:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">build</span> <span class="o">-</span><span class="n">t</span> <span class="n">alice</span><span class="o">/</span><span class="n">ubuntu</span><span class="o">-</span><span class="n">dummy</span> <span class="o">.</span>
</pre></div>
</div>
<div class="admonition-did-it-work exercise important admonition" id="exercise-1">
<p class="admonition-title">Did it work?</p>
<p>Can you remember how to run a container interactively? Try
that with this one.  Once inside, try running the Python script.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>You can start the container interactively like so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>alice/ubuntu-dummy<span class="w"> </span>sh
</pre></div>
</div>
<p>You should be able to run the python command inside the
container like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/#<span class="w"> </span>python3<span class="w"> </span>/home/dummy.py
</pre></div>
</div>
</div>
</div>
<p>This <cite>COPY</cite> keyword can be used to place your own scripts or own data
into a container that you want to publish or use as a record. Note
that it’s not necessarily a good idea to put your scripts inside the
container if you’re constantly changing or editing them.  Then,
referencing the scripts from outside the container is a good idea, as
we did in the previous section. You also want to think carefully about
size – if you run <cite>docker image ls</cite> you’ll see the size of each image
all the way on the right of the screen. The bigger your image becomes,
the harder it will be to easily download.</p>
<div class="admonition-copying-alternatives callout admonition" id="callout-2">
<p class="admonition-title">Copying alternatives</p>
<p>Another trick for getting your own files into a container is by
using the <cite>RUN</cite> keyword and downloading the files from the
internet. For example, if your code is in a GitHub repository, you
could include this statement in your Dockerfile to download the
latest version every time you build the container:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">RUN</span><span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/alice/mycode
</pre></div>
</div>
<p>Similarly, the <cite>wget</cite> command can be used to download any file
publicly available on the internet:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">RUN</span><span class="w"> </span>wget<span class="w"> </span>ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/2.10.0/ncbi-blast-2.10.0+-x64-linux.tar.gz
</pre></div>
</div>
</div>
</section>
<section id="more-fancy-dockerfile-options">
<h3>More fancy <cite>Dockerfile</cite> options<a class="headerlink" href="#more-fancy-dockerfile-options" title="Link to this heading"></a></h3>
<p>We can expand on the example above to make our container even more
“automatic”.  Here are some ideas:</p>
<p>Make the <cite>dummy.py</cite> script run automatically:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">ubuntu:18.04</span>

<span class="k">COPY</span><span class="w"> </span>dummy.py<span class="w"> </span>/home
<span class="k">RUN</span><span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3

<span class="c"># Run the dummy.py script as the default command</span>
<span class="k">CMD</span><span class="w"> </span>python3<span class="w"> </span>/home/dummy.py
<span class="c"># OR</span>
<span class="c"># CMD [&quot;python3&quot;, &quot;/home/dummy.py&quot;]</span>
</pre></div>
</div>
<p>Build and test it:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>ubuntu-dummy:v1<span class="w"> </span>.
docker<span class="w"> </span>run<span class="w"> </span>ubuntu-dummy:v1
</pre></div>
</div>
<p>Make the <cite>dummy.py</cite> script run automatically with arguments from the
command line:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">ubuntu:18.04</span>

<span class="k">COPY</span><span class="w"> </span>dummy.py<span class="w"> </span>/home
<span class="k">RUN</span><span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3

<span class="c"># Run the dummy.py script as the default command and</span>
<span class="c"># allow people to enter arguments for it</span>
<span class="k">ENTRYPOINT</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;python3&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;/home/dummy.py&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Build and test it:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>ubuntu-dummy:v2<span class="w"> </span>.
docker<span class="w"> </span>run<span class="w"> </span>ubuntu-dummy:v2
</pre></div>
</div>
<p>Add the <cite>dummy.py</cite> script to the <cite>PATH</cite> so you can run it directly:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">ubuntu:18.04</span>

<span class="k">COPY</span><span class="w"> </span>dummy.py<span class="w"> </span>/home
<span class="c"># set script permissions</span>
<span class="k">RUN</span><span class="w"> </span>chmod<span class="w"> </span>+x<span class="w"> </span>/home/dummy.py
<span class="c"># add /home folder to the PATH</span>
<span class="k">ENV</span><span class="w"> </span>PATH<span class="w"> </span>/home:<span class="nv">$PATH</span>

<span class="k">RUN</span><span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3
</pre></div>
</div>
<p>Build and test it:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>ubuntu-dummy:v3<span class="w"> </span>.
docker<span class="w"> </span>run<span class="w"> </span>alpine-dummy:v3<span class="w"> </span>dummy.py
</pre></div>
</div>
</section>
</section>
<span id="document-singlrty_start"></span><section id="what-is-singularity">
<span id="singlrty-start"></span><h2>What is Singularity?<a class="headerlink" href="#what-is-singularity" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://sylabs.io/singularity/">Singularity</a> is another container
platform. In some ways it appears similar to Docker from a user
perspective, but in others, particularly in the system’s architecture,
it is fundamentally different. These differences mean that Singularity
is particularly well-suited to running on distributed, High
Performance Computing (HPC) infrastructure, as well as a Linux laptop
or desktop!</p>
<p>System administrators will not, generally, install Docker on shared
computing platforms such as lab desktops, research clusters or HPC
platforms because the design of Docker presents potential security
issues for shared platforms with multiple users. Singularity, on the
other hand, can be run by end-users entirely within “user space”, that
is, no special administrative privileges need to be assigned to a user
in order for them to run and interact with containers on a platform
where Singularity has been installed.</p>
<section id="getting-started-with-singularity">
<h3>Getting started with Singularity<a class="headerlink" href="#getting-started-with-singularity" title="Link to this heading"></a></h3>
<p>Initially developed within the research community, Singularity is open
source and the <a class="reference external" href="https://github.com/hpcng/singularity">repository</a> is
currently available in the “<a class="reference external" href="https://github.com/hpcng">The Next Generation of High Performance
Computing</a>” GitHub organisations.  Part I
of this Singularity material is intended to be undertaken on a remote
platform where Singularity has been pre-installed.</p>
<p>If you’re attending a taught version of this course, you will be
provided with access details for a remote platform made available to
you for use for Part I of the Singularity material. This platform will
have the Singularity software pre-installed.</p>
<div class="admonition-installing-singularity-on-your-own-laptop-desktop callout admonition" id="callout-0">
<p class="admonition-title">Installing Singularity on your own laptop/desktop</p>
<p>If you have a Linux system on which you have administrator access
and you can install Singularity locally on this system.</p>
</div>
<p>Sign in to the Vega system that you’ve been provided with access to.
Check that the <cite>singularity</cite> command is available in your terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>--version
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>SingularityPRO<span class="w"> </span>version<span class="w"> </span><span class="m">3</span>.7-5.el8
</pre></div>
</div>
<p>Depending on the version of Singularity installed on your system, you
may see a different version.</p>
<div class="admonition-singularity-on-hpc-systems-loading-a-module callout admonition" id="callout-1">
<p class="admonition-title">Singularity on HPC systems: Loading a module</p>
<p>HPC systems often use <em>modules</em> to provide access to software on the
system.  If you get a command not found error (e.g. <cite>bash:
singularity: command not found</cite> or similar) you may need to load the
<em>singularity module</em> before you can use the <cite>singularity</cite> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>singularity
</pre></div>
</div>
</div>
<p>Nonetheless, for the Vega system <cite>singularity</cite> is readily available.</p>
</section>
<section id="images-and-containers">
<h3>Images and containers<a class="headerlink" href="#images-and-containers" title="Link to this heading"></a></h3>
<p>We’ll start with a brief note on the terminology used in this section
of the course.  We refer to both <strong>images</strong> and <strong>containers</strong>. What
is the distinction between these two terms?</p>
<p><strong>Images</strong> are bundles of files including an operating system,
software and potentially data and other application-related
files. They may sometimes be referred to as a disk image or container
image and they may be stored in different ways, perhaps as a single
file, or as a group of files.  Either way, we refer to this file, or
collection of files, as an image.</p>
<p>A <strong>container</strong> is a virtual environment that is based on an
image. That is, the files, applications, tools, etc that are available
within a running container are determined by the image that the
container is started from. It may be possible to start multiple
container instances from an image. You could, perhaps, consider an
image to be a form of template from which running container instances
can be started.</p>
</section>
<section id="getting-an-image-and-running-a-singularity-container">
<h3>Getting an image and running a Singularity container<a class="headerlink" href="#getting-an-image-and-running-a-singularity-container" title="Link to this heading"></a></h3>
<p>If you recall from learning about Docker, Docker images are formed of
a set of layers that make up the complete image. When you pull a
Docker image from Docker Hub, you see the different layers being
downloaded to your system. They are stored in your local Docker
repository on your system and you can see details of the available
images using the <cite>docker</cite> command.</p>
<p>Singularity images are a little different. Singularity uses the
<a class="reference external" href="https://github.com/sylabs/sif">Signularity Image Format (SIF)</a> and
images are provided as single <cite>SIF</cite> files. Singularity images can be
pulled from <a class="reference external" href="https://singularity-hub.org/">Singularity Hub</a>, a
registry for container images. Singularity is also capable of running
containers based on images pulled from <a class="reference external" href="https://hub.docker.com/">Docker Hub</a> and some other sources. We’ll look at
accessing containers from Docker Hub later in the Singularity
material.</p>
<div class="admonition-singularity-hub callout admonition" id="callout-2">
<p class="admonition-title">Singularity Hub</p>
<p>Note that in addition to providing a repository that you can pull
images from, <a class="reference external" href="https://singularity-hub.org/">Singularity Hub</a> can
also build Singularity images for you from a <cite>recipe</cite> - a
configuration file defining the steps to build an image.  We’ll look
at recipes and building images later.</p>
</div>
<p>Let’s begin by creating a <cite>test</cite> directory, changing into it and
pulling a test Hello World image from Singularity Hub:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span><span class="nb">test</span>
<span class="nb">cd</span><span class="w"> </span><span class="nb">test</span>
singularity<span class="w"> </span>pull<span class="w"> </span>hello-world.sif<span class="w"> </span>shub://vsoch/hello-world
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>What is the main difference between above and the Docker pull request?</p>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>INFO:<span class="w">    </span>Downloading<span class="w"> </span>shub<span class="w"> </span>image
<span class="m">59</span>.75<span class="w"> </span>MiB<span class="w"> </span>/<span class="w"> </span><span class="m">59</span>.75<span class="w"> </span>MiB<span class="w"> </span><span class="o">[=====================================================================]</span><span class="w"> </span><span class="m">100</span>.00%<span class="w"> </span><span class="m">52</span>.03<span class="w"> </span>MiB/s<span class="w"> </span>1s
</pre></div>
</div>
<p>What just happened?! We pulled a SIF image from Singularity Hub using
the <cite>singularity pull</cite> command and directed it to store the image file
using the name <cite>hello-world.sif</cite>. If you run the <cite>ls</cite> command, you
should see that the <cite>hello-world.sif</cite> file is now in your current
directory. This is our image and we can now run a container based on
this image:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>run<span class="w"> </span>hello-world.sif
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>RaawwWWWWWRRRR!!<span class="w"> </span>Avocado!
</pre></div>
</div>
<p>The above command ran the hello-world container from the image we
downloaded from Singularity Hub and the resulting output was shown.</p>
<p>How did the container determine what to do when we ran it?! What did
running the container actually do to result in the displayed output?</p>
<p>When you run a container from an image without using any additional
command line arguments, the container runs the default run script that
is embedded within the image. This is a shell script that can be used
to run commands, tools or applications stored within the image on
container startup. We can inspect the image’s run script using the
<cite>singularity inspect</cite> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>inspect<span class="w"> </span>-r<span class="w"> </span>hello-world.sif
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/sh</span>

<span class="nb">exec</span><span class="w"> </span>/bin/bash<span class="w"> </span>/rawr.sh
</pre></div>
</div>
<p>This shows us the script within the <cite>hello-world.sif</cite> image configured
to run by default when we use the <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">run</span></code> command.</p>
<p>That concludes this introductory Singularity episode. The next episode
looks in more detail at running containers.</p>
</section>
</section>
<span id="document-work_contain"></span><section id="working-with-singularity-containers">
<span id="work-contain"></span><h2>Working with Singularity containers<a class="headerlink" href="#working-with-singularity-containers" title="Link to this heading"></a></h2>
<section id="singularity-s-image-cache">
<h3>Singularity’s image cache<a class="headerlink" href="#singularity-s-image-cache" title="Link to this heading"></a></h3>
<p>While Singularity doesn’t have a local image repository in the same
way as Docker, it does cache downloaded image files. As we saw in the
previous episode, images are simply <cite>.sif</cite> files stored on your local
disk.</p>
<p>If you delete a local <cite>.sif</cite> image that you have pulled from a remote
image repository and then pull it again, if the image is unchanged
from the version you previously pulled, you will be given a copy of
the image file from your local cache rather than the image being
downloaded again from the remote source. This removes unnecessary
network transfers and is particularly useful for large images which
may take some time to transfer over the network.  To demonstrate this,
remove the <cite>hello-world.sif</cite> file stored in your <cite>test</cite> directory and
then issue the <cite>pull</cite> command again:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rm<span class="w"> </span>hello-world.sif
singularity<span class="w"> </span>pull<span class="w"> </span>hello-world.sif<span class="w"> </span>shub://vsoch/hello-world
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>INFO:<span class="w">    </span>Use<span class="w"> </span>image<span class="w"> </span>from<span class="w"> </span>cache
</pre></div>
</div>
<p>As we can see in the above output, the image has been returned from
the cache and we don’t see the output that we saw previously showing
the image being downloaded from Singularity Hub.</p>
<p>How do we know what is stored in the local cache? We can find out
using the <cite>singularity cache</cite> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>cache<span class="w"> </span>list
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>There<span class="w"> </span>are<span class="w"> </span><span class="m">1</span><span class="w"> </span>container<span class="w"> </span>file<span class="o">(</span>s<span class="o">)</span><span class="w"> </span>using<span class="w"> </span><span class="m">62</span>.65<span class="w"> </span>MB<span class="w"> </span>and<span class="w"> </span><span class="m">0</span><span class="w"> </span>oci<span class="w"> </span>blob<span class="w"> </span>file<span class="o">(</span>s<span class="o">)</span>
using<span class="w"> </span><span class="m">0</span>.00<span class="w"> </span>kB<span class="w"> </span>of<span class="w"> </span>space<span class="w"> </span>Total<span class="w"> </span>space<span class="w"> </span>used:<span class="w"> </span><span class="m">62</span>.65<span class="w"> </span>MB
</pre></div>
</div>
<p>This tells us how many container files are stored in the cache and how
much disk space the cache is using but it doesn’t tell us <strong>what</strong> is
actually being stored. To find out more information we can add the
<cite>-v</cite> verbose flag to the <cite>list</cite> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>cache<span class="w"> </span>list<span class="w"> </span>-v
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>NAME<span class="w">                     </span>DATE<span class="w"> </span>CREATED<span class="w">           </span>SIZE<span class="w">             </span>TYPE
hello-world_latest.sif<span class="w">   </span><span class="m">2020</span>-04-03<span class="w"> </span><span class="m">13</span>:20:44<span class="w">    </span><span class="m">62</span>.65<span class="w"> </span>MB<span class="w">         </span>shub

There<span class="w"> </span>are<span class="w"> </span><span class="m">1</span><span class="w"> </span>container<span class="w"> </span>file<span class="o">(</span>s<span class="o">)</span><span class="w"> </span>using<span class="w"> </span><span class="m">62</span>.65<span class="w"> </span>MB<span class="w"> </span>and<span class="w"> </span><span class="m">0</span><span class="w"> </span>oci<span class="w"> </span>blob<span class="w"> </span>file<span class="o">(</span>s<span class="o">)</span><span class="w"> </span>using<span class="w"> </span><span class="m">0</span>.00<span class="w"> </span>kB<span class="w"> </span>of<span class="w"> </span>space
Total<span class="w"> </span>space<span class="w"> </span>used:<span class="w"> </span><span class="m">62</span>.65<span class="w"> </span>MB
</pre></div>
</div>
<p>This provides us with some more useful information about the actual
images stored in the cache.  In the <cite>TYPE</cite> column we can see that our
image type is <cite>shub</cite> because it’s a <cite>SIF</cite> image that has been pulled
from Singularity Hub.</p>
<div class="admonition-cleaning-the-singularity-image-cache callout admonition" id="callout-0">
<p class="admonition-title">Cleaning the Singularity image cache</p>
<p>We can remove images from the cache using the <cite>singularity cache
clean</cite> command.  Running the command without any options will
display a warning and ask you to confirm that you want to remove
everything from your cache. You can also remove specific images or
all images of a particular type. Look at the output of <cite>singularity
cache clean –help</cite> for more information.</p>
</div>
<div class="admonition-basic-exercise-clearing-specific-image-types-from-the-cache exercise important admonition" id="exercise-0">
<p class="admonition-title">Basic exercise: Clearing specific image types from the cache</p>
<p>What command would you use to remove only images of type
<strong>shub</strong> from your local Singularity image cache?  How could you
test this safely to ensure your command is going to do the right
thing?</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can see exactly what would be deleted by using the <code class="docutils literal notranslate"><span class="pre">--dry-run</span></code> option or <code class="docutils literal notranslate"><span class="pre">-n</span></code>.</p>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>cache<span class="w"> </span>clean<span class="w"> </span>--type<span class="o">=</span>shub
singularity<span class="w"> </span>cache<span class="w"> </span>clean<span class="w"> </span>-n<span class="w"> </span>--type<span class="o">=</span>shub
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Removing<span class="w"> </span>/&lt;cache_dir&gt;/.singularity/cache/shub
</pre></div>
</div>
</div>
</div>
<section id="working-with-containers">
<h4>Working with containers<a class="headerlink" href="#working-with-containers" title="Link to this heading"></a></h4>
<section id="running-specific-commands-within-a-container">
<h5>Running specific commands within a container<a class="headerlink" href="#running-specific-commands-within-a-container" title="Link to this heading"></a></h5>
<p>We saw earlier that we can use the <cite>singularity inspect</cite> command to
see the run script that a container is configured to run by
default. What if we want to run a different command within a
container, or we want to open a shell within a container that we can
interact with?</p>
<p>If we know the path of an executable that we want to run within a
container, we can use the <cite>singularity exec</cite> command. For example,
using the <cite>hello-world.sif</cite> container that we’ve already pulled from
Singularity Hub, we can run the following within the <cite>test</cite> directory
where the <cite>hello-world.sif</cite> file is located:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>hello-world.sif<span class="w"> </span>/bin/echo<span class="w"> </span>Hello<span class="w"> </span>World!
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Hello<span class="w"> </span>World!
</pre></div>
</div>
<p>Here we see that a container has been started from the
<cite>hello-world.sif</cite> image and the <cite>/bin/echo</cite> command has been run
within the container, passing the input <cite>Hello World!</cite>.  The command
has echoed the provided input to the console and the container has
terminated.</p>
<div class="admonition-basic-exercise-running-a-different-command-within-the-hello-world-container exercise important admonition" id="exercise-1">
<p class="admonition-title">Basic exercise: Running a different command within the
           “hello-world” container</p>
<p>Can you run a container based on the <cite>hello-world.sif</cite> image that
<strong>prints the current date and time</strong>?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>hello-world.sif<span class="w"> </span>/bin/date
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Fri<span class="w"> </span>Jun<span class="w"> </span><span class="m">26</span><span class="w"> </span><span class="m">15</span>:17:44<span class="w"> </span>BST<span class="w"> </span><span class="m">2020</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="running-a-shell-within-a-container">
<h5>Running a shell within a container<a class="headerlink" href="#running-a-shell-within-a-container" title="Link to this heading"></a></h5>
<p>If you want to open an interactive shell within a container,
Singularity provides the <cite>singularity shell</cite> command. Again, using the
<cite>hello-world.sif</cite> image, and within our <cite>test</cite> directory, we can run a
shell within a container from the hello-world image:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>shell<span class="w"> </span>hello-world.sif
</pre></div>
</div>
<p>Output</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Singularity&gt;<span class="w"> </span>whoami
<span class="o">[</span>&lt;your<span class="w"> </span>username&gt;<span class="o">]</span>
Singularity&gt;<span class="w"> </span>ls
hello-world.sif
Singularity&gt;
</pre></div>
</div>
<p>As shown above, we have opened a shell in a new container started from
the <cite>hello-world.sif</cite> image.</p>
<div class="admonition-running-a-shell-inside-a-singularity-container callout admonition" id="callout-1">
<p class="admonition-title">Running a shell inside a Singularity container</p>
<p>Q: What do you notice about the output of the above commands
entered within the Singularity container shell?</p>
<p>Q: Does this differ from what you might see within a Docker container?</p>
</div>
<p>Use the <cite>exit</cite> command to exit from the container shell.</p>
</section>
<section id="users-files-and-directories-within-a-singularity-container">
<h5>Users, files and directories within a Singularity container<a class="headerlink" href="#users-files-and-directories-within-a-singularity-container" title="Link to this heading"></a></h5>
<p>The first thing to note is that when you run <cite>whoami</cite> within the
container you should see the username that you are signed in as on the
host system when you run the container.  For example, if my username
is <cite>jc1000</cite>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>shell<span class="w"> </span>hello-world.sif

Singularity&gt;<span class="w"> </span>whoami
jc1000
</pre></div>
</div>
<p>But hang on! I downloaded the standard, public version of the
<cite>hello-world.sif</cite> image from Singularity Hub. I haven’t customised it
in any way. How is it configured with my own user details?!</p>
<p>If you have any familiarity with Linux system administration, you may
be aware that in Linux, users and their Unix groups are configured in
the <cite>/etc/passwd</cite> and <cite>/etc/group</cite> files respectively.  In order for
the shell within the container to know of my user, the relevant user
information needs to be available within these files within the
container.</p>
<p>Assuming this feature is enabled on your system, when the container is
started, Singularity appends the relevant user and group lines from
the host system to the <cite>/etc/passwd</cite> and <cite>/etc/group</cite> files within the
container <a class="reference external" href="https://www.intel.com/content/dam/www/public/us/en/documents/presentation/hpc-containers-singularity-advanced.pdf">[1]</a>.</p>
<p>Singularity also <strong>binds</strong> some directories from the host system where
you are running the <cite>singularity</cite> command into the container that
you’re starting. Note that this bind process isn’t copying files into
the running container, it is simply making an existing directory on
the host system visible and accessible within the container
environment. If you write files to this directory within the running
container, when the container shuts down, those changes will persist
in the relevant location on the host system.</p>
<p>There is a default configuration of which files and directories are
bound into the container but ultimate control of how things are set up
on the system where you’re running Singularity is determined by the
system administrator.  As a result, this section provides an overview
but you may find that things are a little different on the system that
you’re running on.</p>
<p>One directory that is likely to be accessible within a container that
you start is your <strong>home directory</strong>.  The mapping of file content and
directories from a host system into a Singularity container is
illustrated in the example below showing a subset of the directories
on the host Linux system and in a Singularity container:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Host system:                                                      Singularity container:
-------------                                                     ----------------------
/                                                                 /
├── bin                                                           ├── bin
├── etc                                                           ├── etc
│   ├── ...                                                       │   ├── ...
│   ├── group  ─&gt; user&#39;s group added to group file in container ─&gt;│   ├── group
│   └── passwd ──&gt; user info added to passwd file in container ──&gt;│   └── passwd
├── home                                                          ├── usr
│   └── jc1000 ───&gt; user home directory made available ──&gt; ─┐     ├── sbin
├── usr                 in container via bind mount         │     ├── home
├── sbin                                                    └────────&gt;└── jc1000
└── ...                                                           └── ...
</pre></div>
</div>
<div class="admonition-questions-and-exercises-files-in-singularity-containers exercise important admonition" id="exercise-2">
<p class="admonition-title">Questions and exercises: Files in Singularity containers</p>
<p><strong>Q1:</strong> What do you notice about the ownership of files in a
container started from the hello-world image?  (e.g. take a look
at the ownership of files in the root directory (<cite>/</cite>))</p>
<p><strong>Exercise 1:</strong> In this container, try editing (for example
using the editor <cite>vi</cite> which should be avaiable in the container)
the <cite>/rawr.sh</cite> file. What do you notice?</p>
<p>If you’re not familiar with <cite>vi</cite> there are many quick
reference pages online showing  the main commands for using the editor,
for example <a class="reference external" href="http://web.mit.edu/merolish/Public/vi-ref.pdf">this one</a></p>
<p><strong>Exercise 2:</strong> In your home directory within the container
shell, try and create a simple text file. Is it possible to
do this? If so, why? If not, why not?! If you can  successfully create
a file, what happens to it when you exit the shell and the container
shuts down?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<p><strong>A1:</strong> Use the <cite>ls -l</cite> command to see a detailed file
listing including file ownership and permission details.  You may
see that all the files are owned by you, alternatively, most files
in the root (<cite>/</cite>) directory may be owned by the <cite>root</cite> user. If
the files are owned by you, this looks good - you should be ready
to edit something in the exercise that follows…otherwise, if the
files are owned by root, maybe not…</p>
<p><strong>Exercise 1:</strong> Unfortunately, it’s not so easy, depending on how
you tried to edit <cite>/rawr.sh</cite> you probably saw an error similar to
the following: <cite>Can’t open file for writing</cite> or <cite>Read-only file
system</cite></p>
<p><strong>Exercise 2:</strong> Within your home directory, you <strong>should</strong> be able
to successfully create a file. Since you’re seeing your home
directory on the host system which has been bound into the
container, when you exit and the container shuts down, the file
that you created within the container should still be present when
you look at your home directory on the host system.</p>
</div>
</div>
</section>
</section>
</section>
<section id="using-docker-images-with-singularity">
<h3>Using Docker images with Singularity<a class="headerlink" href="#using-docker-images-with-singularity" title="Link to this heading"></a></h3>
<p>Singularity can also start containers from Docker images, opening up
access to a huge number of existing container images available on
<a class="reference external" href="https://hub.docker.com/">Docker Hub</a> and other registries.</p>
<p>While Singularity doesn’t support running Docker images directly, it
can pull them from Docker Hub and convert them into a suitable format
for running via Singularity. When you pull a Docker image, Singularity
pulls the slices or <strong>layers</strong> that make up the Docker image and
converts them into a single-file Singularity SIF image.</p>
<p>For example, moving on from the simple <strong>Hello World</strong> examples that
we’ve looked at so far, let’s pull one of the <a class="reference external" href="https://hub.docker.com/_/python">official Docker Python
images</a>. We’ll use the image with
the tag <cite>3.8.6-slim-buster</cite> which has Python 3.8.6 installed on
Debian’s <a class="reference external" href="https://www.debian.org/releases/buster/">Buster</a> (v10)
Linux distribution:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>pull<span class="w"> </span>python-3.8.6.sif<span class="w"> </span>docker://python:3.8.6-slim-buster
</pre></div>
</div>
<p>Output</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
Getting image source signatures
Copying blob 852e50cd189d done
Copying blob 334ed303e4ad done
Copying blob a687a65725ea done
Copying blob fe607cb30fbe done
Copying blob b8a3bc0a3645 done
Copying config 08d8e312de done
Writing manifest to image destination
Storing signatures
2020/12/07 18:36:18  info unpack layer: sha256:852e50cd189dfeb54d97680d9fa6bed21a6d7d18cfb56d6abfe2de9d7f173795
2020/12/07 18:36:19  info unpack layer: sha256:334ed303e4ad2f8dc872f2e845d79012ad648eaced444e009ae9a397cc4b4dbb
2020/12/07 18:36:19  info unpack layer: sha256:a687a65725ea883366a61d24db0f946ad384aea893297d9510e50fa13f565539
2020/12/07 18:36:19  info unpack layer: sha256:fe607cb30fbe1148b5885d58c909d0c08cbf2c0848cc871845112f3ee0a0f9ba
2020/12/07 18:36:19  info unpack layer: sha256:b8a3bc0a3645e2afcd8807830833a0df0bd243d58d518e17b2335342e2614bd3
INFO:    Creating SIF file...
INFO:    Build complete: python-3.8.6.sif
</pre></div>
</div>
<p>Note how we see singularity saying that it’s “<strong>Converting OCI blobs
to SIF format</strong>”. We then see the layers of the Docker image being
downloaded and unpacked and written into a single SIF file. Once the
process is complete, we should see the python-3.8.6.sif image file in
the current directory.</p>
<p>We can now run a container from this image as we would with any other
singularity image.</p>
<div class="admonition-running-the-python-3-8-6-image-that-we-just-pulled-from-docker-hub exercise important admonition" id="exercise-3">
<p class="admonition-title">Running the Python 3.8.6 image that we just pulled from Docker Hub</p>
<ul class="simple">
<li><p><strong>E1</strong>: Try running the Python 3.8.6 image. What happens?</p></li>
<li><p><strong>E2</strong>: Try running some simple Python statements…</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Solution</p>
<p>Running the Python 3.8.6 image</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>run<span class="w"> </span>python-3.8.6.sif
</pre></div>
</div>
<p>This should put you straight into a Python interactive shell
within the running container:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Python<span class="w"> </span><span class="m">3</span>.8.6<span class="w"> </span><span class="o">(</span>default,<span class="w"> </span>Nov<span class="w"> </span><span class="m">25</span><span class="w"> </span><span class="m">2020</span>,<span class="w"> </span><span class="m">02</span>:47:44<span class="o">)</span>
<span class="w">      </span><span class="o">[</span>GCC<span class="w"> </span><span class="m">8</span>.3.0<span class="o">]</span><span class="w"> </span>on<span class="w"> </span>linux
<span class="w">      </span>Type<span class="w"> </span><span class="s2">&quot;help&quot;</span>,<span class="w"> </span><span class="s2">&quot;copyright&quot;</span>,<span class="w"> </span><span class="s2">&quot;credits&quot;</span><span class="w"> </span>or<span class="w"> </span><span class="s2">&quot;license&quot;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>more<span class="w"> </span>information.
<span class="w">      </span>&gt;&gt;&gt;
</pre></div>
</div>
<p>Now try running some simple Python statements:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>import<span class="w"> </span>math
<span class="w">      </span>&gt;&gt;&gt;<span class="w"> </span>math.pi
<span class="w">      </span><span class="m">3</span>.141592653589793
<span class="w">      </span>&gt;&gt;&gt;
</pre></div>
</div>
</div>
</div>
<p>In addition to running a container and having it run the default run
script, you could also start a container running a shell in case you
want to undertake any configuration prior to running Python. This is
covered in the following exercise:</p>
<div class="admonition-open-a-shell-within-a-python-container exercise important admonition" id="exercise-4">
<p class="admonition-title">Open a shell within a Python container</p>
<p>Try to run a shell within a singularity container based on
the <cite>python-3.8.6.sif</cite> image.  That is, run a container that
opens a shell rather than the default Python interactive console
as we saw above. Can you find more than one way to achieve this?</p>
<p>Within the shell, try starting the Python interactive console
and running some Python commands.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-4">
<p class="admonition-title">Solution</p>
<p>Recall from the earlier material that we can use the
<cite>singularity shell</cite> command to open a shell within a container.
To open a regular shell within a container based on the
<cite>python-3.8.6.sif</cite> image, we can therefore simply run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>singularity shell python-3.8.6.sif

      Singularity&gt; echo $SHELL
      /bin/bash

      Singularity&gt; cat /etc/issue
      Debian GNU/Linux 10 \n \l

Singularity&gt; exit
</pre></div>
</div>
<p>It is also possible to use the <cite>singularity exec</cite> command to
run an executable within a container.  We could, therefore, use
the <cite>exec</cite> command to run <cite>/bin/bash</cite>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>python-3.8.6.sif<span class="w"> </span>/bin/bash

Singularity&gt;<span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="nv">$SHELL</span>
<span class="w">      </span>/bin/bash
</pre></div>
</div>
<p>You can run the Python console from your container shell
simply by running the <cite>python</cite> command.</p>
</div>
</div>
<p>This concludes the second episode and Part I of the Singularity
material. Part II contains a further two episodes where we’ll look
creating your own images and then more advanced use of containers for
running MPI parallel applications.</p>
</section>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.intel.com/content/dam/www/public/us/en/documents/presentation/hpc-containers-singularity-advanced.pdf">[1] Gregory M. Kurzer, Containers for Science, Reproducibility and Mobility: Singularity P2. Intel HPC Developer Conference, 2017.</a></p>
</section>
</section>
<span id="document-build_contain"></span><section id="building-singularity-images">
<span id="build-contain"></span><h2>Building Singularity images<a class="headerlink" href="#building-singularity-images" title="Link to this heading"></a></h2>
<section id="preparing-to-use-singularity-for-building-images">
<h3>Preparing to use Singularity for building images<a class="headerlink" href="#preparing-to-use-singularity-for-building-images" title="Link to this heading"></a></h3>
<p>So far you’ve been able to work with Singularity from your own user
account as a non-privileged user.  This part of the Singularity
material requires that you use Singularity in an environment where you
have administrative (root) access. While it is possible to build
Singularity containers without root access, it is highly recommended
that you do this as the <strong>root</strong> user, as highlighted in <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/build_a_container.html#creating-writable-sandbox-directories">this section</a>
of the Singularity documentation. Bear in mind that the system that
you use to build containers doesn’t have to be the system where you
intend to run the containers. If, for example, you are intending to
build a container that you can subsequently run on a Linux-based
cluster, you could build the container on your own Linux-based desktop
or laptop computer. You could then transfer the built image directly
to the target platform or upload it to an image repository and pull it
onto the target platform from this repository.</p>
<p>There are three different options for accessing a suitable environment
to undertake the material in this part of the course:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Run Singularity from within a Docker container - this will enable you to have the required privileges to build images</p></li>
<li><p>Install Singularity locally on a system where you have administrative access</p></li>
<li><p>Use Singularity on a system where it is already pre-installed and you have administrative (root) access</p></li>
</ol>
</div></blockquote>
<p>We’ll focus on the first option in this part of the course. If you
would like to install Singularity directly on your system, see the box
below for some further pointers. Note that the installation process is
an advanced task that is beyond the scope of this course so we won’t
be covering this.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Installing Singularity on your local system (optional)</p>
<p>If you are running Linux and would like to install Singularity
locally on your system, Singularity provide the free, open source
<a class="reference external" href="https://github.com/hpcng/singularity/releases">Singularity Community Edition</a>. You will need to
install various dependencies on your system and then build
Singularity from source code.</p>
<p>If you are not familiar with building applications from source code,
it is strongly recommended that you use the Docker Singularity
image, as described below in the “Getting started with the Docker
Singularity image” section rather than attempting to build and
install Singularity yourself. The installation process is an
advanced task that is beyond the scope of this session.</p>
<p>However, if you have Linux systems knowledge and would like to
attempt a local install of Singularity, you can find details in the
<a class="reference external" href="https://github.com/sylabs/singularity/blob/master/INSTALL.md">INSTALL.md</a>
file within the Singularity repository that explains how to install
the prerequisites and build and install the software.  Singularity
is written in the <a class="reference external" href="https://golang.org/">Go</a> programming language and
Go is the main dependency that you’ll need to install on your
system. The process of installing Go and any other requirements is
detailed in the INSTALL.md file.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you do not have access to a system with Docker installed, or a
Linux system where you can build and install Singularity but you
have administrative privileges on another system, you could look at
installing a virtualisation tool such as <a class="reference external" href="https://www.virtualbox.org/">VirtualBox</a> on which you could run a Linux
Virtual Machine (VM) image. Within the Linux VM image, you will be
able to install Singularity.  Again this is beyond the scope of the
course.</p>
<p>If you are not able to access/run Singularity yourself on a system
where you have administrative privileges, you can still follow
through this material as it is being taught (or read through it in
your own time if you’re not participating in a taught version of the
course) since it will be helpful to have an understanding of how
Singularity images can be built.</p>
<p>You could also attempt to follow this section of the lesson without
using root and instead using the <code class="docutils literal notranslate"><span class="pre">singularity</span></code> command’s <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/fakeroot.html">–fakeroot</a> option.
However, you may encounter issues with permissions when trying to
build images and run your containers and this is why running the
commands as root is strongly recommended and is the approach
described in this lesson.</p>
</div>
</section>
<section id="getting-started-with-the-docker-singularity-image">
<h3>Getting started with the Docker Singularity image<a class="headerlink" href="#getting-started-with-the-docker-singularity-image" title="Link to this heading"></a></h3>
<p>The <a class="reference external" href="https://quay.io/repository/singularity/singularity">Singularity Docker image</a> is available from
<a class="reference external" href="https://quay.io/">Quay.io</a>.</p>
<div class="admonition-familiarise-yourself-with-the-docker-singularity-image exercise important admonition" id="exercise-0">
<p class="admonition-title">Familiarise yourself with the Docker Singularity image</p>
<ul>
<li><p>Using your previously acquired Docker knowledge, get the
Singularity image for <code class="docutils literal notranslate"><span class="pre">v3.8.2</span></code> and ensure that you can run a Docker
container using this image. You might want to use the <cite>v3.8.2-slim</cite> for Intel/AMD architecture
or <cite>v3.8.2-slim-arm64</cite> for Arm architecture version of this image since it is significantly
smaller than the standard image - the <strong>slim</strong> version of the image will be used in the
examples below.</p></li>
<li><p>Create a directory (e.g. <code class="docutils literal notranslate"><span class="pre">$HOME/singularity_data</span></code>) on your host
machine that you can use for storage of <strong>definition files</strong> (we’ll
introduce these shortly) and generated image files.</p>
<p>This directory should be bind mounted into the Docker container at
the location <cite>/home/singularity</cite> every time you run it - this will
give you a location in which to store built images so that they are
available on the host system once the container exits.  (take a look
at the <code class="docutils literal notranslate"><span class="pre">-v</span></code> switch)</p>
<p><strong>Note</strong>: To be able to build an image using the Docker Singularity
container, you’ll probably need to add the <code class="docutils literal notranslate"><span class="pre">--privileged</span></code> switch to
your docker command line.</p>
<blockquote>
<div><blockquote>
<div><ul class="simple">
<li><p>What is happening when you run the container?</p></li>
<li><p>Can you run an interactive shell in the container?</p></li>
</ul>
</div></blockquote>
<div class="admonition-running-the-image solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Running the image</p>
<dl>
<dt>Having a bound directory from the host system accessible within</dt><dd><p>your running Singularity container will give you somewhere to
place created images so that they are accessible on the host
system after the container exits.  Begin by changing into the
directory that you created above for storing your definiton
files and built images (e.g. <code class="docutils literal notranslate"><span class="pre">$HOME/singularity_data</span></code>).</p>
<p>You may choose to:</p>
<ul class="simple">
<li><p>open a shell within the Docker image so you can work at a
command prompt and run the <code class="docutils literal notranslate"><span class="pre">singularity</span></code> command directly</p></li>
<li><p>use the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command to run a new container instance
every time you want to run the <cite>singularity</cite> command.</p></li>
</ul>
<p>Either option is fine for this section of the material.</p>
<p><strong>Some examples:</strong></p>
<p>To run the <code class="docutils literal notranslate"><span class="pre">singularity</span></code> command within the docker container
directly from the host system’s terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--privileged<span class="w"> </span>--rm<span class="w"> </span>-v<span class="w"> </span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>:/home/singularity
quay.io/singularity/singularity:v3.8.2-slim<span class="w"> </span>cache<span class="w"> </span>list
</pre></div>
</div>
<p>To start a shell within the Singularity Docker container where
the <cite>singularity</cite> command can be run directly:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--entrypoint<span class="o">=</span>/bin/sh<span class="w"> </span>--privileged<span class="w"> </span>--rm<span class="w"> </span>-v<span class="w"> </span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>:/home/singularity<span class="w"> </span>quay.io/singularity/singularity:v3.8.2-slim
</pre></div>
</div>
<p>To make things easier to read in the remainder of the material,
command examples will use the <code class="docutils literal notranslate"><span class="pre">singularity</span></code> command directly,
e.g. <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">cache</span> <span class="pre">list</span></code>. If you’re running a shell in the
Docker container, you can enter the commands as they appear.  If
you’re using the container’s default run behavior and running a
container instance for each run of the command, you’ll need to
replace <code class="docutils literal notranslate"><span class="pre">singularity</span></code> with <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">--privileged</span> <span class="pre">-v</span>
<span class="pre">${PWD}:/home/singularity</span> <span class="pre">quay.io/singularity/singularity:v3.8.2-slim</span></code> or similar.</p>
</dd>
</dl>
</div>
</div></blockquote>
</li>
</ul>
</div>
</section>
<section id="id1">
<h3>Building Singularity images<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<section id="introduction">
<h4>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h4>
<p>As a platform that is widely used in the scientific/research software and HPC communities,
Singularity provides great support for reproducibility.
If you build a Singularity container for some scientific software, it’s likely that you and/or
others will want to be able to reproduce exactly
the same environment again. Maybe you want to verify the results of the code or provide a means
that others can use to verify the results to support a paper or report.
Maybe you’re making a tool available to others and want to ensure that they have exactly the right
version/configuration of the code.</p>
<p>Similarly to Docker and many other modern software tools, Singularity
follows the “Configuration as code” approach and a container
configuration can be stored in a file which can then be committed to
your version control system alongside other code. Assuming it is
suitably configured, this file can then be used by you or other
individuals (or by automated build tools) to reproduce a container
with the same configuration at some point in the future.</p>
</section>
<section id="different-approaches-to-building-images">
<h4>Different approaches to building images<a class="headerlink" href="#different-approaches-to-building-images" title="Link to this heading"></a></h4>
<p>There are various approaches to building Singularity images. We
highlight two different approaches here and focus on one of them:</p>
<ul class="simple">
<li><p>Building within a sandbox: You can build a container
interactively within a <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/build_a_container.html#creating-writable-sandbox-directories">sandbox environment</a>.
This means you get a shell within the container environment and
install and configure packages and code as you wish before exiting the
sandbox and converting it into a container image.</p></li>
<li><p>Building from a <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/build_a_container.html#building-containers-from-singularity-definition-files">Singularity Definition File</a>:
This is Singularity’s equivalent to building a Docker container from a
<cite>Dockerfile</cite> and we’ll discuss this approach in this section.</p></li>
</ul>
<p>You can take a look at Singularity’s “<a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/build_a_container.html">Build a Container</a>”
documentation for more details on different approaches to building
containers.</p>
<div class="admonition-why-look-at-singularity-definition-files exercise important admonition" id="exercise-1">
<p class="admonition-title">Why look at Singularity Definition Files?</p>
<p>Why do you think we might be looking at the definition file
approach here rather than the <em>sandbox approach</em>?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>The sandbox approach is great for prototyping and testing out an
image configuration but it doesn’t provide the best support for
our ultimate goal of <strong>reproducibility</strong>. If you spend time
sitting at your terminal in front of a shell typing different
commands to add configuration, maybe you realize you made a
mistake so you undo one piece of configuration and change
it. This goes on until you have your completed configuration but
there’s no explicit record of exactly what you did to create
that configuration.</p>
<p>Say your container image file gets deleted by accident, or
someone else wants to create an equivalent image to test
something.  How will they do this and know for sure that they
have the same configuration that you had?  With a definition
file, the configuration steps are explicitly defined and can be
easily stored, for example within a version control system, and
re-run.</p>
<p>Definition files are small text files while container files may
be very large, multi-gigabyte files that are difficult and time
consuming to move around. This makes definition files ideal for
storing in a version control system along with their revisions.</p>
</div>
</div>
</section>
<section id="creating-a-singularity-definition-file">
<h4>Creating a Singularity Definition File<a class="headerlink" href="#creating-a-singularity-definition-file" title="Link to this heading"></a></h4>
<p>A Singularity Definition File is a text file that contains a series of statements that are used to create a container image.
In line with the <em>configuration as code</em> approach mentioned above, the definition file can be stored in your code repository
alongside your application code and used to create a reproducible image. This means that for a given commit in your repository,
the version of the definition file present at that commit can be used to reproduce a container with a known state.
It was pointed out earlier in the course, when covering Docker, that this property also applies for Dockerfiles.</p>
<p>We’ll now look at a very simple example of a definition file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Bootstrap:<span class="w"> </span>docker
From:<span class="w"> </span>ubuntu:20.04

%post
<span class="w">  </span>apt-get<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3

%runscript
<span class="w">  </span>python3<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;print(&quot;Hello World! Hello from our custom Singularity image!&quot;)&#39;</span>
</pre></div>
</div>
<p>A definition file has a number of optional sections, specified using the <cite>%</cite> prefix,
that are used to define or undertake different configuration during different stages of the image build process.
You can find full details in Singularity’s <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/definition_files.html">Definition Files documentation</a>.
In our very simple example here, we only use the <cite>%post</cite> and <cite>%runscript</cite> sections.</p>
<p>Let’s step through this definition file and look at the lines in more detail:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Bootstrap:<span class="w"> </span>docker
From:<span class="w"> </span>ubuntu:20.04
</pre></div>
</div>
<p>These first two lines define where to <strong>bootstrap</strong> our image from. Why can’t we just put some application binaries into
a blank image? Any applications or tools that we want to run will need to interact with standard system libraries and
potentially a wide range of other libraries and tools. These need to be available within the image and we therefore
need some sort of operating system as the basis for our image. The most straightforward way to achieve this is to start
from an existing base image containing an operating system. In this case, we’re going to start from a minimal Ubuntu 20.04
Linux Docker image. Note that we’re using a Docker image as the basis for creating a Singularity image.
This demonstrates the flexibility in being able to start from different types of images when creating a new Singularity image.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Bootstrap:</span> <span class="pre">docker</span></code> line is similar to prefixing an image path with <code class="docutils literal notranslate"><span class="pre">docker://</span></code> when using, for example,
the <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">pull</span></code> command. A range of <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/definition_files.html#preferred-bootstrap-agents">different bootstrap options</a>
are supported. <code class="docutils literal notranslate"><span class="pre">From:</span> <span class="pre">ubuntu:20.04</span></code> says that we want to use the <code class="docutils literal notranslate"><span class="pre">ubuntu</span></code> image with the tag <code class="docutils literal notranslate"><span class="pre">20.04</span></code>.</p>
<p>Next we have the <cite>%post</cite> section of the definition file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%post
<span class="w">  </span>apt-get<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3
</pre></div>
</div>
<p>In this section of the file we can do tasks such as package installation, pulling data files from remote locations
and undertaking local configuration within the image. The commands that appear in this section are standard shell
commands and they are run <strong>within</strong> the context of our new container image. So, in the case of this example,
these commands are being run within the context of a minimal Ubuntu 20.04 image that initially has only a very small
set of core packages installed.</p>
<p>Here we use Ubuntu’s package manager to update our package indexes and then install the <code class="docutils literal notranslate"><span class="pre">python3</span></code> package along
with any required dependencies (in Ubuntu 20.04, the <strong>python3</strong> package installs <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">3.8.5</span></code>). The <code class="docutils literal notranslate"><span class="pre">-y</span></code> switches
are used to accept, by default, interactive prompts that might appear asking you to confirm package updates or installation.
This is required because our definition file should be able to run in an unattended, non-interactive environment.</p>
<p>Finally we have the <code class="docutils literal notranslate"><span class="pre">%runscript</span></code> section:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%runscript
<span class="w">  </span>python3<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;print(&quot;Hello World! Hello from our custom Singularity image!&quot;)&#39;</span>
</pre></div>
</div>
<p>This section is used to define a script that should be run when a container is started based on this image using
the <cite>singularity run</cite> command. In this simple example we use <cite>python3</cite> to print out some text to the console.</p>
<p>We can now save the contents of the simple defintion file shown above to a file and build an image based on it.
In the case of this example, the definition file has been named <cite>my_test_image.def</cite>. (Note that the instructions
here assume you’ve bound the image output directory you created to the <cite>/home/singularity</cite> directory in your Docker Singularity container):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>build<span class="w"> </span>/home/singularity/my_test_image.sif<span class="w"> </span>/home/singularity/my_test_image.def
</pre></div>
</div>
<p>Recall from the details at the start of this section that if you are running your command from the host system command line,
running an instance of a Docker container for each run of the command, your command will look something like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--privileged<span class="w"> </span>--rm<span class="w"> </span>-v<span class="w"> </span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>:/home/singularity<span class="w"> </span>quay.io/singularity/singularity:v3.8.2-slim<span class="w"> </span>build<span class="w"> </span>/home/singularity/my_test_image.sif<span class="w"> </span>/home/singularity/my_test_image.def
</pre></div>
</div>
<p>The above command requests the building of an image based on the <cite>my_test_image.def</cite> file with the resulting image
saved to the <cite>my_test_image.sif</cite> file. Note that you will need to prefix the command with <cite>sudo</cite> if you’re running
a locally installed version of Singularity and not running via Docker because it is necessary to have administrative
privileges to build the image. You should see output similar to the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>INFO:    Starting build...
Getting image source signatures
Copying blob da7391352a9b done
Copying blob 14428a6d4bcd done
Copying blob 2c2d948710f2 done
Copying config aa23411143 done
Writing manifest to image destination
Storing signatures
2020/12/08 09:15:18  info unpack layer: sha256:da7391352a9bb76b292a568c066aa4c3cbae8d494e6a3c68e3c596d34f7c75f8
2020/12/08 09:15:19  info unpack layer: sha256:14428a6d4bcdba49a64127900a0691fb00a3f329aced25eb77e3b65646638f8d
2020/12/08 09:15:19  info unpack layer: sha256:2c2d948710f21ad82dce71743b1654b45acb5c059cf5c19da491582cef6f2601
INFO:    Running post scriptlet
+ apt-get -y update
Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]
...
[Package update output truncated]
...
Fetched 16.6 MB in 3s (6050 kB/s)
Reading package lists...
+ apt-get install -y python3
Reading package lists...
...
[Package install output truncated]
...
Processing triggers for libc-bin (2.31-0ubuntu9.1) ...
INFO:    Adding runscript
INFO:    Creating SIF file...
INFO:    Build complete: my_test_image.sif
$
</pre></div>
</div>
<p>You should now have a <code class="docutils literal notranslate"><span class="pre">my_test_image.sif</span></code> file in the current directory. Note that in
your version of the above output, after it says <code class="docutils literal notranslate"><span class="pre">INFO:</span>&#160; <span class="pre">Starting</span> <span class="pre">build...</span></code> you may see
a series of <code class="docutils literal notranslate"><span class="pre">skipped:</span> <span class="pre">already</span> <span class="pre">exists</span></code> messages for the <code class="docutils literal notranslate"><span class="pre">Copying</span> <span class="pre">blob</span></code> lines. This happens
when the Docker image slices for the Ubuntu 20.04 image have previously been downloaded and
are cached on the system where this example is being run. On your system, if the image is not
already cached, you will see the slices being downloaded from Docker Hub when these lines of output appear.</p>
</section>
<section id="permissions-of-the-created-image-file">
<h4>Permissions of the created image file<a class="headerlink" href="#permissions-of-the-created-image-file" title="Link to this heading"></a></h4>
<p>You may find that the created Singularity image file on your host filesystem is owned by the <cite>root</cite> user and not your user.
In this case, you won’t be able to change the ownership/permissions of the file directly if you don’t have root access.
However, the image file will be readable by you and you should be able to take a copy of the file under a new name which
you will then own. You will then be able to modify the permissions of this copy of the image and delete the original
root-owned file since the default permissions should allow this.</p>
<p><strong>Testing your Singularity image</strong></p>
<p>In a moment we’ll test the created image on our HPC platform but, first, you should be able to run a shell in an instance of
the Docker Singularity container and run your singularity image there.</p>
<div class="admonition-run-the-singularity-image-you-ve-created exercise important admonition" id="exercise-2">
<p class="admonition-title">Run the Singularity image you’ve created</p>
<p>Can you run the Singularity image you’ve just built from a shell
within the Docker Singularity container?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--entrypoint<span class="o">=</span>/bin/sh<span class="w"> </span>--privileged<span class="w"> </span>--rm<span class="w"> </span>-v<span class="w"> </span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>:/home/singularity<span class="w"> </span>quay.io/singularity/singularity:v3.8.2-slim
/#<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/home/singularity
/home/singularity#<span class="w"> </span>singularity<span class="w"> </span>run<span class="w"> </span>my_test_image.sif
</pre></div>
</div>
<p>Output</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Hello World! Hello from our custom Singularity image!
/home/singularity#
</pre></div>
</div>
</div>
</div>
<div class="admonition-using-singularity-run-from-within-the-docker-container callout admonition" id="callout-0">
<p class="admonition-title">Using <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">run</span></code> from within the Docker container</p>
<p>It is strongly recommended that you don’t use the Docker container for running Singularity images
in any production setting, only for creating them, since the Singularity command runs within the
container as the root user.
However, for the purposes of this simple example, the Docker Singularity container provides an ideal
environment to test that
you have successfully built your container.</p>
</div>
<p>Now we’ll test our image on an HPC platform. Move your created <code class="docutils literal notranslate"><span class="pre">.sif</span></code> image file to a platform with
an installation of Singularity.
You could, for example, do this using the command line secure copy command <code class="docutils literal notranslate"><span class="pre">scp</span></code>. For example,
the following command would copy
<cite>my_test_image.sif</cite> to the remote server identified by <code class="docutils literal notranslate"><span class="pre">&lt;target</span> <span class="pre">hostname&gt;</span></code> (don’t forget
the colon at the end of the hostname!):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span>-i<span class="w"> </span>&lt;full<span class="w"> </span>path<span class="w"> </span>to<span class="w"> </span>SSH<span class="w"> </span>key<span class="w"> </span>file&gt;<span class="w"> </span>my_test_image.sif<span class="w"> </span>&lt;target<span class="w"> </span>hostname&gt;:
</pre></div>
</div>
<p>You could provide a destination path for the file straight after the colon at the end of the above
command (without a space),
but by default, the file will be uploaded to you home directory.</p>
<p>Try to run the container on the login node of the HPC platform and check that you get the expected output.</p>
<p>It is recommended that you move the create <cite>.sif</cite> file to a platform with an installation of Singularity,
rather than attempting to run
the image using the Docker container. However, if you do try to use the Docker container,
see the notes below on “<em>Using singularity run from within the Docker container</em>” for further information.</p>
<p>Now that we’ve built an image, we can attempt to run it:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>run<span class="w"> </span>my_test_image.sif
</pre></div>
</div>
<p>If everything worked successfully, you should see the message printed
by Python:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Hello<span class="w"> </span>World!<span class="w"> </span>Hello<span class="w"> </span>from<span class="w"> </span>our<span class="w"> </span>custom<span class="w"> </span>Singularity<span class="w"> </span>image!
</pre></div>
</div>
<div class="admonition-using-singularity-run-from-within-the-docker-container callout admonition" id="callout-1">
<p class="admonition-title">Using <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">run</span></code> from within the Docker container</p>
<p>It is strongly recommended that you don’t use the Docker container
for running Singularity images, only for creating then, since the
Singularity command runs within the container as the root
user. However, for the purposes of this simple example, if you are
trying to run the container using the <code class="docutils literal notranslate"><span class="pre">singularity</span></code> command from
within the Docker container, it is likely that you will get an error
relating to <code class="docutils literal notranslate"><span class="pre">/etc/localtime</span></code> similar to the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>WARNING: skipping mount of /etc/localtime: no such file or directory
FATAL: container creation failed: mount
/etc/localtime-&gt;/etc/localtime error: while mounting
/etc/localtime: mount source /etc/localtime doesn&#39;t exist
</pre></div>
</div>
<p>This occurs because the <code class="docutils literal notranslate"><span class="pre">/etc/localtime</span></code> file that provides
timezone configuration is not present within the Docker container.
If you want to use the Docker container to test that your newly
created image runs, you’ll need to open a shell in the Docker
container and add a timezone configuration as described in the
<a class="reference external" href="https://wiki.alpinelinux.org/wiki/Setting_the_timezone">Alpine Linux documentation</a>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apk<span class="w"> </span>add<span class="w"> </span>tzdata
cp<span class="w"> </span>/usr/share/zoneinfo/Europe/London<span class="w"> </span>/etc/localtime
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">run</span></code> command should now work successfully.</p>
</section>
</section>
<section id="more-about-definiton-files">
<h3>More about definiton files<a class="headerlink" href="#more-about-definiton-files" title="Link to this heading"></a></h3>
<p>A {Singularity} Definition file is divided into two parts:</p>
<ol class="arabic simple">
<li><p><strong>Header</strong>: The Header describes the core operating system to build within
the container. Here you will configure the base operating system features
needed within the container. You can specify, the Linux distribution, the
specific version, and the packages that must be part of the core install
(borrowed from the host system).</p></li>
<li><p><strong>Sections</strong>: The rest of the definition is comprised of sections, (sometimes
called scriptlets or blobs of data). Each section is defined by a <code class="docutils literal notranslate"><span class="pre">%</span></code>
character followed by the name of the particular section. All sections are
optional, and a def file may contain more than one instance of a given
section. Sections that are executed at build time are executed with the
<code class="docutils literal notranslate"><span class="pre">/bin/sh</span></code> interpreter and can accept <code class="docutils literal notranslate"><span class="pre">/bin/sh</span></code> options. Similarly,
sections that produce scripts to be executed at runtime can accept options
intended for <code class="docutils literal notranslate"><span class="pre">/bin/sh</span></code></p></li>
</ol>
<p>For more in-depth and practical examples of def files, see the <a class="reference external" href="https://github.com/hpcng/singularity/tree/master/examples">Singularity examples
repository</a></p>
<p>For a comparison between Dockerfile and {Singularity} definition file,
please see: <span class="xref std std-ref">this section</span>.</p>
<section id="header">
<h4>Header<a class="headerlink" href="#header" title="Link to this heading"></a></h4>
<p>The header should be written at the top of the def file. It tells {Singularity}
about the base operating system that it should use to build the container. It is
composed of several keywords.</p>
<p>The only keyword that is required for every type of build is <code class="docutils literal notranslate"><span class="pre">Bootstrap</span></code>.
It determines the <em>bootstrap agent</em>  that will be used to create the base
operating system you want to use. For example, the <code class="docutils literal notranslate"><span class="pre">library</span></code> bootstrap agent
will pull a container from the <a class="reference external" href="https://cloud.sylabs.io/library">Container Library</a> as a base. Similarly, the <code class="docutils literal notranslate"><span class="pre">docker</span></code>
bootstrap agent will pull docker layers from <a class="reference external" href="https://hub.docker.com/">Docker Hub</a> as a base OS to start your image.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Bootstrap</span></code> keyword needs to be the first
entry in the header section.  This breaks compatibility with older versions
that allow the parameters of the header to appear in any order.</p>
<p>Depending on the value assigned to <code class="docutils literal notranslate"><span class="pre">Bootstrap</span></code>, other keywords may also be
valid in the header. For example, when using the <code class="docutils literal notranslate"><span class="pre">library</span></code> bootstrap agent,
the <code class="docutils literal notranslate"><span class="pre">From</span></code> keyword becomes valid. Observe the following example for building a
Debian container from the Container Library:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="k">Bootstrap</span>:<span class="w"> </span>library
<span class="k">From</span>:<span class="w"> </span>debian:<span class="m">7</span>
</pre></div>
</div>
<p>A def file that uses an official mirror to install Centos-7 might look like
this:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="k">Bootstrap</span>:<span class="w"> </span>yum
<span class="k">OSVersion</span>:<span class="w"> </span><span class="m">7</span>
<span class="k">MirrorURL</span>:<span class="w"> </span>http://mirror.centos.org/centos-%{OSVERSION}/%{OSVERSION}/os/$basearch/
<span class="k">Include</span>:<span class="w"> </span>yum
</pre></div>
</div>
<p>Each bootstrap agent enables its own options and keywords. You can read about
them and see examples in the <span class="xref std std-ref">appendix section</span>:</p>
<p><strong>Preferred bootstrap agents</strong></p>
<ul class="simple">
<li><p>library (images hosted on the <a class="reference external" href="https://cloud.sylabs.io/library">Container Library</a>)</p></li>
<li><p>docker (images hosted on Docker Hub)</p></li>
<li><p>build-shub (images hosted on Singularity Hub)</p></li>
<li><p>oras (images from supporting OCI registries)</p></li>
<li><p>scratch (a flexible option for building a container from scratch)</p></li>
</ul>
<p><strong>Other bootstrap agents</strong></p>
<ul class="simple">
<li><p>localimage (images saved on your machine)</p></li>
<li><p>yum (yum based systems such as CentOS and Scientific Linux)</p></li>
<li><p>debootstrap (apt based systems such as Debian and Ubuntu)</p></li>
<li><p>oci (bundle compliant with OCI Image Specification)</p></li>
<li><p>oci-archive (tar files obeying the OCI Image Layout Specification)</p></li>
<li><p>docker-daemon (images managed by the locally running docker daemon)</p></li>
<li><p>docker-archive (archived docker images)</p></li>
<li><p>arch (Arch Linux)</p></li>
<li><p>busybox (BusyBox)</p></li>
<li><p>zypper (zypper based systems such as Suse and OpenSuse)</p></li>
</ul>
</section>
<section id="a-general-definition">
<h4>A general definition<a class="headerlink" href="#a-general-definition" title="Link to this heading"></a></h4>
<p>The main content of the bootstrap file is broken into sections. Different
sections add different content or execute commands at different times during the
build process. Note that if any command fails, the build process will halt.</p>
<p>Here is an example definition file that uses every available section. We will
discuss each section in turn. It is not necessary to include every section (or
any sections at all) within a def file. Furthermore, multiple sections of the
same name can be included and will be appended to one another during the build
process.</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="k">Bootstrap</span>:<span class="w"> </span>library
<span class="k">From</span>:<span class="w"> </span>ubuntu:<span class="m">18.04</span>
Stage:<span class="w"> </span>build

<span class="gh">%setup</span>
<span class="w">    </span>touch<span class="w"> </span>/file1
<span class="w">    </span>touch<span class="w"> </span><span class="si">${</span><span class="nv">SINGULARITY_ROOTFS</span><span class="si">}</span>/file2

<span class="gh">%files</span>
<span class="w">    </span>/file1
<span class="w">    </span>/file1<span class="w"> </span>/opt

<span class="gh">%environment</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LISTEN_PORT</span><span class="o">=</span><span class="m">12345</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LC_ALL</span><span class="o">=</span>C

<span class="gh">%post</span>
<span class="w">    </span>apt-get<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>netcat
<span class="w">    </span><span class="nv">NOW</span><span class="o">=</span><span class="sb">`</span>date<span class="sb">`</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;export NOW=\&quot;</span><span class="si">${</span><span class="nv">NOW</span><span class="si">}</span><span class="s2">\&quot;&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span><span class="nv">$SINGULARITY_ENVIRONMENT</span>

<span class="gh">%runscript</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Container was created </span><span class="nv">$NOW</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Arguments received: </span><span class="nv">$*</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">exec</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>

<span class="gh">%startscript</span>
<span class="w">    </span>nc<span class="w"> </span>-lp<span class="w"> </span><span class="nv">$LISTEN_PORT</span>

<span class="gh">%test</span>
<span class="w">    </span>grep<span class="w"> </span>-q<span class="w"> </span><span class="nv">NAME</span><span class="o">=</span><span class="se">\&quot;</span>Ubuntu<span class="se">\&quot;</span><span class="w"> </span>/etc/os-release
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$?</span><span class="w"> </span>-eq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Container base is Ubuntu as expected.&quot;</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Container base is not Ubuntu.&quot;</span>
<span class="w">        </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="w">    </span><span class="k">fi</span>

<span class="gh">%labels</span>
<span class="w">    </span>Author<span class="w"> </span>d@sylabs.io
<span class="w">    </span>Version<span class="w"> </span>v0.0.1

<span class="gh">%help</span>
<span class="w">    </span>This<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>demo<span class="w"> </span>container<span class="w"> </span>used<span class="w"> </span>to<span class="w"> </span>illustrate<span class="w"> </span>a<span class="w"> </span>def<span class="w"> </span>file<span class="w"> </span>that<span class="w"> </span>uses<span class="w"> </span>all
<span class="w">    </span>supported<span class="w"> </span>sections.
</pre></div>
</div>
<p>Although the order of the sections in the def file is unimportant, they have
been documented below in the order of their execution during the build process
for logical understanding.</p>
</section>
<section id="setup">
<h4><code class="docutils literal notranslate"><span class="pre">%setup</span></code><a class="headerlink" href="#setup" title="Link to this heading"></a></h4>
<p>During the build process, commands in the <code class="docutils literal notranslate"><span class="pre">%setup</span></code> section are first executed
on the host system outside of the container after the base OS has been installed.
You can reference the container file system with the <code class="docutils literal notranslate"><span class="pre">$SINGULARITY_ROOTFS</span></code>
environment variable in the <code class="docutils literal notranslate"><span class="pre">%setup</span></code> section.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Be careful with the <code class="docutils literal notranslate"><span class="pre">%setup</span></code> section! This scriptlet is executed outside
of the container on the host system itself, and is executed with elevated
privileges.</p>
</div>
<p>Consider the example from the definition file above:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%setup</span>
<span class="w">    </span>touch<span class="w"> </span>/file1
<span class="w">    </span>touch<span class="w"> </span><span class="si">${</span><span class="nv">SINGULARITY_ROOTFS</span><span class="si">}</span>/file2
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">file1</span></code> is created at the root of the file system <strong>on the host</strong>.
We’ll use <code class="docutils literal notranslate"><span class="pre">file1</span></code> to demonstrate the usage of the <code class="docutils literal notranslate"><span class="pre">%files</span></code> section below.
The <code class="docutils literal notranslate"><span class="pre">file2</span></code> is created at the root of the file system <strong>within the
container</strong>.</p>
<p>In later versions of {Singularity} the <code class="docutils literal notranslate"><span class="pre">%files</span></code> section is provided as a safer
alternative to copying files from the host system into the container during the
build. Because of the potential danger involved in running the <code class="docutils literal notranslate"><span class="pre">%setup</span></code>
scriptlet with elevated privileges on the host system during the build, it’s
use is generally discouraged.</p>
<p><code class="docutils literal notranslate"><span class="pre">%setup</span></code> can be used for exporting environmental variables.</p>
</section>
<section id="files">
<h4><code class="docutils literal notranslate"><span class="pre">%files</span></code><a class="headerlink" href="#files" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">%files</span></code> section allows you to copy files into the container with greater
safety than using the <code class="docutils literal notranslate"><span class="pre">%setup</span></code> section. Its general form is:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%files</span><span class="w"> </span><span class="o">[</span>from<span class="w"> </span>&lt;stage&gt;<span class="o">]</span>
<span class="w">    </span>&lt;source&gt;<span class="w"> </span><span class="o">[</span>&lt;destination&gt;<span class="o">]</span>
<span class="w">    </span>...
</pre></div>
</div>
<p>Each line is a <code class="docutils literal notranslate"><span class="pre">&lt;source&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;destination&gt;</span></code> pair. The <code class="docutils literal notranslate"><span class="pre">&lt;source&gt;</span></code> is either:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>A valid path on your host system</p></li>
<li><p>A valid path in a previous stage of the build</p></li>
</ol>
</div></blockquote>
<p>while the <code class="docutils literal notranslate"><span class="pre">&lt;destination&gt;</span></code> is always a path into the current container. If the
<code class="docutils literal notranslate"><span class="pre">&lt;destination&gt;</span></code> path is omitted it will be assumed to be the same as <code class="docutils literal notranslate"><span class="pre">&lt;source&gt;</span></code>.
To show how copying from your host system works, let’s consider the example from
the definition file above:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%files</span>
<span class="w">    </span>/file1
<span class="w">    </span>/file1<span class="w"> </span>/opt
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">file1</span></code> was created in the root of the host file system during the <code class="docutils literal notranslate"><span class="pre">%setup</span></code>
section (see above).  The <code class="docutils literal notranslate"><span class="pre">%files</span></code> scriptlet will copy <code class="docutils literal notranslate"><span class="pre">file1</span></code> to the root
of the container file system and then make a second copy of <code class="docutils literal notranslate"><span class="pre">file1</span></code> within the
container in <code class="docutils literal notranslate"><span class="pre">/opt</span></code>.</p>
<p>Files can also be copied from other stages by providing the source location in the
previous stage and the destination in the current container.</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%files</span><span class="w"> </span>from<span class="w"> </span>stage_name
<span class="w">  </span>/root/hello<span class="w"> </span>/bin/hello
</pre></div>
</div>
<p>The only difference in behavior between copying files from your host system and copying them
from previous stages is that in the former case symbolic links are always followed
during the copy to the container, while in the latter symbolic links are preserved.</p>
<p>Files in the <code class="docutils literal notranslate"><span class="pre">%files</span></code> section are always copied before the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section is
executed so that they are available during the build and configuration process.</p>
</section>
<section id="post">
<h4><code class="docutils literal notranslate"><span class="pre">%post</span></code><a class="headerlink" href="#post" title="Link to this heading"></a></h4>
<p>This section is where you can download files from the internet with tools like <code class="docutils literal notranslate"><span class="pre">git</span></code>
and <code class="docutils literal notranslate"><span class="pre">wget</span></code>, install new software and libraries, write configuration files,
create new directories, etc.</p>
<p>Consider the example from the definition file above:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%post</span>
<span class="w">    </span>apt-get<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>netcat
<span class="w">    </span><span class="nv">NOW</span><span class="o">=</span><span class="sb">`</span>date<span class="sb">`</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;export NOW=\&quot;</span><span class="si">${</span><span class="nv">NOW</span><span class="si">}</span><span class="s2">\&quot;&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span><span class="nv">$SINGULARITY_ENVIRONMENT</span>
</pre></div>
</div>
<p>This <code class="docutils literal notranslate"><span class="pre">%post</span></code> scriptlet uses the Ubuntu package manager <code class="docutils literal notranslate"><span class="pre">apt</span></code> to update the
container and install the program <code class="docutils literal notranslate"><span class="pre">netcat</span></code> (that will be used in the
<code class="docutils literal notranslate"><span class="pre">%startscript</span></code> section below).</p>
<p>The script is also setting an environment variable at build time.  Note that the
value of this variable cannot be anticipated, and therefore cannot be set during
the <code class="docutils literal notranslate"><span class="pre">%environment</span></code> section. For situations like this, the <code class="docutils literal notranslate"><span class="pre">$SINGULARITY_ENVIRONMENT</span></code>
variable is provided. Redirecting text to this variable will cause it to be
written to a file called <code class="docutils literal notranslate"><span class="pre">/.singularity.d/env/91-environment.sh</span></code> that will be
sourced at runtime.</p>
</section>
<section id="test">
<h4><code class="docutils literal notranslate"><span class="pre">%test</span></code><a class="headerlink" href="#test" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">%test</span></code> section runs at the very end of the build process to
validate the container using a method of your choice. You can also
execute this scriptlet through the container itself, using the
<code class="docutils literal notranslate"><span class="pre">test</span></code> command.</p>
<p>Consider the example from the def file above:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%test</span>
<span class="w">    </span>grep<span class="w"> </span>-q<span class="w"> </span><span class="nv">NAME</span><span class="o">=</span><span class="se">\&quot;</span>Ubuntu<span class="se">\&quot;</span><span class="w"> </span>/etc/os-release
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$?</span><span class="w"> </span>-eq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Container base is Ubuntu as expected.&quot;</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Container base is not Ubuntu.&quot;</span>
<span class="w">        </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="w">    </span><span class="k">fi</span>
</pre></div>
</div>
<p>This (somewhat silly) script tests if the base OS is Ubuntu. You could
also write a script to test that binaries were appropriately
downloaded and built, or that software works as expected on custom
hardware. If you want to build a container without running the
<code class="docutils literal notranslate"><span class="pre">%test</span></code> section (for example, if the build system does not have the
same hardware that will be used on the production system), you can do
so with the <code class="docutils literal notranslate"><span class="pre">--notest</span></code> build option:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ sudo singularity build --notest my_container.sif my_container.def
</pre></div>
</div>
<p>Running the test command on a container built with this def file yields the
following:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity test my_container.sif
Container base is Ubuntu as expected.
</pre></div>
</div>
<p>One common use of the <code class="docutils literal notranslate"><span class="pre">%test</span></code> section is to run a quick check that
the programs you intend to install in the container are present. If
you installed the program <code class="docutils literal notranslate"><span class="pre">samtools</span></code>, which shows a usage screen when
run without any options, you might test it can be run with:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%test</span>
<span class="w">    </span><span class="c1"># Run samtools - exits okay with usage screen if installed</span>
<span class="w">    </span>samtools
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">samtools</span></code> is not successfully installed in the container then the
<code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">test</span></code> will exit with an error such as <code class="docutils literal notranslate"><span class="pre">samtools:</span>
<span class="pre">command</span> <span class="pre">not</span> <span class="pre">found</span></code>.</p>
<p>Some programs return an error code when run without mandatory
options. If you want to ignore this, and just check the program is
present and can be called, you can run it as <code class="docutils literal notranslate"><span class="pre">myprog</span> <span class="pre">||</span> <span class="pre">true</span></code> in
your test:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%test</span>
<span class="w">    </span><span class="c1"># Run bwa - exits with error code if installed and run without</span>
<span class="w">    </span><span class="c1"># options</span>
<span class="w">    </span>bwa<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">true</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">||</span> <span class="pre">true</span></code> means that if the command before it is found but
returns an error code it will be ignored, and replaced with the error
code from <code class="docutils literal notranslate"><span class="pre">true</span></code> - which is always <code class="docutils literal notranslate"><span class="pre">0</span></code> indicating success.</p>
<p>Because the <code class="docutils literal notranslate"><span class="pre">%test</span></code> section is a shell scriptlet, complex tests are
possible. Your scriptlet should usually be written so it will exit
with a non-zero error code if there is a problem during the tests.</p>
<p>Now, the following sections are all inserted into the container filesystem in
single step:</p>
</section>
<section id="environment">
<h4><code class="docutils literal notranslate"><span class="pre">%environment</span></code><a class="headerlink" href="#environment" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">%environment</span></code> section allows you to define environment variables that
will be set at runtime. Note that these variables are not made available at
build time by their inclusion in the <code class="docutils literal notranslate"><span class="pre">%environment</span></code> section. This means that
if you need the same variables during the build process, you should also define
them in your <code class="docutils literal notranslate"><span class="pre">%post</span></code> section. Specifically:</p>
<ul class="simple">
<li><p><strong>during build</strong>: The <code class="docutils literal notranslate"><span class="pre">%environment</span></code> section is written to a file in the
container metadata directory. This file is not sourced.</p></li>
<li><p><strong>during runtime</strong>: The file in the container metadata directory is sourced.</p></li>
</ul>
<p>You should use the same conventions that you would use in a <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> or
<code class="docutils literal notranslate"><span class="pre">.profile</span></code> file. Consider this example from the def file above:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%environment</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LISTEN_PORT</span><span class="o">=</span><span class="m">12345</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LC_ALL</span><span class="o">=</span>C
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">$LISTEN_PORT</span></code> variable will be used in the <code class="docutils literal notranslate"><span class="pre">%startscript</span></code> section
below. The <code class="docutils literal notranslate"><span class="pre">$LC_ALL</span></code> variable is useful for many programs (often written in
Perl) that complain when no locale is set.</p>
<p>After building this container, you can verify that the environment variables are
set appropriately at runtime with the following command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity exec my_container.sif env | grep -E &#39;LISTEN_PORT|LC_ALL&#39;
LISTEN_PORT=12345
LC_ALL=C
</pre></div>
</div>
<p>In the special case of variables generated at build time, you can also add
environment variables to your container in the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section.</p>
<p>At build time, the content of the <code class="docutils literal notranslate"><span class="pre">%environment</span></code> section is written to a file
called <code class="docutils literal notranslate"><span class="pre">/.singularity.d/env/90-environment.sh</span></code> inside of the container.  Text
redirected to the <code class="docutils literal notranslate"><span class="pre">$SINGULARITY_ENVIRONMENT</span></code> variable during <code class="docutils literal notranslate"><span class="pre">%post</span></code> is
added to a file called <code class="docutils literal notranslate"><span class="pre">/.singularity.d/env/91-environment.sh</span></code>.</p>
<p>At runtime, scripts in <code class="docutils literal notranslate"><span class="pre">/.singularity/env</span></code> are sourced in order. This means
that variables in the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section take precedence over those added  via
<code class="docutils literal notranslate"><span class="pre">%environment</span></code>.</p>
<p>See <span class="xref std std-ref">Environment and Metadata</span> for more
information about the {Singularity} container environment.</p>
</section>
<section id="runscript">
<h4><code class="docutils literal notranslate"><span class="pre">%runscript</span></code><a class="headerlink" href="#runscript" title="Link to this heading"></a></h4>
<p>The contents of the <code class="docutils literal notranslate"><span class="pre">%runscript</span></code> section are written to a file within the
container that is executed when the container image is run (either via the
<code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">run</span></code> command or by executing the container directly as a
command). When the container is invoked, arguments following the container name
are passed to the runscript. This means that you can (and should) process
arguments within your runscript.</p>
<p>Consider the example from the def file above:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%runscript</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Container was created </span><span class="nv">$NOW</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Arguments received: </span><span class="nv">$*</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">exec</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>In this runscript, the time that the container was created is echoed via the
<code class="docutils literal notranslate"><span class="pre">$NOW</span></code> variable (set in the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section above). The options passed to
the container at runtime are printed as a single string (<code class="docutils literal notranslate"><span class="pre">$*</span></code>) and then they
are passed to echo via a quoted array (<code class="docutils literal notranslate"><span class="pre">$&#64;</span></code>) which ensures that all of the
arguments are properly parsed by the executed command. The <code class="docutils literal notranslate"><span class="pre">exec</span></code> preceding
the final <code class="docutils literal notranslate"><span class="pre">echo</span></code> command replaces the current entry in the process table
(which originally was the call to {Singularity}). Thus the runscript shell process
ceases to exist, and only the process running within the container remains.</p>
<p>Running the container built using this def file will yield the following:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ ./my_container.sif
Container was created Thu Dec  6 20:01:56 UTC 2018
Arguments received:

$ ./my_container.sif this that and the other
Container was created Thu Dec  6 20:01:56 UTC 2018
Arguments received: this that and the other
this that and the other
</pre></div>
</div>
</section>
<section id="labels">
<h4><code class="docutils literal notranslate"><span class="pre">%labels</span></code><a class="headerlink" href="#labels" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">%labels</span></code> section is used to add metadata to the file
<code class="docutils literal notranslate"><span class="pre">/.singularity.d/labels.json</span></code> within your container. The general format is a
name-value pair.</p>
<p>Consider the example from the def file above:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%labels</span>
<span class="w">    </span>Author<span class="w"> </span>d@sylabs.io
<span class="w">    </span>Version<span class="w"> </span>v0.0.1
<span class="w">    </span>MyLabel<span class="w"> </span>Hello<span class="w"> </span>World
</pre></div>
</div>
<p>Note that labels are defined by key-value pairs. To define a label just add it
on the labels section and after the first space character add the correspondent value to the label.</p>
<p>In the previous example, the first label name is <code class="docutils literal notranslate"><span class="pre">Author`</span></code> with a
value of <code class="docutils literal notranslate"><span class="pre">d&#64;sylabs.io</span></code>. The second label name is <code class="docutils literal notranslate"><span class="pre">Version</span></code> with a value of <code class="docutils literal notranslate"><span class="pre">v0.0.1</span></code>.
Finally, the last label named <code class="docutils literal notranslate"><span class="pre">MyLabel</span></code> has the value of <code class="docutils literal notranslate"><span class="pre">Hello</span> <span class="pre">World</span></code>.</p>
<p>To inspect the available labels on your image you can do so by running the following command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity inspect my_container.sif

{
  &quot;Author&quot;: &quot;d@sylabs.io&quot;,
  &quot;Version&quot;: &quot;v0.0.1&quot;,
  &quot;MyLabel&quot;: &quot;Hello World&quot;,
  &quot;org.label-schema.build-date&quot;: &quot;Thursday_6_December_2018_20:1:56_UTC&quot;,
  &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;,
  &quot;org.label-schema.usage&quot;: &quot;/.singularity.d/runscript.help&quot;,
  &quot;org.label-schema.usage.singularity.deffile.bootstrap&quot;: &quot;library&quot;,
  &quot;org.label-schema.usage.singularity.deffile.from&quot;: &quot;ubuntu:18.04&quot;,
  &quot;org.label-schema.usage.singularity.runscript.help&quot;: &quot;/.singularity.d/runscript.help&quot;,
  &quot;org.label-schema.usage.singularity.version&quot;: &quot;3.0.1&quot;
}
</pre></div>
</div>
<p>Some labels that are captured automatically from the build process. You can read
more about labels and metadata <span class="xref std std-ref">here</span>.</p>
</section>
<section id="help">
<h4><code class="docutils literal notranslate"><span class="pre">%help</span></code><a class="headerlink" href="#help" title="Link to this heading"></a></h4>
<p>Any text in the <code class="docutils literal notranslate"><span class="pre">%help</span></code> section is transcribed into a metadata file in the
container during the build. This text can then be displayed using the
<code class="docutils literal notranslate"><span class="pre">run-help</span></code> command.</p>
<p>Consider the example from the def file above:</p>
<div class="highlight-singularity notranslate"><div class="highlight"><pre><span></span><span class="gh">%help</span>
<span class="w">    </span>This<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>demo<span class="w"> </span>container<span class="w"> </span>used<span class="w"> </span>to<span class="w"> </span>illustrate<span class="w"> </span>a<span class="w"> </span>def<span class="w"> </span>file<span class="w"> </span>that<span class="w"> </span>uses<span class="w"> </span>all
<span class="w">    </span>supported<span class="w"> </span>sections.
</pre></div>
</div>
<p>After building the help can be displayed like so:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity run-help my_container.sif
    This is a demo container used to illustrate a def file that uses all
    supported sections.
</pre></div>
</div>
<p>The <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/definition_files.html#sections">“Sections” part of the definition file documentation</a>
details all the sections and provides an example definition file that
makes use of all the sections.</p>
<section id="additional-singularity-features">
<h5>Additional Singularity features<a class="headerlink" href="#additional-singularity-features" title="Link to this heading"></a></h5>
<p>Singularity has a wide range of features. You can find full details in
the <a class="reference external" href="https://sylabs.io/guides/3.5/user-guide/index.html">Singularity User Guide</a> and we
highlight a couple of key features here that may be of use/interest:</p>
<p><strong>Remote Builder Capabilities:</strong> If you have access to a platform with
Singularity installed but you don’t have root access to create
containers, you may be able to use the <a class="reference external" href="https://cloud.sylabs.io/builder">Remote
Builder</a> functionality to offload the
process of building an image to remote cloud resources.  You’ll need
to register for a <strong>cloud token</strong> via the link on the Remote Builder
page.</p>
<p><strong>Signing containers:</strong> If you do want to share container image
(<code class="docutils literal notranslate"><span class="pre">.sif</span></code>) files directly with colleagues or collaborators, how can the
people you send an image to be sure that they have received the file
without it being tampered with or suffering from corruption during
transfer/storage? And how can you be sure that the same goes for any
container image file you receive from others? Singularity supports
signing containers. This allows a digital signature to be linked to
an image file. This signature can be used to verify that an image
file has been signed by the holder of a specific key and that the
file is unchanged from when it was signed. You can find full details
of how to use this functionality in the Singularity documentation on
<a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/signNverify.html">Signing and Verifying Containers</a>.</p>
</section>
</section>
</section>
</section>
<span id="document-tf_intro"></span><figure class="align-default" id="id3">
<span id="tf-intro"></span><a class="reference internal image-reference" href="https://www.gstatic.com/devrel-devsite/prod/v93a6dcf50ad5e38e51034415df5b4a8345b5c8613f785e48818ae468dabf73c8/tensorflow/images/lockup.svg"><img alt="https://www.gstatic.com/devrel-devsite/prod/v93a6dcf50ad5e38e51034415df5b4a8345b5c8613f785e48818ae468dabf73c8/tensorflow/images/lockup.svg" src="https://www.gstatic.com/devrel-devsite/prod/v93a6dcf50ad5e38e51034415df5b4a8345b5c8613f785e48818ae468dabf73c8/tensorflow/images/lockup.svg" style="width: 40%;" />
</a>
<figcaption>
<p><span class="caption-text"><a class="reference external" href="https://www.tensorflow.org">(Image Source)</a></span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="tensorflow-on-a-single-gpu">
<h2>TensorFlow on a single GPU<a class="headerlink" href="#tensorflow-on-a-single-gpu" title="Link to this heading"></a></h2>
<p>TensorFlow is a well-known library developed primarily in Google which has been
proven to be one of the most robust, reliable, and fast libraries for deep learning
among developers. I think most of us have had some form of exposure to TensorFlow
at some point in our deep learning/machine learning journey.</p>
<p>In this section we focus on using a single GPU for training our model. It is rather
easy to transfer/port training of the model to the GPU with minimal coding.</p>
<p>TensorFlow supports running computations on a variety of types of devices, including
CPU and GPU. They are represented with string identifiers for example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/device:CPU:0</span></code>: The CPU of your machine.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/GPU:0</span></code>: Short-hand notation for the first GPU of your machine that is
visible to TensorFlow.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/job:localhost/replica:0/task:0/device:GPU:1</span></code>: Fully qualified name of
the second GPU of your machine that is visible to TensorFlow.</p></li>
</ul>
</div></blockquote>
<p>If a TensorFlow operation has both CPU and GPU implementations, by default,
the GPU device is prioritized when the operation is assigned. For example, <code class="docutils literal notranslate"><span class="pre">tf.matmul</span></code>
has both CPU and GPU kernels and on a system with devices <code class="docutils literal notranslate"><span class="pre">CPU:0</span></code> and <code class="docutils literal notranslate"><span class="pre">GPU:0</span></code>,
the <code class="docutils literal notranslate"><span class="pre">GPU:0</span></code> device is selected to run <code class="docutils literal notranslate"><span class="pre">tf.matmul</span></code> unless you explicitly request
to run it on another device.</p>
<p>If a TensorFlow operation has no corresponding GPU implementation, then the operation
falls back to the CPU device. For example, since <code class="docutils literal notranslate"><span class="pre">tf.cast</span></code> only has a CPU kernel,
on a system with devices <code class="docutils literal notranslate"><span class="pre">CPU:0</span></code> and <code class="docutils literal notranslate"><span class="pre">GPU:0</span></code>, the <code class="docutils literal notranslate"><span class="pre">CPU:0</span></code> device is selected
to run <code class="docutils literal notranslate"><span class="pre">tf.cast</span></code>, even if requested to run on the <code class="docutils literal notranslate"><span class="pre">GPU:0</span></code> device.</p>
<section id="get-the-physical-devices">
<h3>Get the physical devices<a class="headerlink" href="#get-the-physical-devices" title="Link to this heading"></a></h3>
<p>After booking a node with multiple GPUs, let’s check if we have TensorFlow module
loaded and if the physical GPU device is available.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num of GPUs Available: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorFlow version: &quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Num<span class="w"> </span>of<span class="w"> </span>GPUs<span class="w"> </span>Available:<span class="w">  </span><span class="m">6</span>
TensorFlow<span class="w"> </span>version:<span class="w">  </span><span class="m">2</span>.5.0
</pre></div>
</div>
<p>We can see the list of all of available devices:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>PhysicalDevice<span class="o">(</span><span class="nv">name</span><span class="o">=</span><span class="s1">&#39;/physical_device:CPU:0&#39;</span>,<span class="w"> </span><span class="nv">device_type</span><span class="o">=</span><span class="s1">&#39;CPU&#39;</span><span class="o">)</span>,
PhysicalDevice<span class="o">(</span><span class="nv">name</span><span class="o">=</span><span class="s1">&#39;/physical_device:GPU:0&#39;</span>,<span class="w"> </span><span class="nv">device_type</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="o">)</span>,
PhysicalDevice<span class="o">(</span><span class="nv">name</span><span class="o">=</span><span class="s1">&#39;/physical_device:GPU:1&#39;</span>,<span class="w"> </span><span class="nv">device_type</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="o">)</span>,
PhysicalDevice<span class="o">(</span><span class="nv">name</span><span class="o">=</span><span class="s1">&#39;/physical_device:GPU:2&#39;</span>,<span class="w"> </span><span class="nv">device_type</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="o">)</span>,
PhysicalDevice<span class="o">(</span><span class="nv">name</span><span class="o">=</span><span class="s1">&#39;/physical_device:GPU:3&#39;</span>,<span class="w"> </span><span class="nv">device_type</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="o">)</span>,
PhysicalDevice<span class="o">(</span><span class="nv">name</span><span class="o">=</span><span class="s1">&#39;/physical_device:GPU:4&#39;</span>,<span class="w"> </span><span class="nv">device_type</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="o">)</span>,
PhysicalDevice<span class="o">(</span><span class="nv">name</span><span class="o">=</span><span class="s1">&#39;/physical_device:GPU:5&#39;</span>,<span class="w"> </span><span class="nv">device_type</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="o">)]</span>
</pre></div>
</div>
<p>If you have GPUs, then you should see the GPU device in the above list.
We can also check specifically for the GPU or CPU devices.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;CPU&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="placement-of-calculations">
<h3>Placement of calculations<a class="headerlink" href="#placement-of-calculations" title="Link to this heading"></a></h3>
<p>TensorFlow automatically place tensor operations to physical devices which is by
default is the GPU if available. Now, let’s define a random Tensor, and check where
it is placed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">device</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;/job:localhost/replica:0/task:0/device:GPU:0&#39;</span>
</pre></div>
</div>
<p>The above string will end with <code class="docutils literal notranslate"><span class="pre">GPU:K</span></code> if the Tensor is placed on the K-th GPU device.
We can also check if a tensor is placed on a specific device by using <code class="docutils literal notranslate"><span class="pre">device_endswith</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Is the Tensor on CPU #0:  &quot;</span><span class="p">),</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;CPU:0&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Is the Tensor on GPU #0:  &quot;</span><span class="p">),</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;GPU:0&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Is<span class="w"> </span>the<span class="w"> </span>Tensor<span class="w"> </span>on<span class="w"> </span>CPU<span class="w"> </span><span class="c1">#0:</span>
False

Is<span class="w"> </span>the<span class="w"> </span>Tensor<span class="w"> </span>on<span class="w"> </span>GPU<span class="w"> </span><span class="c1">#0:</span>
True
</pre></div>
</div>
</section>
<section id="determining-the-placement">
<h3>Determining the Placement<a class="headerlink" href="#determining-the-placement" title="Link to this heading"></a></h3>
<p>It is possible to force placement on specific devices, if they are available. We can view
the benefits of GPU acceleration by running some tests and placing the operations on
the CPU or GPU respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="k">def</span><span class="w"> </span><span class="nf">time_matadd</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">loop</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matrix addition (10 loops): </span><span class="si">{:0.2f}</span><span class="s2">ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="n">result</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">time_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">loop</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matrix multiplication (10 loops): </span><span class="si">{:0.2f}</span><span class="s2">ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="n">result</span><span class="p">))</span>
</pre></div>
</div>
<p>We run the above tests first on a CPU using <code class="docutils literal notranslate"><span class="pre">tf.device(&quot;CPU:0&quot;)</span></code>,
which forces the operations to be run on the CPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;On CPU:&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;CPU:0&quot;</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
  <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;CPU:0&quot;</span><span class="p">)</span>
  <span class="n">time_matadd</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">time_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>On<span class="w"> </span>CPU:
Matrix<span class="w"> </span>addition<span class="w"> </span><span class="o">(</span><span class="m">10</span><span class="w"> </span>loops<span class="o">)</span>:<span class="w"> </span><span class="m">3</span>.51ms
Matrix<span class="w"> </span>multiplication<span class="w"> </span><span class="o">(</span><span class="m">10</span><span class="w"> </span>loops<span class="o">)</span>:<span class="w"> </span><span class="m">199</span>.40ms
</pre></div>
</div>
<p>And doing the same operations on the GPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;On GPU:&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;GPU:0&quot;</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;GPU:0&quot;</span><span class="p">)</span>
    <span class="n">time_matadd</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">time_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>On<span class="w"> </span>GPU:
Matrix<span class="w"> </span>addition<span class="w"> </span><span class="o">(</span><span class="m">10</span><span class="w"> </span>loops<span class="o">)</span>:<span class="w"> </span><span class="m">0</span>.89ms
Matrix<span class="w"> </span>multiplication<span class="w"> </span><span class="o">(</span><span class="m">10</span><span class="w"> </span>loops<span class="o">)</span>:<span class="w"> </span><span class="m">22</span>.64ms
</pre></div>
</div>
<p>Note the significant time difference between running these operations on different devices.</p>
</section>
<section id="logging-device-placement">
<h3>Logging device placement<a class="headerlink" href="#logging-device-placement" title="Link to this heading"></a></h3>
<p>We can find out which devices your operations and tensors are assigned to by putting
<code class="docutils literal notranslate"><span class="pre">tf.debugging.set_log_device_placement(True)</span></code> as the first statement of your program.
Enabling device placement logging causes any Tensor allocations or operations to be printed.</p>
<p>The NLP model and the Quora dataset</p>
<p>The <a class="reference external" href="https://www.kaggle.com/c/quora-insincere-questions-classification/data">Quora Insincere Questions Classification</a>
dataset is consistent of a large set of question which were asked on Quora platform with a label
to identify whether the question is sincere or insincere. An insincere question is defined
as a question intended to make a statement rather than look for helpful answers, i.e. toxic content. The dataset
can be downloaded from <a class="reference external" href="https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip">this link</a>.</p>
<p>Our task is to use a language model to classify these questions. We need to tokenize questions and
calculate the word embeddings using an NLP model afterwards. The output vector then can be attached
to a classification head that can be trained on the dataset.</p>
<p>We have to possibilities to get the embeddings. We can either use</p>
<ul class="simple">
<li><p>word-based representations or</p></li>
<li><p>context-based representations.</p></li>
</ul>
<p>In a <strong>word-based representation</strong> of a question, the embeddings for each word (token) is calculated
and the result will be the combined of all the embeddings, averaged over the question length.</p>
<p>Examples of pre-trained embeddings include:</p>
<ul class="simple">
<li><p><strong>Word2Vec</strong>: These are pre-trained embeddings of words learned from a large text corpora.
Word2Vec has been pre-trained on a corpus of news articles with  300 million tokens, resulting
in 300-dimensional vectors.</p></li>
<li><p><strong>GloVe</strong>: has been pre-trained on a corpus of tweets with 27 billion tokens, resulting
in 200-dimensional vectors.</p></li>
</ul>
<p>In a <strong>Context-based representations</strong>, instead of learning vectors for each word in the sentence,
a vector for a sentence on the whole, by taking into account the order of words and the set of
co-occurring words, is computed</p>
<p>Examples of deep contextualized vectors include:</p>
<ul class="simple">
<li><p><strong>Embeddings from Language Models (ELMo)</strong>: uses character-based word representations and
bidirectional LSTMs. The pre-trained model computes a contextualized vector of 1024 dimensions.
ELMo is available on Tensorflow Hub.</p></li>
<li><p><strong>Universal Sentence Encoder (USE)</strong>: The encoder uses a Transformer
architecture that uses attention mechanism to incorporate information about
the order and the collection of words. The pre-trained model of USE that returns
a vector of 512 dimensions is also available on Tensorflow Hub.</p></li>
<li><p><strong>Neural-Net Language Model (NNLM)</strong>: The model simultaneously learns representations
of words and probability functions for word sequences, allowing it to capture semantics of
a sentence.</p></li>
</ul>
<p>We will use <a class="reference external" href="https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1">a pretrained NNLM model</a>
available on Tensorflow Hub, that are trained on the English Google News 200B corpus,
and computes a vector of 128 dimensions.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-similarity.png"><img alt="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-similarity.png" src="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-similarity.png" style="width: 90%;" />
</a>
</figure>
<p><a class="reference external" href="https://tfhub.dev/google/universal-sentence-encoder/4">(Image Source)</a></p>
<p>The figure above can help us to better understand of how embeddings calculated using context-based
representation can be achieved. <em>Semantic similarity</em> is a measure of the degree to which two pieces
of text carry the same meaning. This is broadly useful in obtaining good coverage over the numerous
ways that a thought can be expressed using language without needing to manually enumerate them.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-classification.png"><img alt="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-classification.png" src="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-classification.png" style="width: 90%;" />
</a>
</figure>
<p><a class="reference external" href="https://tfhub.dev/google/universal-sentence-encoder/4">(Image Source)</a></p>
<div class="admonition-training-on-cpu-and-gpu exercise important admonition" id="exercise-0">
<p class="admonition-title">Training on CPU and GPU</p>
<p>You can find two neural networks for image classifier for the <cite>NNLM Language Model</cite> in
the github <a class="reference download internal" download="" href="_downloads/10696fc9ea0defd12e4b66e96821a29b/Transfer_Learning_NLP.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Transfer_Learning_NLP</span> <span class="pre">notebook</span></code></a>.
Try to train the model on CPU and GPU. Compare the results.</p>
<p>Can you place manually some parts on GPU and some on CPU?</p>
</div>
</section>
</section>
<span id="document-tf_mltgpus"></span><section id="distributed-training-in-tensorflow">
<span id="tf-mltgpus"></span><h2>Distributed training in TensorFlow<a class="headerlink" href="#distributed-training-in-tensorflow" title="Link to this heading"></a></h2>
<p>TensorFlow provides different methods to distribute training with minimal coding.
<code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> is a TensorFlow API to distribute training across
multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute
your existing models.</p>
<p>The main advantages of using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code>, according to TensorFlow, are:</p>
<ul class="simple">
<li><p>Easy to use and support multiple user segments,
including researchers, machine learning engineers, etc.</p></li>
<li><p>Provide good performance out of the box.</p></li>
<li><p>Easy switching between strategies.</p></li>
</ul>
<p>You can distribute training using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with a high-level
API like Keras <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code>, as we are familiar with, as well as custom training
loops (and, in general, any computation using TensorFlow).
You can use <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with very few changes to your code, because
the underlying components of TensorFlow have been changed to become strategy-aware.
This includes variables, layers, models, optimizers, metrics, summaries, and checkpoints.</p>
<section id="types-of-strategies">
<h3>Types of strategies<a class="headerlink" href="#types-of-strategies" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> covers several use cases along different axes.
Some of these combinations are currently supported. TensorFlow promises in the website
that others will be added in the future. Some of these axes are:</p>
<ul class="simple">
<li><p><strong>Synchronous vs asynchronous training</strong>: These are two common ways of distributing
training with data parallelism. In sync training, all workers train over different
slices of input data in sync, and aggregating gradients at each step. In async training,
all workers are independently training over the input data and updating variables asynchronously.
Typically sync training is supported via all-reduce and async through parameter server architecture.</p></li>
<li><p><strong>Hardware platform</strong>: You may want to scale your training onto multiple GPUs on
one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.</p></li>
</ul>
</section>
<section id="mirroredstrategy">
<h3>MirroredStrategy<a class="headerlink" href="#mirroredstrategy" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> supports synchronous distributed training on
multiple GPUs on one machine. It creates one replica per GPU device. Each variable
in the model is mirrored across all the replicas. Together, these variables form
a single conceptual variable called MirroredVariable. These variables are kept
in sync with each other by applying identical updates.</p>
<p>Efficient all-reduce algorithms are used to communicate the variable updates across
the devices. All-reduce aggregates tensors across all the devices by adding them up,
and makes them available on each device. It’s a fused algorithm that is very efficient
and can reduce the overhead of synchronization significantly. There are many all-reduce
algorithms and implementations available, depending on the type of communication available
between devices. By default, it uses the NVIDIA Collective Communication Library
(<a class="reference external" href="https://developer.nvidia.com/nccl">NCCL</a>) as the all-reduce implementation.</p>
<p>The main features of <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code>:</p>
<ul class="simple">
<li><p>All the variables and the model graph is replicated on the replicas.</p></li>
<li><p>Input is evenly distributed across the replicas.</p></li>
<li><p>Each replica calculates the loss and gradients for the input it received.</p></li>
<li><p>The gradients are synced across all the replicas by summing them.</p></li>
<li><p>After the sync, the same update is made to the copies of the variables on each replica.</p></li>
</ul>
<p>We can initiate the strategy Using</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>
</pre></div>
</div>
<p>If the list of devices is not specified in the <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code>
constructor, it will be auto-detected. For example, if we book a node with 5 GPUs,
the result of</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Number of devices: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">num_replicas_in_sync</span><span class="p">))</span>
</pre></div>
</div>
<p>will be</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Number<span class="w"> </span>of<span class="w"> </span>devices:<span class="w"> </span><span class="m">5</span>
</pre></div>
</div>
<p>Let’s apply the <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> to the Quora dataset using the NNLM model.
Since we already have download the dataset and saved as a pickle library, we can simply use
loading part from previous section.</p>
<p>We need to change the shape of dataset in order to feed it to the model. The
global batch sizes is equal to the batch size*number of replicas because each
replica will take a batch per run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">buffer_size</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">size</span>
<span class="n">batch_size_per_replica</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">global_batch_size</span> <span class="o">=</span> <span class="n">batch_size_per_replica</span> <span class="o">*</span> <span class="n">strategy</span><span class="o">.</span><span class="n">num_replicas_in_sync</span>
</pre></div>
</div>
<p>Transforming to the TensorFlow type tensor dataset and distributing among replicas</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">train_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
                <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>
                <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">))</span> <span class="c1">#.shuffle(buffer_size)</span>

<span class="n">valid_dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">valid_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
                <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">))</span>
</pre></div>
</div>
<p>We use <code class="docutils literal notranslate"><span class="pre">tf.keras.callbacks</span></code> for different purposes. Here, three callbacks are</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras.callbacks.TensorBoard</span></code>: writes a log for TensorBoard, which allows
you to visualize the graphs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tfdocs.modeling.EpochDots()</span></code>:  To reduce the logging noise use the tfdocs.EpochDots
which simply prints a . for each epoch, and a full set of metrics every 100 epochs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras.callbacks.EarlyStopping</span></code> : to avoid long and unnecessary training times.
This callback is set to monitor the val_loss.</p></li>
</ul>
<p>There are other callbacks which can be of interests for specific purposes. Nonetheless, we just use
the callbacks mentioned above.</p>
</section>
<section id="training-with-model-fit">
<h3>Training with <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code><a class="headerlink" href="#training-with-model-fit" title="Link to this heading"></a></h3>
<p>We define a function to instantiate the model, train it and returns the history object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_and_evaluate_model</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">hub_layer</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">[</span><span class="n">embed_size</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
                                      <span class="n">hub_layer</span><span class="p">,</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="p">])</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">),</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(),</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)])</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="c1">#train_df[&#39;question_text&#39;], train_df[&#39;target&#39;],</span>
                      <span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                      <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                      <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="c1">#(valid_df[&#39;question_text&#39;], valid_df[&#39;target&#39;]),</span>
                      <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tfdocs</span><span class="o">.</span><span class="n">modeling</span><span class="o">.</span><span class="n">EpochDots</span><span class="p">(),</span>
                                 <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">),</span>
                                 <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">logdir</span><span class="o">/</span><span class="n">name</span><span class="p">)],</span>
                      <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span>
                      <span class="p">)</span>
    <span class="k">return</span> <span class="n">history</span>
</pre></div>
</div>
<p>Now, we can simply call the usual <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code> function to train the model!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
   <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
   <span class="n">histories</span><span class="p">[</span><span class="s1">&#39;nnlm-en-dim128&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_and_evaluate_model</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;nnlm-en-dim128&#39;</span><span class="p">)</span>
   <span class="n">endt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span>
   <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> </span><span class="se">\n</span><span class="s2"> Time for </span><span class="si">{}</span><span class="s2"> ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="n">endt</span><span class="p">))</span>
</pre></div>
</div>
<p>Which will print</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">:</span><span class="mf">0.9326</span><span class="p">,</span>  <span class="n">loss</span><span class="p">:</span><span class="mf">0.2864</span><span class="p">,</span>  <span class="n">val_accuracy</span><span class="p">:</span><span class="mf">0.9385</span><span class="p">,</span>  <span class="n">val_loss</span><span class="p">:</span><span class="mf">0.1761</span><span class="p">,</span>
<span class="o">.....................</span>

<span class="n">Time</span> <span class="k">for</span> <span class="mf">85504.98509407043</span> <span class="n">ms</span>
</pre></div>
</div>
<p>That simple! <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> APIs to build the model and <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code> for training it
made the distributed training very easy.</p>
</section>
<section id="custom-loop-training">
<h3>Custom loop training<a class="headerlink" href="#custom-loop-training" title="Link to this heading"></a></h3>
<p>In cases where we need to customize the training procedure, we still are able to use
the <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code>. Here, the setup is a bit more elaborated and
needs some care. Let’s create a model using <code class="docutils literal notranslate"><span class="pre">tf.keras.Sequential</span></code>.</p>
<p>There is a difference to create the datasets in comparison to the previous section; as will be explained
below, here we need to add a dummy dimension to our dataset_inputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">buffer_size</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">size</span>
<span class="n">batch_size_per_replica</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">global_batch_size</span> <span class="o">=</span> <span class="n">batch_size_per_replica</span> <span class="o">*</span> <span class="n">strategy</span><span class="o">.</span><span class="n">num_replicas_in_sync</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">train_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">],</span> <span class="n">train_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">]))</span>
                <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>
                <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">))</span>

<span class="n">valid_dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">valid_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">],</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="kc">None</span><span class="p">]))</span>
                <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">))</span>
</pre></div>
</div>
<p>The model function can be defined using Keras Sequential API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">module_url</span> <span class="o">=</span> <span class="s2">&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1&quot;</span>
<span class="n">embeding_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">name_of_model</span> <span class="o">=</span> <span class="s1">&#39;nnlm-en-dim128/1&#39;</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_model</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">hub_layer</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[],</span>
                               <span class="n">output_shape</span><span class="o">=</span><span class="p">[</span><span class="n">embed_size</span><span class="p">],</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">hub_layer</span><span class="p">,</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</section>
<section id="calculation-of-loss-with-mirrored-strategy">
<h3>Calculation of loss with Mirrored Strategy:<a class="headerlink" href="#calculation-of-loss-with-mirrored-strategy" title="Link to this heading"></a></h3>
<p>Normally, on a single machine with 1 GPU/CPU, loss is divided by the number of examples
in the batch of input. How should the loss function be calculated within <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code>?</p>
<p>It requires special care. Why?</p>
<ul class="simple">
<li><p>For an example, let’s say you have 4 GPU’s and a batch size of 64. One batch of input is
distributed across the replicas (4 GPUs), each replica getting an input of size 16.</p></li>
<li><p>The model on each replica does a forward pass with its respective input and calculates the loss.
Now, instead of dividing the loss by the number of examples in its respective input
(<code class="docutils literal notranslate"><span class="pre">BATCH_SIZE_PER_REPLICA</span> <span class="pre">=</span> <span class="pre">16</span></code>), the loss should be divided by the <code class="docutils literal notranslate"><span class="pre">GLOBAL_BATCH_SIZE</span> <span class="pre">(64)</span></code>.</p></li>
</ul>
<p><strong>Why do this?</strong></p>
<ul class="simple">
<li><p>This needs to be done because after the gradients are calculated on each replica,
they are synced across the replicas by summing them.</p></li>
</ul>
<p>How to do this in TensorFlow?</p>
<ul class="simple">
<li><p>If we’re writing a custom training loop, as in this tutorial, you should sum
the per example losses and divide the sum by the GLOBAL_BATCH_SIZE:
<code class="docutils literal notranslate"><span class="pre">scale_loss</span> <span class="pre">=</span> <span class="pre">tf.reduce_sum(loss)</span> <span class="pre">*</span> <span class="pre">(1.</span> <span class="pre">/</span> <span class="pre">GLOBAL_BATCH_SIZE)</span></code>
or you can use tf.nn.compute_average_loss which takes the per example loss,
optional sample weights, and GLOBAL_BATCH_SIZE as arguments and returns the scaled loss.</p></li>
<li><p>If you are using regularization losses in your model then you need to scale
the loss value by number of replicas. You can do this by using the
<code class="docutils literal notranslate"><span class="pre">tf.nn.scale_regularization_loss</span></code> function.</p></li>
<li><p>Using <code class="docutils literal notranslate"><span class="pre">tf.reduce_mean</span></code> is not recommended. Doing so divides the loss by actual
per replica batch size which may vary step to step. More on this below.</p></li>
<li><p>This reduction and scaling is done automatically in keras <code class="docutils literal notranslate"><span class="pre">model.compile</span></code>
and <code class="docutils literal notranslate"><span class="pre">model.fit</span></code> (Why aren’t we grateful then?!)</p></li>
<li><p>If using <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> classes (as in the example below),
the loss reduction needs to be explicitly specified to be one of <code class="docutils literal notranslate"><span class="pre">NONE</span></code> or <code class="docutils literal notranslate"><span class="pre">SUM</span></code>.
<code class="docutils literal notranslate"><span class="pre">AUTO</span></code> and <code class="docutils literal notranslate"><span class="pre">SUM_OVER_BATCH_SIZE</span></code> are disallowed when used with <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code>.
<code class="docutils literal notranslate"><span class="pre">AUTO</span></code> is disallowed because the user should explicitly think about what reduction
they want to make sure it is correct in the distributed case. <code class="docutils literal notranslate"><span class="pre">SUM_OVER_BATCH_SIZE</span></code>
is disallowed because currently it would only divide by per replica batch size,
and leave the dividing by number of replicas to the user, which might be easy to miss.
So the user must do the reduction themselves explicitly.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is multi-dimensional, then average the <code class="docutils literal notranslate"><span class="pre">per_example_loss</span></code> across
the number of elements in each sample. For example, if the shape of <code class="docutils literal notranslate"><span class="pre">predictions</span></code>
is <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">H,</span> <span class="pre">W,</span> <span class="pre">n_classes)</span></code> and labels is <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>,
you will need to update <code class="docutils literal notranslate"><span class="pre">per_example_loss</span></code> like:
<code class="docutils literal notranslate"><span class="pre">per_example_loss</span> <span class="pre">/=</span> <span class="pre">tf.cast(tf.reduce_prod(tf.shape(labels)[1:]),</span> <span class="pre">tf.float32)</span></code></p></li>
</ul>
<div class="admonition-verify-the-shape-of-the-loss callout admonition" id="callout-0">
<p class="admonition-title">Verify the shape of the loss</p>
<p>Loss functions in tf.losses/tf.keras.losses typically return the average over
the last dimension of the input. The loss classes wrap these functions. Passing
<code class="docutils literal notranslate"><span class="pre">reduction=Reduction.NONE</span></code> when creating an instance of a loss class means
“no additional reduction”. For categorical losses with an example input shape of
<code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">W,</span> <span class="pre">H,</span> <span class="pre">n_classes]</span></code> the n_classes dimension is reduced. For pointwise
losses like <code class="docutils literal notranslate"><span class="pre">losses.mean_squared_error</span></code> or <code class="docutils literal notranslate"><span class="pre">losses.binary_crossentropy</span></code> include
a dummy axis so that <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">W,</span> <span class="pre">H,</span> <span class="pre">1]</span></code> is reduced to [batch, W, H].
Without the dummy axis <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">W,</span> <span class="pre">H]</span></code> will be incorrectly reduced to <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">W]</span></code>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
<span class="c1"># Set reduction to `none` so we can do the reduction afterwards and divide by</span>
<span class="c1"># global batch size.</span>

    <span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span>
        <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
        <span class="n">per_example_loss</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span><span class="n">per_example_loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

    <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;train_accuracy&#39;</span><span class="p">)</span>
    <span class="n">valid_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;valid_accuracy&#39;</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="o">=</span><span class="n">embeding_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name_of_model</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span>
</pre></div>
</div>
<p>By defining the metrics, we track the test loss and training and test accuracy.
We can use .result() to get the accumulated statistics at any time.</p>
<p>The next step is the calculations of loss, gradients and updating the gradients.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="n">texts</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="n">train_accuracy</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span><span class="w"> </span><span class="nf">valid_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="n">texts</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">v_loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="n">valid_accuracy</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">v_loss</span>
</pre></div>
</div>
<p>Before being able to run the training, we need to create <code class="docutils literal notranslate"><span class="pre">replica</span> <span class="pre">datasets</span></code> objects for
distributed training using</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_dist_dataset</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">valid_dist_dataset</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">run</span></code> command replicates the provided computation and runs it with
the distributed input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1"># `run` replicates the provided computation and runs it</span>
<span class="c1"># with the distributed input.</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">distributed_train_step</span><span class="p">(</span><span class="n">dataset_inputs</span><span class="p">):</span>
    <span class="n">per_replica_losses</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dataset_inputs</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">strategy</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">per_replica_losses</span><span class="p">,</span>
                         <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">distributed_valid_step</span><span class="p">(</span><span class="n">dataset_inputs</span><span class="p">):</span>
    <span class="n">per_replica_losses</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">valid_step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dataset_inputs</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">strategy</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">per_replica_losses</span><span class="p">,</span>
                         <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;epochs&#39;</span><span class="p">,</span> <span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="s1">&#39;valid_loss&#39;</span><span class="p">,</span> <span class="s1">&#39;train_acc&#39;</span><span class="p">,</span> <span class="s1">&#39;valid_acc&#39;</span><span class="p">])</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># TRAIN LOOP</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">train_dist_dataset</span><span class="p">:</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">distributed_train_step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">num_batches</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">num_batches</span>

    <span class="c1"># TEST LOOP</span>
    <span class="n">v_total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">v_num_batches</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">valid_dist_dataset</span><span class="p">:</span>
        <span class="n">v_total_loss</span> <span class="o">+=</span> <span class="n">distributed_valid_step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v_num_batches</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">v_total_loss</span> <span class="o">/</span> <span class="n">v_num_batches</span>

    <span class="n">template</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">{}</span><span class="s2">, Loss: </span><span class="si">{:.4f}</span><span class="s2">, Accuracy: </span><span class="si">{:.4f}</span><span class="s2">, Valid Loss: </span><span class="si">{:.4f}</span><span class="s2">, Valid Accuracy: </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span>
                         <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
                         <span class="n">valid_loss</span><span class="p">,</span>
                         <span class="n">valid_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

    <span class="n">history_df</span> <span class="o">=</span> <span class="n">history_df</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;epochs&#39;</span><span class="p">:</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                    <span class="s1">&#39;train_loss&#39;</span><span class="p">:</span><span class="n">train_loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                                    <span class="s1">&#39;valid_loss&#39;</span><span class="p">:</span><span class="n">valid_loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                                    <span class="s1">&#39;train_acc&#39;</span><span class="p">:</span><span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
                                    <span class="s1">&#39;valid_acc&#39;</span><span class="p">:</span><span class="n">valid_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">},</span>
                                  <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">train_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
    <span class="n">valid_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>

<span class="n">endt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">timelp</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">endt</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
<p>The output will be</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1653</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.3007</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1384</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.6289</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1416</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.7266</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1334</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.3125</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1371</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.9104</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1311</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.0195</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1322</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.0705</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1266</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.1172</span>
<span class="n">Epoch</span> <span class="mi">5</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1271</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.2275</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1306</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.8242</span>
<span class="n">Epoch</span> <span class="mi">6</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1225</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.3569</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1329</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.6289</span>
<span class="n">Epoch</span> <span class="mi">7</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1174</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.5476</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1367</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.0195</span>
<span class="n">Epoch</span> <span class="mi">8</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1124</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.7629</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1374</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.8242</span>
<span class="n">Epoch</span> <span class="mi">9</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1073</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">95.9566</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1430</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.8242</span>
<span class="n">Epoch</span> <span class="mi">10</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1024</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">96.1298</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1481</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.6289</span>
<span class="n">Epoch</span> <span class="mi">11</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0970</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">96.3626</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1521</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.4336</span>
<span class="n">Epoch</span> <span class="mi">12</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0918</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">96.5173</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1577</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.5312</span>
<span class="n">Epoch</span> <span class="mi">13</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0867</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">96.7639</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1723</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.4336</span>
<span class="n">Epoch</span> <span class="mi">14</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0814</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">96.9439</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1681</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.1406</span>
<span class="n">Epoch</span> <span class="mi">15</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0774</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">97.0573</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1741</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.4336</span>
<span class="n">Epoch</span> <span class="mi">16</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0719</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">97.2963</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1874</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">93.8477</span>
<span class="n">Epoch</span> <span class="mi">17</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0666</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">97.5559</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1871</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">93.5547</span>
<span class="n">Epoch</span> <span class="mi">18</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0622</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">97.7168</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1976</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">94.5312</span>
<span class="n">Epoch</span> <span class="mi">19</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0579</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">97.8577</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2086</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">93.9453</span>
<span class="n">Epoch</span> <span class="mi">20</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0533</span><span class="p">,</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">98.1013</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2278</span><span class="p">,</span> <span class="n">Valid</span> <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">93.7500</span>
<span class="n">Elapsed</span> <span class="n">time</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span> <span class="mf">71537.03</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">for</span></code> loop that marches though the input (training or test datasets) can be implemented
using other methods too. For example, one can make use of Python iterator functions
<code class="docutils literal notranslate"><span class="pre">iter</span></code> and <code class="docutils literal notranslate"><span class="pre">next</span></code>. Using iterator we have more control over the number of steps we wish to
execute the commands. Another way of implementing could be using <code class="docutils literal notranslate"><span class="pre">for</span></code> inside <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>.</p>
</section>
<section id="parameterserverstrategy">
<h3>ParameterServerStrategy<a class="headerlink" href="#parameterserverstrategy" title="Link to this heading"></a></h3>
<p>Parameter server training is a common data-parallel method to scale up model training on
multiple machines. A parameter server training cluster consists of workers and parameter servers.
Variables are created on parameter servers and they are read and updated by workers in each step.
Similar to <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>, it can be implemented using Keras API <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code> or custom
training loop.</p>
<p>In TensorFlow 2, parameter server training uses a central coordinator-based architecture via the
<code class="docutils literal notranslate"><span class="pre">tf.distribute.experimental.coordinator.ClusterCoordinator</span></code> class. In this implementation,
the worker and parameter server tasks run <code class="docutils literal notranslate"><span class="pre">tf.distribute.Servers</span></code> that listen for tasks
from the coordinator. The coordinator creates resources, dispatches training tasks, writes
checkpoints, and deals with task failures.</p>
<p>In the programming running on the coordinator, one uses a <code class="docutils literal notranslate"><span class="pre">ParameterServerStrategy</span></code> object to
define a training step and use a <code class="docutils literal notranslate"><span class="pre">ClusterCoordinator</span></code> to dispatch training steps to remote workers.</p>
</section>
<section id="multiworkermirroredstrategy">
<h3>MultiWorkerMirroredStrategy<a class="headerlink" href="#multiworkermirroredstrategy" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code> is very similar to <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>. It implements
synchronous distributed training across multiple workers, each with potentially multiple GPUs.
Similar to tf.distribute.MirroredStrategy, it creates copies of all variables in the model on
each device across all workers. One of the key differences to get multi worker training going,
as compared to multi-GPU training, is the multi-worker setup. The ‘TF_CONFIG’ environment variable
is the standard way in TensorFlow to specify the cluster configuration to each worker that is part
of the cluster. In other words, the main difference between <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> and
<code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> is While in <em>MultiWorkerMirroredStrategy</em>, the network setup is necessary,
in <em>MirroredStrategy</em> the setup is automatically topology aware meaning that we don’t need
to setup the network and interconnects.</p>
<div class="admonition-which-one-is-faster exercise important admonition" id="exercise-0">
<p class="admonition-title">Which one is faster?</p>
<p>Comment out the <code class="docutils literal notranslate"><span class="pre">EarlyStopping</span></code> callback, fix the number of epochs to <code class="docutils literal notranslate"><span class="pre">20</span></code> and
train the model using <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code> API:
1. On 4 GPUs using <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>
2. On a single GPU using pinning method
and compare the elapsed time with the number you obtained above.</p>
<p>Which of these methods faster? Do you have any explanation for that?</p>
</div>
<div class="admonition-evaluation-for-a-custom-training exercise important admonition" id="exercise-1">
<p class="admonition-title">Evaluation for a custom training</p>
<p>Evaluate the performance of the metrics on the tests datasets for custom training loop.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">eval_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;eval_accuracy&#39;</span><span class="p">)</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">eval_step</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">eval_accuracy</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">eval_accuracy</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="k">for</span> <span class="n">texts</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">valid_dataset</span><span class="p">:</span>
    <span class="n">eval_step</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The model accuracy : </span><span class="si">{:5.2f}</span><span class="s1">%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eval_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-advanced-custom-training-loop-for-svhn exercise important admonition" id="exercise-2">
<p class="admonition-title">(Advanced) Custom training loop for SVHN</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">SVHN_class</span></code> code provided in <a class="reference internal" href="#document-tf_intro"><span class="doc">TensorFlow on a single GPU</span></a> and write a custom training loop using
<code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>.</p>
</div>
</section>
</section>
<span id="document-hvd_intro"></span><section id="intoduction-to-horovod">
<span id="hvd-intro"></span><h2>Intoduction to Horovod<a class="headerlink" href="#intoduction-to-horovod" title="Link to this heading"></a></h2>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="https://horovod.readthedocs.io/en/stable/_static/logo.png"><img alt="https://horovod.readthedocs.io/en/stable/_static/logo.png" src="https://horovod.readthedocs.io/en/stable/_static/logo.png" style="width: 40%;" />
</a>
<figcaption>
<p><span class="caption-text"><a class="reference external" href="https://horovod.ai">(Image Source)</a></span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="why-horovod">
<h3>Why Horovod<a class="headerlink" href="#why-horovod" title="Link to this heading"></a></h3>
<p>Horovod was developed at Uber with the primary motivation of making it easy to
take a single-GPU training script and successfully scale it to train across many
GPUs in parallel. This has two aspects:</p>
<ul class="simple">
<li><p>How much modification does one have to make to a program to make it distributed,
and how easy is it to run it?</p></li>
<li><p>How much faster would it run in distributed mode?</p></li>
</ul>
<p>What researchers at Uber discovered was that the MPI model to be much more straightforward
and require far less code changes than previous solutions such as Distributed TensorFlow with
parameter servers. Once a training script has been written for scale with Horovod, it can run
on a single-GPU, multiple-GPUs, or even multiple hosts without any further code changes.</p>
<p>In addition to being easy to use, Horovod is fast. Below is a chart representing the benchmark
that was done on 128 servers with 4 Pascal GPUs each connected by RoCE-capable 25 Gbit/s network:</p>
<img alt="scaling" src="https://user-images.githubusercontent.com/16640218/38965607-bf5c46ca-4332-11e8-895a-b9c137e86013.png" />
<p>Horovod achieves 90% scaling efficiency for both Inception V3 and ResNet-101, and
68% scaling efficiency for VGG-16. While installing MPI and NCCL itself may seem like an extra hassle,
it only needs to be done once by the team dealing with infrastructure, while everyone else in the company
who builds the models can enjoy the simplicity of training them at scale. Plus, in modern clusters where
GPUs are available, MPI and NCCL are readily installed. Installation of Horovod is not as difficult.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1802.05799.pdf">Alex Sergeev and Mike Del Balso</a>,
the researchers behind the development of Horovod at Uber,
published an excellent history and review of <a class="reference external" href="https://eng.uber.com/horovod/">the Hovorod</a>.
Here are some points they mentioned in their article:</p>
<ul class="simple">
<li><p>The first try for distributed training was based on TensorFlow distributed method.
However, they experienced two major difficulties: 1. Difficulty of following instructions
given by TensorFlow. In particular, they found the newly introduced concepts by TensorFlow
for distributed training causes <em>hard-to-diagnose bugs that slowed training</em>.</p></li>
<li><p>The second issue dealt with the challenge of computing at Uber’s scale.
After running a few benchmarks, they found that they could not get the standard
distributed TensorFlow to scale as well as our services required.
For example, about half of their resources was lost due to scaling inefficiencies
when training on 128 GPUs.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/image4-1-768x330.png"><img alt="https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/image4-1-768x330.png" src="https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/image4-1-768x330.png" style="width: 90%;" />
</a>
</figure>
<ul class="simple">
<li><p>An article by Facebook researchers entitled  “<a class="reference external" href="https://scontent-arn2-1.xx.fbcdn.net/v/t39.8562-6/240818965_455586748763065_8609026679315857149_n.pdf?_nc_cat=111&amp;ccb=1-5&amp;_nc_sid=ad8a9d&amp;_nc_ohc=CtM02FZ33KwAX8zcuRy&amp;_nc_ht=scontent-arn2-1.xx&amp;oh=00_AT_dczJ90lEGzFc3ugwhrl3vI3fnIvBVhWsxpQrWaamVTQ&amp;oe=62647A23">Accurate, Large Minibatch SGD:
Training ImageNet in 1 Hour,</a>
demonstrating their training of a ResNet-50 network in one hour on 256
GPUs by combining principles of data parallelism peaked their interests.</p></li>
<li><p>A paper published by Baidu researchers in early 2017, “Bringing HPC Techniques to
Deep Learning,” evangelizing a different algorithm for averaging gradients
and communicating those gradients to all nodes, called <strong>ring-allreduce</strong>.
The algorithm was based on the approach introduced in the 2009 paper
“<a class="reference external" href="http://www.cs.fsu.edu/~xyuan/paper/09jpdc.pdf">Bandwidth Optimal All-reduce Algorithms for Clusters of Workstations</a>”
by Patarasuk and Yuan.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/figure4-7564694e76d08e091ce453f681515e59.png"><img alt="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/figure4-7564694e76d08e091ce453f681515e59.png" src="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/figure4-7564694e76d08e091ce453f681515e59.png" style="width: 80%;" />
</a>
</figure>
<p><a class="reference external" href="https://www.oreilly.com/content/distributed-tensorflow/">(Image Source)</a></p>
<ul class="simple">
<li><p>The realization that a ring-allreduce approach can improve both usability
and performance motivated them to work on our own implementation to address
Uber’s TensorFlow needs.</p></li>
<li><p>Horovod (Khorovod) is named after a traditional Russian folk dance in which
performers dance with linked arms in a circle, much like how distributed
TensorFlow processes use Horovod to communicate with each other.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="https://cdni.russiatoday.com/rbthmedia/images/all/2016/11/10/khorovod_rian_02920393_b.jpg"><img alt="https://cdni.russiatoday.com/rbthmedia/images/all/2016/11/10/khorovod_rian_02920393_b.jpg" src="https://cdni.russiatoday.com/rbthmedia/images/all/2016/11/10/khorovod_rian_02920393_b.jpg" style="width: 75%;" />
</a>
</figure>
<p><a class="reference external" href="https://www.rbth.com/arts/2016/12/07/8-facts-about-the-khorovod-russias-oldest-dance_654295">(Image Source)</a></p>
<ul class="simple">
<li><p>They replaced the Baidu ring-allreduce implementation with NCCL.
NCCL provides a highly optimized version of ring-allreduce.
NCCL 2 introduced the ability to run ring-allreduce across multiple machines,
enabling us to take advantage of its many performance boosting optimizations.</p></li>
</ul>
</section>
<section id="main-concept">
<h3>Main concept<a class="headerlink" href="#main-concept" title="Link to this heading"></a></h3>
<p>Horovod’s connection to MPI is deep, and for those familiar with MPI programming,
much of what you program to distribute model training with Horovod will feel familiar.</p>
<p>Four core principles that Horovod is based on are the MPI concepts: <em>size</em>, <em>rank</em>, <em>local rank</em>,
<em>allreduce</em>, <em>allgather</em>, <em>broadcast</em>, and <em>alltoall</em>.
These are best explained by example.
Say we launched a training script on 4 servers, each having 4 GPUs.
If we launched one copy of the script per GPU:</p>
<ul>
<li><p><strong>Size</strong> would be the number of processes, in this case, 16.</p></li>
<li><p><strong>Rank</strong> would be the unique process ID from 0 to 15 (size - 1).</p></li>
<li><p><strong>Local rank</strong> would be the unique process ID within the server from 0 to 3.</p></li>
<li><p><strong>Allreduce</strong> is an operation that aggregates data among multiple processes and
distributes results back to them. Allreduce is used to average dense tensors.</p>
<img alt="Allreduce" src="http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_allreduce_1.png" />
</li>
<li><p><strong>Allgather</strong> is an operation that gathers data from all processes on every process.
Allgather is used to collect values of sparse tensors.</p>
<img alt="allgather" src="http://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/allgather.png" />
</li>
<li><p><strong>Broadcast</strong> is an operation that broadcasts data from one process, identified by
root rank, onto every other process.</p>
<img alt="broadcast" src="http://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/broadcast_pattern.png" />
</li>
<li><p><strong>Alltoall</strong> is an operation to exchange data between all processes.
Alltoall may be useful to implement neural networks with advanced architectures that span multiple devices.</p></li>
</ul>
<p><a class="reference external" href="http://mpitutorial.com/tutorials">(Images Source)</a></p>
<p>Horovod, as with MPI, strictly follows the Single-Program Multiple-Data (SPMD)
paradigm where we implement the instruction flow of multiple processes in the
same file/program. Because multiple processes are executing code in parallel,
we have to take care about race conditions and also the synchronization of participating
processes.</p>
<p>Horovod assigns a unique numerical ID or rank (an MPI concept) to each process executing
the program. This rank can be accessed programmatically. As you will see below when writing Horovod code, by identifying a process’s rank programmatically in the code we can take steps such as:</p>
<ul class="simple">
<li><p>Pin that process to its own exclusive GPU.</p></li>
<li><p>Utilize a single rank for broadcasting values that need to be used uniformly by all ranks.</p></li>
<li><p>Utilize a single rank for collecting and/or reducing values produced by all ranks.</p></li>
<li><p>Utilize a single rank for logging or writing to disk.</p></li>
</ul>
</section>
<section id="how-to-use-horovod">
<h3>How to use Horovod<a class="headerlink" href="#how-to-use-horovod" title="Link to this heading"></a></h3>
<p>To use Horovod, we should add the following to the program:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">hvd.init()</span></code> to initialize Horovod.</p></li>
</ol>
<p>2. Pin each GPU to a single process. This is to avoid resource contention. With the typical
setup of one GPU per process, set this to local rank.
The first process on the server will be allocated the first GPU, the second process
will be allocated the second GPU, and so forth.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Horovod: pin GPU to be used to process local rank (one GPU per process)</span>
<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>3. Print Verbose Logs Only on the First Worker. When running on several N processors,
all N TensorFlow processes printed their progress to stdout (standard output).
We only want to see the state of the output once at any given time.
To accomplish this, we can arbitrarily select a single
rank to display the training progress. By convention, we typically call rank
0 the “root” rank and use it for logistical work such as I/O when only one
rank is required.</p>
<p>4. Add Distributed Optimizer. In the previous two sections we ran with multiple processes,
but each process was running completely independently – this is not data parallel training,
it is just multiple processes running serial training at the same time. The key step to
make the training data parallel is to average out gradients across all workers, so that
all workers are updating with the same gradients and thus moving in the same direction.
Horovod implements an operation that averages gradients across workers. Deploying this in
your code is very straightforward and just requires wrapping an existing
optimizer (<code class="docutils literal notranslate"><span class="pre">keras.optimizers.Optimizer</span></code>) with a Horovod distributed
optimizer (<code class="docutils literal notranslate"><span class="pre">horovod.keras.DistributedOptimizer</span></code>).</p>
<p>5. Initialize Random Weights on Only One Processor. Data parallel stochastic gradient
descent, at least in its traditionally defined sequential algorithm, requires weights to
be synchronized between all processors. We already know that this is accomplished for
backpropagation by averaging out the gradients among all processors prior to the weight
updates. Then the only other required step is for the weights to be synchronized initially.
Assuming we start from the beginning of the training (we’ll handle checkpoint/restart
later), this means that every processor needs to have the same random weights.
In a previous section, we mentioned that the first worker would broadcast parameters to
the rest of the workers. We will use
<code class="docutils literal notranslate"><span class="pre">horovod.keras.callbacks.BroadcastGlobalVariablesCallback</span></code> to make this happen.</p>
<p>6. Modify Training Loop to Execute Fewer Steps Per Epoch. As it stands, we are running
the same number of steps per epoch for the serial training implementation. But since we
have increased the number of workers by a factor of <code class="docutils literal notranslate"><span class="pre">N</span></code>, that means we’re doing <code class="docutils literal notranslate"><span class="pre">N</span></code> times
more work (when we sum the amount of work done over all processes). Our target was to
get the same answer in less time (that is, to speed up the training), so we want to keep
the total amount of work done the same (that is, to process the same number of examples
in the dataset). This means we need to do a factor of <code class="docutils literal notranslate"><span class="pre">N</span></code> fewer steps per epoch, so the
number of steps goes to <code class="docutils literal notranslate"><span class="pre">steps_per_epoch</span> <span class="pre">/</span> <span class="pre">number_of_workers</span></code>.
We will also speed up validation by validating <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">*</span> <span class="pre">num_test_iterations</span> <span class="pre">/</span> <span class="pre">number_of_workers</span></code>
steps on each worker. While we could just do num_test_iterations / number_of_workers on each
worker to get a linear speedup in the validation, the multiplier 3 provides over-sampling of
the validation data and helps to increase the probability that every validation example will be evaluated.</p>
<p>7. Average Validation Results Among Workers. Since we are not validating the full dataset
on each worker anymore, each worker will have different validation results. To improve
validation metric quality and reduce variance, we will average validation results among
all workers. To do so, we can use <code class="docutils literal notranslate"><span class="pre">horovod.keras.callbacks.MetricAverageCallback</span></code>.</p>
<p>8. Do Checkpointing Logic Only Using the Root Worker. The most important issue is that
there can be a race condition while writing the checkpoint to a file. If every rank
finishes the epoch at the same time, they might be writing to the same filename, and
this could result in corrupted data. But more to the point, we don’t even need to do
this: by construction in synchronous data parallel SGD, every rank has the same copy
of the weights at all times, so only one worker needs to write the checkpoint. As usual,
our convention will be that the root worker (rank 0) handles this.</p>
<p>9. Increase the learning rate. Given a fixed batch size per GPU, the effective batch size
for training increases when you use more GPUs, since we average out the gradients among
all processors. Standard practice is to scale the learning rate by the same factor that
you have scaled the batch size – that is, by the number of workers present. This can be
done so that the training script does not change for single-process runs, since in that
case you just multiply by 1.</p>
<p>The reason we do this is that the error of a mean of <em>n</em> samples (random variables) with
finite variance <span class="math notranslate nohighlight">\(\sigma\)</span> is approximately <span class="math notranslate nohighlight">\(\sigma/\sqrt(n)\)</span> when <span class="math notranslate nohighlight">\(n\)</span> is large (see the
<a class="reference external" href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>).
Hence, learning rates should be scaled at least with <span class="math notranslate nohighlight">\(\sqrt(k)\)</span> when using <span class="math notranslate nohighlight">\(k\)</span> times
bigger batch sizes in order to preserve the variance of the batch-averaged gradient.
In practice we use linear scaling, often out of convenience, although in different
circumstances one or the other may be superior in practice.</p>
<p>10. (Optional) Add learning rate warmup. Many models are sensitive to using a large learning
rate immediately after initialization and can benefit from learning rate warmup.
We saw earlier that we typically scale the learning rate linear with batch sizes.
But if the batch size gets large enough, then the learning rate will be very high,
and the network tends to diverge, especially in the very first few iterations.
We counteract this by gently ramping the learning rate to the target learning rate.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">lr_schedule</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">25</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1e-1</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">35</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1e-2</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">return</span> <span class="mf">1e-3</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>

<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateScheduler</span><span class="p">(</span><span class="n">lr_schedule</span><span class="p">))</span>
</pre></div>
</div>
<p>In practice, the idea is to start training with a lower learning rate and gradually raise it
to a target learning rate over a few epochs. Horovod has the convenient
<code class="docutils literal notranslate"><span class="pre">horovod.keras.callbacks.LearningRateWarmupCallback</span></code> for the Keras API that implements that logic.
By default it will, over the first 5 epochs, gradually increase the learning rate from
<code class="docutils literal notranslate"><span class="pre">initial</span> <span class="pre">learning</span> <span class="pre">rate</span> <span class="pre">/</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">workers</span></code> up to initial learning rate.</p>
</div></blockquote>
<p>Once the script is transformed to a proper form, it can be launched using <code class="docutils literal notranslate"><span class="pre">horovodrun</span></code>
command. Here are some general examples for how to run the train a model on a machine with 4 GPUs.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>horovodrun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span>-H<span class="w"> </span>localhost:4<span class="w"> </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<p>And for running on 4 machines with 4 GPUs each, we use</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>horovodrun<span class="w"> </span>-np<span class="w"> </span><span class="m">16</span><span class="w"> </span>-H<span class="w"> </span>server1:4,server2:4,server3:4,server4:4<span class="w"> </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<p>It is also possible to run the script using Open MPI without the horovodrun wrapper.
The launch command for the first example using <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> would be</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-bind-to<span class="w"> </span>none<span class="w"> </span>-map-by<span class="w"> </span>slot<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-x<span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO<span class="w"> </span>-x<span class="w"> </span>LD_LIBRARY_PATH<span class="w"> </span>-x<span class="w"> </span>PATH<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-mca<span class="w"> </span>pml<span class="w"> </span>ob1<span class="w"> </span>-mca<span class="w"> </span>btl<span class="w"> </span>^openib<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<p>And for the second example</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span>server1:4,server2:4,server3:4,server4:4<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-bind-to<span class="w"> </span>none<span class="w"> </span>-map-by<span class="w"> </span>slot<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-x<span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO<span class="w"> </span>-x<span class="w"> </span>LD_LIBRARY_PATH<span class="w"> </span>-x<span class="w"> </span>PATH<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-mca<span class="w"> </span>pml<span class="w"> </span>ob1<span class="w"> </span>-mca<span class="w"> </span>btl<span class="w"> </span>^openib<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>python<span class="w"> </span>train.py
</pre></div>
</div>
</section>
<section id="base-model">
<h3>Base model<a class="headerlink" href="#base-model" title="Link to this heading"></a></h3>
<p>The base model is the same as <a class="reference internal" href="#document-tf_intro"><span class="doc">TensorFlow on a single GPU</span></a> section, an NLP model, given below.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>

<span class="c1"># Suppress tensorflow logging</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CPP_MIN_LOG_LEVEL&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow_hub</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hub</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Version: &quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hub version: &quot;</span><span class="p">,</span> <span class="n">hub</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU is&quot;</span><span class="p">,</span> <span class="s2">&quot;available&quot;</span> <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;NOT AVAILABLE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of GPUs :&#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)))</span>

<span class="n">logdir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span><span class="o">/</span><span class="s2">&quot;tensorboard_logs&quot;</span>
<span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">logdir</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Parse input arguments</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Transfer Learning Example&#39;</span><span class="p">,</span>
                                 <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--log-dir&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">logdir</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;tensorboard log directory&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch-size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;input batch size for training&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;number of epochs to train&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--base-lr&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;learning rate for a single GPU&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--patience&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Number of epochs that meet target before stopping&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--use-checkpointing&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># Steps</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip&#39;</span><span class="p">,</span>
             <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;zip&#39;</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">)</span>

<span class="n">train_df</span><span class="p">,</span> <span class="n">remaining</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">valid_df</span><span class="p">,</span> <span class="n">_</span>  <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">remaining</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">remaining</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The shape of training </span><span class="si">{}</span><span class="s2"> and validation </span><span class="si">{}</span><span class="s2"> datasets.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##-------------------------##&quot;</span><span class="p">)</span>

<span class="n">buffer_size</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">size</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">train_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">valid_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">module_url</span> <span class="o">=</span> <span class="s2">&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1&quot;</span>
<span class="n">embeding_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">name_of_model</span> <span class="o">=</span> <span class="s1">&#39;nnlm-en-dim128&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch size :&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_checkpointing</span><span class="p">:</span>
    <span class="c1"># callbacks.append(tfdocs.modeling.EpochDots()),</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">patience</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">)),</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">logdir</span><span class="o">/</span><span class="n">name</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_and_evaluate_model_ds</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">hub_layer</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">[</span><span class="n">embed_size</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
                                      <span class="n">hub_layer</span><span class="p">,</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="p">])</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">base_lr</span><span class="p">),</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(),</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)])</span>

    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                      <span class="n">epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
                      <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
                      <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                      <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span>
                      <span class="p">)</span>
    <span class="k">return</span> <span class="n">history</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;GPU:0&quot;</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">##-------------------------##&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training starts ...&quot;</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">train_and_evaluate_model_ds</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="o">=</span><span class="n">embeding_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name_of_model</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">endt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsed Time: </span><span class="si">{}</span><span class="s2"> ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="n">endt</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##-------------------------##&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Save above script as <code class="docutils literal notranslate"><span class="pre">Transfer_Learning_NLP.py</span></code> (or directly download <a class="reference download internal" download="" href="_downloads/113108d50e2dc0302b11ce92b88d2a9a/Transfer_Learning_NLP.py"><code class="xref download docutils literal notranslate"><span class="pre">Transfer_Learning_NLP.py</span></code></a> )
and follow the instructions given in <a class="reference internal" href="#document-setup"><span class="doc">Access to Vega</span></a> to start a notebook. Once the Jupyter notebook started, you can open a terminal from drop down on
the right side of the notebook and watch the usage of the GPUs using <code class="docutils literal notranslate"><span class="pre">watch</span> <span class="pre">-n</span> <span class="pre">0.5</span> <span class="pre">nvidia-smi</span></code>.
In the Jupyter notebook, we need to install TensorFlow Hub first</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow_hub</span>
</pre></div>
</div>
<p>And suppress the standard outputs of TensorFlow</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="c1"># Suppress tensorflow logging outputs</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CPP_MIN_LOG_LEVEL&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>
</pre></div>
</div>
<p>Now, we can run the base model using for 10 epochs and with the default batch size of 32.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">python</span> <span class="n">Transfer_Learning_NLP</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">epochs</span> <span class="mi">10</span>
</pre></div>
</div>
<div class="admonition-elapsed-time-as-a-function-of-batch-size exercise important admonition" id="exercise-0">
<p class="admonition-title">Elapsed time as a function of batch size</p>
<p>As you perhaps noticed, it took a rather long time to finish the job.
Do you know any parameter that can be tuned to make the calculations faster?
How does the elapsed time scale with the batch size?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Increasing the batch size reduces the training time. The reduction must be
almost linear.</p>
</div>
</div>
</section>
<section id="training-with-model-fit">
<h3>Training with <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code><a class="headerlink" href="#training-with-model-fit" title="Link to this heading"></a></h3>
<p>Applying the 10-step processors mentioned above to the <code class="docutils literal notranslate"><span class="pre">Transfer_Learning_NLP.py</span></code>, we will have</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>

<span class="c1"># Suppress tensorflow logging outputs</span>
<span class="c1"># os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &quot;2&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow_hub</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hub</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">logdir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span><span class="o">/</span><span class="s2">&quot;tensorboard_logs&quot;</span>
<span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">logdir</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Parse input arguments</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Transfer Learning Example&#39;</span><span class="p">,</span>
                                 <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--log-dir&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">logdir</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;tensorboard log directory&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num-worker&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;number of workers for training part&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch-size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;input batch size for training&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--base-lr&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;learning rate for a single GPU&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;number of epochs to train&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--momentum&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;SGD momentum&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--target-accuracy&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">.96</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Target accuracy to stop training&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--patience&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Number of epochs that meet target before stopping&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--use-checkpointing&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">)</span>

<span class="c1"># Step 10: register `--warmup-epochs`</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--warmup-epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;number of warmup epochs&#39;</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># Define a function for a simple learning rate decay over time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">lr_schedule</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">25</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1e-1</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">35</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1e-2</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>
    <span class="k">return</span> <span class="mf">1e-3</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span>

<span class="c1">##### Steps</span>
<span class="c1"># Step 1: import Horovod</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">horovod.tensorflow.keras</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hvd</span>

<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="c1"># Nomrally Step 2: pin to a GPU</span>
<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>

<span class="c1"># Step 2: but in our case</span>
<span class="c1"># gpus = tf.config.list_physical_devices(&#39;GPU&#39;)</span>
<span class="c1"># if gpus:</span>
<span class="c1">#    tf.config.experimental.set_memory_growth(gpus[0], True)</span>

<span class="c1"># Step 3: only set `verbose` to `1` if this is the root worker.</span>
<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Version: &quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hub version: &quot;</span><span class="p">,</span> <span class="n">hub</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU is&quot;</span><span class="p">,</span> <span class="s2">&quot;available&quot;</span> <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;NOT AVAILABLE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of GPUs :&#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)))</span>
    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1">#####</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip&#39;</span><span class="p">,</span>
             <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;zip&#39;</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;dataset.pkl&#39;</span><span class="p">)</span>

<span class="n">train_df</span><span class="p">,</span> <span class="n">remaining</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">valid_df</span><span class="p">,</span> <span class="n">_</span>  <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">remaining</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.09</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">remaining</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The shape of training </span><span class="si">{}</span><span class="s2"> and validation </span><span class="si">{}</span><span class="s2"> datasets.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##-------------------------##&quot;</span><span class="p">)</span>

<span class="n">buffer_size</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#train_dataset = tf.data.Dataset.from_tensor_slices((train_df.question_text.values, train_df.target.values)).repeat(args.epochs*2).shuffle(buffer_size).batch(args.batch_size)</span>
<span class="c1">#valid_dataset = tf.data.Dataset.from_tensor_slices((valid_df.question_text.values, valid_df.target.values)).repeat(args.epochs*2).batch(args.batch_size)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">train_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">valid_df</span><span class="o">.</span><span class="n">question_text</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">valid_df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">))</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">module_url</span> <span class="o">=</span> <span class="s2">&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1&quot;</span>
<span class="n">embeding_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">name_of_model</span> <span class="o">=</span> <span class="s1">&#39;nnlm-en-dim128&#39;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_model</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">hub_layer</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">[</span><span class="n">embed_size</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">hub_layer</span><span class="p">,</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)])</span>

    <span class="c1"># Step 9: Scale the learning rate by the number of workers.</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">base_lr</span> <span class="o">*</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">momentum</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span>
    <span class="c1"># opt = tf.optimizers.Adam(learning_rate=args.base_lr * hvd.size())</span>

    <span class="c1">#Step 4: Wrap the optimizer in a Horovod distributed optimizer</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span>
                                   <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                   <span class="n">average_aggregated_gradients</span><span class="o">=</span><span class="kc">True</span>
                                   <span class="p">)</span>

    <span class="c1"># For Horovod: We specify `experimental_run_tf_function=False` to ensure TensorFlow</span>
    <span class="c1"># uses hvd.DistributedOptimizer() to compute gradients.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(),</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)],</span>
                <span class="n">experimental_run_tf_function</span> <span class="o">=</span> <span class="kc">False</span>
                 <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Step 5: broadcast initial variable states from the first worker to</span>
<span class="c1"># all others by adding the broadcast global variables callback.</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># Step 7: average the metrics among workers at the end of every epoch</span>
<span class="c1"># by adding the metric average callback.</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">MetricAverageCallback</span><span class="p">())</span>

<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_checkpointing</span><span class="p">:</span>
    <span class="c1"># TensorFlow normal callbacks</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">apped</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">))</span>

    <span class="c1"># Step 8: checkpointing should only be done on the root worker.</span>
    <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">callbacks</span><span class="o">.</span><span class="n">apped</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">logdir</span><span class="o">/</span><span class="n">name_of_model</span><span class="p">))</span>

<span class="c1"># Step 10: implement a LR warmup over `args.warmup_epochs`</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateWarmupCallback</span><span class="p">(</span><span class="n">initial_lr</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span><span class="p">,</span> <span class="n">warmup_epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">warmup_epochs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">))</span>

<span class="c1"># Step 10: replace with the Horovod learning rate scheduler,</span>
<span class="c1"># taking care not to start until after warmup is complete</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateScheduleCallback</span><span class="p">(</span><span class="n">initial_lr</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">base_lr</span><span class="p">,</span> <span class="n">start_epoch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">warmup_epochs</span><span class="p">,</span> <span class="n">multiplier</span><span class="o">=</span><span class="n">lr_schedule</span><span class="p">))</span>


<span class="c1"># Creating model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">(</span><span class="n">module_url</span><span class="p">,</span> <span class="n">embed_size</span><span class="o">=</span><span class="n">embeding_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name_of_model</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">##-------------------------##&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training starts ...&quot;</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                    <span class="c1"># Step 6: keep the total number of steps the same despite of an increased number of workers</span>
                    <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="p">)</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                    <span class="c1"># steps_per_epoch = ( 5000 ) // hvd.size(),</span>
                    <span class="n">workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_worker</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
                    <span class="c1">#Step 6: set this value to be 3 * num_test_iterations / number_of_workers</span>
                    <span class="n">validation_steps</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="n">valid_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="p">)</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                    <span class="c1"># validation_steps = ( 5000 ) // hvd.size(),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
                    <span class="c1"># use_multiprocessing = True,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>

<span class="n">endt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span>

<span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsed Time: </span><span class="si">{}</span><span class="s2"> ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="n">endt</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##-------------------------##&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We can also download <a class="reference download internal" download="" href="_downloads/23c91e6565c3bc46858a123c488ec6b5/Transfer_Learning_NLP_Horovod.py"><code class="xref download docutils literal notranslate"><span class="pre">the</span> <span class="pre">python</span> <span class="pre">script</span></code></a>
from the Github repository.</p>
<div class="admonition-does-the-training-scale exercise important admonition" id="exercise-1">
<p class="admonition-title">Does the training scale?</p>
<p>Now you can launch the Horovod training on the number of GPUs you booked in
your Jupyter notebook using</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="err">$</span><span class="n">np</span> <span class="o">-</span><span class="n">H</span> <span class="n">localhost</span><span class="p">:</span><span class="err">$</span><span class="n">np</span> <span class="o">-</span><span class="n">bind</span><span class="o">-</span><span class="n">to</span> <span class="n">none</span> <span class="o">-</span><span class="nb">map</span><span class="o">-</span><span class="n">by</span> <span class="n">slot</span> \
<span class="n">python</span> <span class="n">Transfer_Learning_NLP_Horovod</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">epochs</span> <span class="mi">10</span> <span class="o">--</span><span class="n">batch</span><span class="o">-</span><span class="n">size</span> <span class="mi">64</span>
</pre></div>
</div>
<p>Does the total time scale with the number of processers <code class="docutils literal notranslate"><span class="pre">$np</span></code>? Can you
explain the reason?</p>
<p>What does happen when you increase the batch size?</p>
</div>
<div class="admonition-horovodize-a-cnn-model exercise important admonition" id="exercise-2">
<p class="admonition-title">Horovodize a CNN model.</p>
<p>You can find a CNN model for an MNIST dataset <a class="reference download internal" download="" href="_downloads/97408403ab2b1c4500a5d230517b90be/SVHN_class.py"><code class="xref download docutils literal notranslate"><span class="pre">here</span></code></a>. Apply
the steps mentioned above and test your script.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>The solution can be found <a class="reference download internal" download="" href="_downloads/e900458a6cd35ddf94bb0dbb5e704712/SVHN_class_Horovod.py"><code class="xref download docutils literal notranslate"><span class="pre">here</span></code></a>.</p>
</div>
</div>
<div class="admonition-advanced-custom-training exercise important admonition" id="exercise-3">
<p class="admonition-title">Advanced - Custom training</p>
<p>Instead of using <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code>, write a custom training loop within the framework of Horovod.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>Two main differences that should be made are:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Definig the loss function using Horovod</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">first_batch</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
          <span class="n">probs</span> <span class="o">=</span> <span class="n">mnist_model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

    <span class="c1"># Horovod: add Horovod Distributed GradientTape.</span>
    <span class="n">tape</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedGradientTape</span><span class="p">(</span><span class="n">tape</span><span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss_value</span><span class="p">,</span> <span class="n">mnist_model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">mnist_model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="c1"># Horovod: broadcast initial variable states from rank 0 to all other processes.</span>
    <span class="c1"># This is necessary to ensure consistent initialization of all workers when</span>
    <span class="c1"># training is started with random weights or restored from a checkpoint.</span>
    <span class="c1"># Please see `the documentation &lt;https://horovod.readthedocs.io/en/stable/api.html#horovod.tensorflow.broadcast_variables&gt;`_.</span>
    <span class="c1"># Note: broadcast should be done after the first gradient step to ensure optimizer</span>
    <span class="c1"># initialization.</span>

    <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">mnist_model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss_value</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Looping over the dataset</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10000</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">())):</span>
    <span class="n">loss_value</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Step #</span><span class="si">%d</span><span class="se">\t</span><span class="s1">Loss: </span><span class="si">%.6f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">loss_value</span><span class="p">))</span>
</pre></div>
</div>
</div></blockquote>
</div>
</div>
</section>
<section id="analysis-of-performance">
<h3>Analysis of Performance<a class="headerlink" href="#analysis-of-performance" title="Link to this heading"></a></h3>
<p>Horovod has the ability to record the timeline of its activity, called Horovod Timeline.
To record a Horovod Timeline, set the <code class="docutils literal notranslate"><span class="pre">--timeline-filename</span></code> command line argument to the
location of the timeline file to be created. This file is only recorded on rank 0,
but it contains information about activity of all workers.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>horovodrun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span>--timeline-filename<span class="w"> </span>/path/to/timeline.json<span class="w"> </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<p>You can then open the timeline file using the <code class="docutils literal notranslate"><span class="pre">chrome://tracing</span></code> facility of
the Chrome-based browsers. Nonetheless, we do not employ the profiler in this workshop.</p>
</section>
</section>
<span id="document-train_contain"></span><section id="training-neural-networks-using-containers">
<span id="train-contain"></span><h2>Training Neural Networks using Containers<a class="headerlink" href="#training-neural-networks-using-containers" title="Link to this heading"></a></h2>
<p>We discussed already different methods of scaling
for the training of the network. The essential part of any scaling
scheme is the communication among the processors whether it is
a bunch of CPUs or GPUs. For the communication between CPUs,
<a class="reference external" href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI (Message Passing Interface)</a> is a widely used standard.
MPI is a well-established standard and it is used for
exchanging messages/data between processes in a parallel application.
If you’ve been involved in developing or working with computational
science software, you may already be familiar with MPI and running MPI
applications.</p>
<p>As for the communication between GPUs, depending on vendor providing GPUs,
there are library, similar to MPI. GPUs which are available on Vega cluster
are NVIDIA GPUs. The standard for communication for such GPUs is the NVIDIA
Collective Communication Library <a class="reference external" href="https://developer.nvidia.com/nccl">(NCCL)</a>
(NCCL, pronounced “Nickel”),
partly as discussed in <a class="reference internal" href="#document-tf_mltgpus"><span class="doc">Distributed training in TensorFlow</span></a>. Nvidia introduces NCCL as a library
that enables multi-GPU and multi-node communication primitives optimized
for NVIDIA GPUs and Networking that are topology-aware and can be easily integrated
into applications.
NCCL implements both collective communication and point-to-point send/receive
primitives. It is not a full-blown parallel programming framework; rather, it is
a library focused on accelerating inter-GPU communication.</p>
<p>NCCL provides the following collective communication primitives :</p>
<ul class="simple">
<li><p>AllReduce</p></li>
<li><p>Broadcast</p></li>
<li><p>Reduce</p></li>
<li><p>AllGather</p></li>
<li><p>ReduceScatter</p></li>
</ul>
<p>Additionally, it allows for point-to-point send/receive communication which allows
for scatter, gather, or all-to-all operations (<a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html">NCCL doc</a>).</p>
<p>In this section of the workshop, we will see how these two libraries will be in
use during the training of a network using containers.</p>
<section id="mpi-codes-with-singularity-containers">
<h3>MPI codes with Singularity containers<a class="headerlink" href="#mpi-codes-with-singularity-containers" title="Link to this heading"></a></h3>
<p>We’ve already seen that building Singularity containers can be
impractical without root access. While it is unlikely to have
root access on a large institutional, regional or national cluster,
building a container directly on the target platform is not normally
an option, the Vega staff cluster has generously given us the necessary
privileges for creating containers.</p>
<p>One of the reasons we mentioned for using containers is their portability across
different platforms/machines. However, it is not the case when we need to
create containers for training a network on specific cluster. If our target platform
uses <a class="reference external" href="https://www.open-mpi.org/">OpenMPI</a>,
one of the two widely used source MPI implementations, we can
build/install a compatible OpenMPI version on our local build
platform, or directly within the image as part of the image build
process. We can then build our code that requires MPI, either
interactively in an image sandbox or via a definition file.</p>
<p>While building a container on a local system that is intended for use
on a remote HPC platform does provide some level of portability, if
you’re after the best possible performance, it can present some
issues. The version of MPI in the container will need to be built and
configured to support the hardware on your target platform if the best
possible performance is to be achieved. Where a platform has
specialist hardware with proprietary drivers, building on a different
platform with different hardware present means that building with the
right driver support for optimal performance is not likely to be
possible. This is especially true if the version of MPI available is
different (but compatible). Singularity’s <a class="reference external" href="https://sylabs.io/guides/3.9/user-guide/mpi.html">MPI documentation</a> highlights two
different models for working with MPI codes namely, the hybrid and bind methods.</p>
<p>The basic idea behind the Hybrid Approach is when you execute a
Singularity container with MPI code, you will call <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code>
or a similar launcher on the <code class="docutils literal notranslate"><span class="pre">singularity</span></code> command itself.
The MPI process outside of the container will then work in tandem with MPI
inside the container and the containerized MPI code to instantiate the job.
Similarly, the basic idea behind the Bind Approach is
to start the MPI application by calling the MPI launcher (e.g., <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>)
from the host. The main difference between the hybrid and bind approach is
the fact that with the bind approach, the container usually does not include
any MPI implementation. This means that SingularityCE needs to mount/bind the
MPI available on the host into the container.</p>
<p>The <a class="reference external" href="https://sylabs.io/guides/3.9/user-guide/mpi.html#hybrid-model">hybrid model</a> that
we’ll be looking at here involves using the MPI executable from the
MPI installation on the host system to launch singularity and run the
application within the container.  The application in the container is
linked against and uses the MPI installation within the container
which, in turn, communicates with the MPI daemon process running on
the host system. In the following sections we’ll look at building a
Singularity image containing a small MPI application that can then be
run using the hybrid model.</p>
</section>
<section id="the-simplest-mpi-example">
<h3>The simplest MPI example<a class="headerlink" href="#the-simplest-mpi-example" title="Link to this heading"></a></h3>
<p>Let’s start with the simplest example of running an app within a container. This
example will show the backbone of scaling an app using MPI primitives.</p>
<p>Create a new directory and save the <code class="docutils literal notranslate"><span class="pre">mpitest.c</span></code> given below.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;mpi.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdlib.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="kt">int</span><span class="w"> </span><span class="n">rc</span><span class="p">;</span>
<span class="w">      </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">;</span>
<span class="w">      </span><span class="kt">int</span><span class="w"> </span><span class="n">myrank</span><span class="p">;</span>

<span class="w">      </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Init</span><span class="w"> </span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">              </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MPI_Init() failed&quot;</span><span class="p">);</span>
<span class="w">              </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>

<span class="w">      </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Comm_size</span><span class="w"> </span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">              </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MPI_Comm_size() failed&quot;</span><span class="p">);</span>
<span class="w">              </span><span class="k">goto</span><span class="w"> </span><span class="n">exit_with_error</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>

<span class="w">      </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Comm_rank</span><span class="w"> </span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">myrank</span><span class="p">);</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">              </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MPI_Comm_rank() failed&quot;</span><span class="p">);</span>
<span class="w">              </span><span class="k">goto</span><span class="w"> </span><span class="n">exit_with_error</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>

<span class="w">      </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stdout</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Hello, I am rank %d/%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">myrank</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="w">      </span><span class="n">MPI_Finalize</span><span class="p">();</span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_SUCCESS</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A possible def file for the app above is given below.</p>
<div class="highlight-docker notranslate"><div class="highlight"><pre><span></span>Bootstrap:<span class="w"> </span>docker
<span class="k">From</span><span class="s">:</span><span class="w"> </span>ubuntu:18.04

%files
<span class="w">  </span>mpitest.c<span class="w"> </span>/opt

%environment
<span class="w">  </span>#<span class="w"> </span>Point<span class="w"> </span>to<span class="w"> </span>OMPI<span class="w"> </span>binaries,<span class="w"> </span>libraries,<span class="w"> </span>man<span class="w"> </span>pages
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/lib:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">MANPATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/share/man:</span><span class="nv">$MANPATH</span><span class="s2">&quot;</span>

%post
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing required packages...&quot;</span>
<span class="w">  </span>apt-get<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>wget<span class="w"> </span>git<span class="w"> </span>bash<span class="w"> </span>gcc<span class="w"> </span>gfortran<span class="w"> </span>g++<span class="w"> </span>make<span class="w"> </span>file

<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing Open MPI&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_VERSION</span><span class="o">=</span><span class="m">4</span>.0.5
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_URL</span><span class="o">=</span><span class="s2">&quot;https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-</span><span class="nv">$OMPI_VERSION</span><span class="s2">.tar.bz2&quot;</span>
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/tmp/ompi
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/opt
<span class="w">  </span>#<span class="w"> </span>Download
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>wget<span class="w"> </span>-O<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2<span class="w"> </span><span class="nv">$OMPI_URL</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>tar<span class="w"> </span>-xjf<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2
<span class="w">  </span>#<span class="w"> </span>Compile<span class="w"> </span>and<span class="w"> </span>install
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi/openmpi-<span class="nv">$OMPI_VERSION</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./configure<span class="w"> </span>--prefix<span class="o">=</span><span class="nv">$OMPI_DIR</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span>-j8<span class="w"> </span>install

<span class="w">  </span>#<span class="w"> </span>Set<span class="w"> </span>env<span class="w"> </span>variables<span class="w"> </span>so<span class="w"> </span>we<span class="w"> </span>can<span class="w"> </span>compile<span class="w"> </span>our<span class="w"> </span>application
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/bin:<span class="nv">$PATH</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>

<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Compiling the MPI application...&quot;</span>
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/opt<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>mpicc<span class="w"> </span>-o<span class="w"> </span>mpitest<span class="w"> </span>mpitest.c
</pre></div>
</div>
<p>A quick recap of what the above definition file is doing:</p>
<blockquote>
<div><ul class="simple">
<li><p>The image is being bootstrapped from the <code class="docutils literal notranslate"><span class="pre">ubuntu:18.04</span></code> Docker
image.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">%environment</span></code> section: Set an environment variable that
will be available within all containers run from the generated
image.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section:</p>
<ul>
<li><p>Ubuntu’s <code class="docutils literal notranslate"><span class="pre">apt-get</span></code> package manager is used to update the package
directory and then install the compilers and other libraries
required for the OpenMPI build.</p></li>
<li><p>The OpenMPI <code class="docutils literal notranslate"><span class="pre">.tar.gz</span></code> file is extracted and the configure, build and
install steps are run.</p></li>
</ul>
</li>
</ul>
</div></blockquote>
<p>We have the option of either compiling <code class="docutils literal notranslate"><span class="pre">mpitest.c</span></code> directly on the cluster,
or compiling it inside the container. For learning purposes, let’s compile the
code inside the container.</p>
<p>To create the container we use</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>build<span class="w"> </span>--fakeroot<span class="w"> </span>--sandbox<span class="w"> </span>mpi_hybrid<span class="w"> </span>mpi_hybrid.def
singularity<span class="w"> </span>build<span class="w"> </span>mpi_hybrid.sif<span class="w"> </span>mpi_hybrid
</pre></div>
</div>
<p>And to run the code on <code class="docutils literal notranslate"><span class="pre">8</span></code> processors we should use
the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>mpi_hybrid.sif<span class="w"> </span>/opt/mpitest
</pre></div>
</div>
<p>The output should look like</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">1</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">2</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">3</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">4</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">5</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">6</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">7</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">0</span>/8
</pre></div>
</div>
<p>Let’s analyze what just happened. The <code class="docutils literal notranslate"><span class="pre">mpitest</span></code> app sent a <code class="docutils literal notranslate"><span class="pre">Hello,</span> <span class="pre">I</span> <span class="pre">am</span> <span class="pre">rank</span> <span class="pre">X/Y</span></code>
message from within the container sent across <code class="docutils literal notranslate"><span class="pre">8</span></code> processors. For this process to
happen, <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> runs a copy of the <code class="docutils literal notranslate"><span class="pre">mpi_hybrid.sif</span></code> container across the <code class="docutils literal notranslate"><span class="pre">8</span></code>
processors and execute <code class="docutils literal notranslate"><span class="pre">/opt/mpitest</span></code> inside the container as we asked.</p>
</section>
<section id="mpi-ping-pong">
<h3>MPI Ping-Pong<a class="headerlink" href="#mpi-ping-pong" title="Link to this heading"></a></h3>
<p>The above example, did not have communicating between CPUs. To have a full-fledged
MPI app that can scale with number of CPUs, communication is a must. Let’s take a
look at how communication works using MPI within a container. To that end, we will
use what is a common test for MPI communication. <cite>Pingpong test</cite> is a routine during
which a message is sent and received in pingpong fashion between two processor.
As result of such test, the latency and bandwidth can be calculated.</p>
<p>One can either use the <code class="docutils literal notranslate"><span class="pre">pingpong</span></code> method as given in <a class="reference external" href="https://www.hlrs.de/about-us/media-publications/teaching-training-material/">HLRS MPI course</a>
below</p>
<div class="highlight-fortran notranslate"><div class="highlight"><pre><span></span><span class="k">PROGRAM </span><span class="n">pingpong</span>

<span class="c">!==============================================================!</span>
<span class="c">!                                                              !</span>
<span class="c">! This file has been written as a sample solution to an        !</span>
<span class="c">! exercise in a course given at the High Performance           !</span>
<span class="c">! Computing Centre Stuttgart (HLRS).                           !</span>
<span class="c">! The examples are based on the examples in the MPI course of  !</span>
<span class="c">! the Edinburgh Parallel Computing Centre (EPCC).              !</span>
<span class="c">! It is made freely available with the understanding that      !</span>
<span class="c">! every copy of this file must include this header and that    !</span>
<span class="c">! HLRS and EPCC take no responsibility for the use of the      !</span>
<span class="c">! enclosed teaching material.                                  !</span>
<span class="c">!                                                              !</span>
<span class="c">! Authors: Joel Malard, Alan Simpson,            (EPCC)        !</span>
<span class="c">!          Rolf Rabenseifner, Traugott Streicher (HLRS)        !</span>
<span class="c">!                                                              !</span>
<span class="c">! Contact: rabenseifner@hlrs.de                                !</span>
<span class="c">!                                                              !</span>
<span class="c">! Purpose: A program to try MPI_Ssend and MPI_Recv.            !</span>
<span class="c">!                                                              !</span>
<span class="c">! Contents: F-Source                                           !</span>
<span class="c">!                                                              !</span>
<span class="c">!==============================================================!</span>

<span class="w">  </span><span class="k">USE </span><span class="n">mpi</span>

<span class="w">  </span><span class="k">IMPLICIT NONE</span>

<span class="k">  </span><span class="kt">INTEGER </span><span class="n">proc_a</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="p">(</span><span class="n">proc_a</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">proc_b</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="p">(</span><span class="n">proc_b</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">ping</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="p">(</span><span class="n">ping</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">pong</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="p">(</span><span class="n">pong</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">number_of_messages</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="w"> </span><span class="p">(</span><span class="n">number_of_messages</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">start_length</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="w"> </span><span class="p">(</span><span class="n">start_length</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">length_factor</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="w"> </span><span class="p">(</span><span class="n">length_factor</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">max_length</span><span class="w">                </span><span class="c">! 2 Mega</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="w"> </span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">2097152</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">number_package_sizes</span>
<span class="w">  </span><span class="k">PARAMETER</span><span class="w"> </span><span class="p">(</span><span class="n">number_package_sizes</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span>
<span class="w">  </span><span class="kt">INTEGER</span><span class="p">(</span><span class="nb">KIND</span><span class="o">=</span><span class="n">MPI_ADDRESS_KIND</span><span class="p">)</span><span class="w"> </span><span class="n">lb</span><span class="p">,</span><span class="w"> </span><span class="n">size_of_real</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">length</span>

<span class="w">  </span><span class="kt">DOUBLE PRECISION </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">finish</span><span class="p">,</span><span class="w"> </span><span class="nb">time</span><span class="p">,</span><span class="w"> </span><span class="n">transfer_time</span>
<span class="w">  </span><span class="kt">INTEGER </span><span class="n">status</span><span class="p">(</span><span class="n">MPI_STATUS_SIZE</span><span class="p">)</span>

<span class="w">  </span><span class="kt">REAL </span><span class="n">buffer</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span>

<span class="w">  </span><span class="kt">INTEGER </span><span class="n">ierror</span><span class="p">,</span><span class="w"> </span><span class="n">my_rank</span><span class="p">,</span><span class="w"> </span><span class="n">size</span>


<span class="w">  </span><span class="k">CALL </span><span class="n">MPI_INIT</span><span class="p">(</span><span class="n">ierror</span><span class="p">)</span>

<span class="w">  </span><span class="k">CALL </span><span class="n">MPI_COMM_RANK</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">my_rank</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">  </span><span class="k">CALL </span><span class="n">MPI_TYPE_GET_EXTENT</span><span class="p">(</span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">lb</span><span class="p">,</span><span class="w"> </span><span class="n">size_of_real</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>

<span class="w">  </span><span class="k">IF</span><span class="w"> </span><span class="p">(</span><span class="n">my_rank</span><span class="w"> </span><span class="p">.</span><span class="n">EQ</span><span class="p">.</span><span class="w"> </span><span class="n">proc_a</span><span class="p">)</span><span class="w"> </span><span class="k">THEN</span>
<span class="k">     WRITE</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="s2">&quot;message size   transfertime    bandwidth&quot;</span>
<span class="w">  </span><span class="k">END IF</span>

<span class="k">  </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">start_length</span>

<span class="w">  </span><span class="k">DO </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">number_package_sizes</span>

<span class="w">     </span><span class="k">IF</span><span class="w"> </span><span class="p">(</span><span class="n">my_rank</span><span class="w"> </span><span class="p">.</span><span class="n">EQ</span><span class="p">.</span><span class="w"> </span><span class="n">proc_a</span><span class="p">)</span><span class="w"> </span><span class="k">THEN</span>
<span class="k">           CALL </span><span class="n">MPI_SEND</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">proc_b</span><span class="p">,</span><span class="w"> </span><span class="n">ping</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">           </span><span class="k">CALL </span><span class="n">MPI_RECV</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">proc_b</span><span class="p">,</span><span class="w"> </span><span class="n">pong</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">status</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">     </span><span class="k">ELSE IF</span><span class="w"> </span><span class="p">(</span><span class="n">my_rank</span><span class="w"> </span><span class="p">.</span><span class="n">EQ</span><span class="p">.</span><span class="w"> </span><span class="n">proc_b</span><span class="p">)</span><span class="w"> </span><span class="k">THEN</span>
<span class="k">           CALL </span><span class="n">MPI_RECV</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">proc_a</span><span class="p">,</span><span class="w"> </span><span class="n">ping</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">status</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">           </span><span class="k">CALL </span><span class="n">MPI_SEND</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">proc_a</span><span class="p">,</span><span class="w"> </span><span class="n">pong</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">     </span><span class="k">END IF</span>

<span class="k">     </span><span class="n">start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_WTIME</span><span class="p">()</span>

<span class="w">     </span><span class="k">DO </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">number_of_messages</span>

<span class="w">        </span><span class="k">IF</span><span class="w"> </span><span class="p">(</span><span class="n">my_rank</span><span class="w"> </span><span class="p">.</span><span class="n">EQ</span><span class="p">.</span><span class="w"> </span><span class="n">proc_a</span><span class="p">)</span><span class="w"> </span><span class="k">THEN</span>
<span class="k">           CALL </span><span class="n">MPI_SEND</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">proc_b</span><span class="p">,</span><span class="w"> </span><span class="n">ping</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">           </span><span class="k">CALL </span><span class="n">MPI_RECV</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">proc_b</span><span class="p">,</span><span class="w"> </span><span class="n">pong</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">status</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">        </span><span class="k">ELSE IF</span><span class="w"> </span><span class="p">(</span><span class="n">my_rank</span><span class="w"> </span><span class="p">.</span><span class="n">EQ</span><span class="p">.</span><span class="w"> </span><span class="n">proc_b</span><span class="p">)</span><span class="w"> </span><span class="k">THEN</span>
<span class="k">           CALL </span><span class="n">MPI_RECV</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">proc_a</span><span class="p">,</span><span class="w"> </span><span class="n">ping</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">status</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">           </span><span class="k">CALL </span><span class="n">MPI_SEND</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_REAL</span><span class="p">,</span><span class="w"> </span><span class="n">proc_a</span><span class="p">,</span><span class="w"> </span><span class="n">pong</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">ierror</span><span class="p">)</span>
<span class="w">        </span><span class="k">END IF</span>

<span class="k">     END DO</span>

<span class="k">     </span><span class="n">finish</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_WTIME</span><span class="p">()</span>

<span class="w">     </span><span class="k">IF</span><span class="w"> </span><span class="p">(</span><span class="n">my_rank</span><span class="w"> </span><span class="p">.</span><span class="n">EQ</span><span class="p">.</span><span class="w"> </span><span class="n">proc_a</span><span class="p">)</span><span class="w"> </span><span class="k">THEN</span>

<span class="k">        </span><span class="nb">time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">finish</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span>
<span class="w">        </span><span class="n">transfer_time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">number_of_messages</span><span class="p">)</span>

<span class="w">        </span><span class="k">WRITE</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="nb">INT</span><span class="p">(</span><span class="n">length</span><span class="o">*</span><span class="n">size_of_real</span><span class="p">),</span><span class="s1">&#39;bytes  &#39;</span><span class="p">,</span><span class="w"> </span><span class="n">transfer_time</span><span class="o">*</span><span class="mf">1e6</span><span class="p">,</span><span class="s1">&#39;usec  &#39;</span><span class="p">,</span><span class="w"> </span><span class="mf">1e-6</span><span class="o">*</span><span class="n">length</span><span class="o">*</span><span class="n">size_of_real</span><span class="o">/</span><span class="n">transfer_time</span><span class="p">,</span><span class="s1">&#39;MB/s&#39;</span>

<span class="w">     </span><span class="k">END IF</span>

<span class="k">     </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">length_factor</span>

<span class="w">  </span><span class="k">END DO</span>

<span class="k">  CALL </span><span class="n">MPI_FINALIZE</span><span class="p">(</span><span class="n">ierror</span><span class="p">)</span>

<span class="k">END PROGRAM</span>
</pre></div>
</div>
<p>Or a similar code from <a class="reference external" href="http://www.archer.ac.uk/training/course-material/2018/07/mpi-epcc/index.php">EPCC - University of Edinburgh</a></p>
<div class="highlight-fortran notranslate"><div class="highlight"><pre><span></span><span class="c">!</span>
<span class="c">! Program in which 2 processes repeatedly pass a message back and forth</span>
<span class="c">!</span>
<span class="c">! The same data is sent from A to B, then returned from B to A.</span>
<span class="c">!</span>

<span class="k">program </span><span class="n">pingpong</span>
<span class="k">implicit none</span>
<span class="k">include</span><span class="w"> </span><span class="s1">&#39;mpif.h&#39;</span>

<span class="kt">integer</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">ierr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">rank</span><span class="p">,</span><span class="w"> </span><span class="n">comm</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">numiter</span>
<span class="kt">integer</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">status</span><span class="p">(</span><span class="n">MPI_STATUS_SIZE</span><span class="p">)</span>
<span class="kt">integer</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">tag1</span><span class="p">,</span><span class="w"> </span><span class="n">tag2</span><span class="p">,</span><span class="w"> </span><span class="n">extent</span>
<span class="kt">character</span><span class="o">*</span><span class="mi">10</span><span class="w"> </span><span class="n">temp_char10</span>
<span class="kt">integer</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iargc</span>
<span class="kt">real</span><span class="p">,</span><span class="w"> </span><span class="k">allocatable</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">sbuffer</span><span class="p">(:)</span>
<span class="kt">double precision</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">tstart</span><span class="p">,</span><span class="w"> </span><span class="n">tstop</span><span class="p">,</span><span class="w"> </span><span class="nb">time</span><span class="p">,</span><span class="w"> </span><span class="n">totmess</span>

<span class="n">comm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span>
<span class="n">tag1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span>
<span class="n">tag2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span>

<span class="k">call </span><span class="n">MPI_INIT</span><span class="p">(</span><span class="n">ierr</span><span class="p">)</span>
<span class="k">call </span><span class="n">MPI_COMM_RANK</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span><span class="n">rank</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>
<span class="k">call </span><span class="n">MPI_COMM_SIZE</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>

<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nb">iargc</span><span class="p">()</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="k"> if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="p">.</span><span class="n">eq</span><span class="p">.</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="k">   write</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="s1">&#39;Usage: pingpong &lt;array length&gt; &lt;number of iterations&gt;&#39;</span>
<span class="w"> </span><span class="k">end if</span>

<span class="k"> call </span><span class="n">mpi_finalize</span><span class="p">(</span><span class="n">ierr</span><span class="p">)</span>
<span class="w"> </span><span class="k">stop</span>
<span class="k">end if</span>


<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="p">.</span><span class="n">gt</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="k"> print</span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Rank not participating&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">rank</span>
<span class="k">end if</span>


<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="p">.</span><span class="n">eq</span><span class="p">.</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="k"> call </span><span class="nb">getarg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">temp_char10</span><span class="p">)</span>
<span class="w"> </span><span class="k">read</span><span class="p">(</span><span class="n">temp_char10</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="n">length</span>
<span class="w"> </span><span class="k">call </span><span class="nb">getarg</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">temp_char10</span><span class="p">)</span>
<span class="w"> </span><span class="k">read</span><span class="p">(</span><span class="n">temp_char10</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="n">numiter</span>

<span class="w"> </span><span class="k">print</span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Array length, number of iterations = &#39;</span>
<span class="w"> </span><span class="k">print</span><span class="o">*</span><span class="p">,</span><span class="w">  </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">numiter</span>
<span class="k">end if</span>

<span class="k">call </span><span class="n">MPI_BCAST</span><span class="p">(</span><span class="n">length</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">MPI_INTEGER</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">comm</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>
<span class="k">call </span><span class="n">MPI_BCAST</span><span class="p">(</span><span class="n">numiter</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">MPI_INTEGER</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">comm</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>

<span class="c">! Must be run on at least 2 processors</span>
<span class="k">if</span><span class="p">(</span><span class="n">size</span><span class="p">.</span><span class="n">lt</span><span class="p">.</span><span class="mi">2</span><span class="p">)</span><span class="k">then</span>
<span class="k"> if</span><span class="p">(</span><span class="n">rank</span><span class="p">.</span><span class="n">eq</span><span class="p">.</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">write</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="s1">&#39; The code must be run on at least 2 processors.&#39;</span>
<span class="w"> </span><span class="k">call </span><span class="n">MPI_FINALIZE</span><span class="p">(</span><span class="n">ierr</span><span class="p">)</span>
<span class="w"> </span><span class="k">stop</span>
<span class="k">endif</span>

<span class="c">! Allocate array</span>
<span class="k">allocate</span><span class="p">(</span><span class="n">sbuffer</span><span class="p">(</span><span class="n">length</span><span class="p">))</span>

<span class="c">! Send &#39;buffer&#39; back and forth between rank 0 and rank 1.</span>
<span class="k">do </span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">length</span>
<span class="w"> </span><span class="n">sbuffer</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="mf">0.d0</span>
<span class="k">enddo</span>

<span class="c">! Start timing the parallel part here.</span>
<span class="k">call </span><span class="n">MPI_BARRIER</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>
<span class="n">tstart</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Wtime</span><span class="p">()</span>

<span class="k">do </span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">numiter</span>
<span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="p">.</span><span class="n">eq</span><span class="p">.</span><span class="mi">0</span><span class="p">)</span><span class="k">then</span>
<span class="k">  call </span><span class="n">MPI_SSEND</span><span class="p">(</span><span class="n">sbuffer</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">length</span><span class="p">,</span><span class="n">MPI_REAL</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">tag1</span><span class="p">,</span><span class="n">comm</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>
<span class="w">  </span><span class="k">call </span><span class="n">MPI_RECV</span><span class="p">(</span><span class="n">sbuffer</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">length</span><span class="p">,</span><span class="n">MPI_REAL</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">tag2</span><span class="p">,</span><span class="n">comm</span><span class="p">,</span><span class="n">status</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>

<span class="w"> </span><span class="k">else if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="p">.</span><span class="n">eq</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span><span class="k">then</span>
<span class="k">  call </span><span class="n">MPI_RECV</span><span class="p">(</span><span class="n">sbuffer</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">length</span><span class="p">,</span><span class="n">MPI_REAL</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">tag1</span><span class="p">,</span><span class="n">comm</span><span class="p">,</span><span class="n">status</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>
<span class="w">  </span><span class="k">call </span><span class="n">MPI_SSEND</span><span class="p">(</span><span class="n">sbuffer</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">length</span><span class="p">,</span><span class="n">MPI_REAL</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">tag2</span><span class="p">,</span><span class="n">comm</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>
<span class="w"> </span><span class="k">endif</span>
<span class="k">enddo</span>


<span class="n">tstop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Wtime</span><span class="p">()</span>
<span class="nb">time</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">tstop</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">tstart</span>

<span class="k">call </span><span class="n">MPI_TYPE_SIZE</span><span class="p">(</span><span class="n">MPI_REAL</span><span class="p">,</span><span class="n">extent</span><span class="p">,</span><span class="n">ierr</span><span class="p">)</span>

<span class="k">if</span><span class="p">(</span><span class="n">rank</span><span class="p">.</span><span class="n">eq</span><span class="p">.</span><span class="mi">0</span><span class="p">)</span><span class="k">then</span>
<span class="k"> </span><span class="n">totmess</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">2.d0</span><span class="o">*</span><span class="n">extent</span><span class="o">*</span><span class="n">length</span><span class="o">/</span><span class="mi">102</span><span class="mf">4.d0</span><span class="o">*</span><span class="n">numiter</span><span class="o">/</span><span class="mi">102</span><span class="mf">4.d0</span>
<span class="w"> </span><span class="k">write</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="s1">&#39; Ping-Pong of twice &#39;</span><span class="p">,</span><span class="n">extent</span><span class="o">*</span><span class="n">length</span><span class="p">,</span><span class="s1">&#39; bytes, for &#39;</span><span class="p">,</span><span class="n">numiter</span><span class="p">,</span><span class="s1">&#39; times.&#39;</span>
<span class="w"> </span><span class="k">write</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="s1">&#39;Total computing time is &#39;</span><span class="p">,</span><span class="nb">time</span><span class="p">,</span><span class="s1">&#39; [s].&#39;</span>
<span class="w"> </span><span class="k">write</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="s1">&#39;Total message size is &#39;</span><span class="p">,</span><span class="n">totmess</span><span class="p">,</span><span class="s1">&#39; [MB].&#39;</span>
<span class="w"> </span><span class="k">write</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="s1">&#39;Latency (time per message) is &#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">time</span><span class="o">/</span><span class="n">numiter</span><span class="o">*</span><span class="mf">0.5d0</span><span class="p">,</span><span class="s1">&#39;[s].&#39;</span>
<span class="w"> </span><span class="k">write</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="s1">&#39;Bandwidth (message per time) is &#39;</span><span class="p">,</span><span class="n">totmess</span><span class="o">/</span><span class="nb">time</span><span class="p">,</span><span class="s1">&#39; [MB/s].&#39;</span>

<span class="w"> </span><span class="k">if</span><span class="p">(</span><span class="nb">time</span><span class="p">.</span><span class="n">lt</span><span class="p">.</span><span class="mf">1.d0</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">         </span><span class="c">! write(*,*) &quot;WARNING! The time is too short to be meaningful, increase the number</span>
<span class="w">  </span><span class="c">! of iterations and/or the array size so time is at least one second!&quot;</span>


<span class="w"> </span><span class="k">endif</span>
<span class="k">endif</span>

<span class="k">deallocate</span><span class="p">(</span><span class="n">sbuffer</span><span class="p">)</span>

<span class="k">call </span><span class="n">MPI_FINALIZE</span><span class="p">(</span><span class="n">ierr</span><span class="p">)</span>

<span class="k">end program </span><span class="n">pingpong</span>
</pre></div>
</div>
<p>Please choose one of these programs and save it to <code class="docutils literal notranslate"><span class="pre">pingpong.f90</span></code>. The def file
that we used for <code class="docutils literal notranslate"><span class="pre">mpitest</span></code> can be used in this case too. All we need to do is to
replace  <code class="docutils literal notranslate"><span class="pre">mpitest.c</span> <span class="pre">/opt</span></code> with <code class="docutils literal notranslate"><span class="pre">pingpong.f90</span> <span class="pre">/opt</span></code> at <code class="docutils literal notranslate"><span class="pre">%files</span></code> and to change
the complition at the end of <code class="docutils literal notranslate"><span class="pre">%post</span></code> from <code class="docutils literal notranslate"><span class="pre">mpicc</span> <span class="pre">-o</span> <span class="pre">mpitest</span> <span class="pre">mpitest.c</span></code> to
<code class="docutils literal notranslate"><span class="pre">mpif90</span> <span class="pre">-o</span> <span class="pre">pingpong.x</span> <span class="pre">pingpong.f90</span></code>. You can also directly compile it on the cluster
and copy the binary file <code class="docutils literal notranslate"><span class="pre">pingpong.x</span></code> instead of the code. The container creation
command remains the same.</p>
<p>Similar to <code class="docutils literal notranslate"><span class="pre">mpitest</span></code>, we run the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>mpi_hybrid.sif<span class="w"> </span>/opt/pingpong.x
</pre></div>
</div>
<p>The output should look like</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>message<span class="w"> </span>size<span class="w">   </span>transfertime<span class="w">    </span>bandwidth
<span class="w">       </span><span class="m">32</span><span class="w"> </span>bytes<span class="w">     </span><span class="m">1</span>.6736700000000000<span class="w">      </span>usec<span class="w">     </span><span class="m">19</span>.119659143802402<span class="w">      </span>MB/s
<span class="w">     </span><span class="m">2048</span><span class="w"> </span>bytes<span class="w">     </span><span class="m">2</span>.9812600000000011<span class="w">      </span>usec<span class="w">     </span><span class="m">686</span>.95786171930536<span class="w">      </span>MB/s
<span class="w">   </span><span class="m">131072</span><span class="w"> </span>bytes<span class="w">     </span><span class="m">19</span>.135579999999994<span class="w">      </span>usec<span class="w">     </span><span class="m">6849</span>.6486476540076<span class="w">      </span>MB/s
<span class="w">  </span><span class="m">8388608</span><span class="w"> </span>bytes<span class="w">     </span><span class="m">523</span>.77068999999995<span class="w">      </span>usec<span class="w">     </span><span class="m">16015</span>.802600219577<span class="w">      </span>MB/s
</pre></div>
</div>
<p>Let’s analyze what just happened:
When the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> is invoked as shown above, the MPI-based application code,
which will be linked against the MPI libraries, will make MPI API calls into these
MPI libraries which in turn talk to the MPI daemon process running on the host system.
This daemon process handles the communication between MPI processes, including talking
to the daemons on other nodes to exchange information between processes running on
different machines, as necessary.</p>
<p>Ultimately, this means that our running MPI code is linking to the MPI libraries
from the MPI install within our container and these are, in turn, communicating
with the MPI daemon on the host system which is part of the host system’s MPI
installation. These two installations of MPI may be different but as long as there
is compatibility between the version of MPI installed in your container image and
the version on the host system, your job should run successfully.</p>
<p>As a side note, when running code within a Singularity container, we don’t use
the MPI executables stored within the container (i.e. we <strong>DO NOT</strong> run
<code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">exec</span> <span class="pre">mpirun</span> <span class="pre">-np</span> <span class="pre">&lt;numprocs&gt;</span> <span class="pre">/path/to/my/executable</span></code>).
Instead we use the MPI installation on the host system to run Singularity and start
an instance of our executable from within a container for each MPI process.</p>
</section>
<section id="gpu-and-mpi">
<h3>GPU and MPI<a class="headerlink" href="#gpu-and-mpi" title="Link to this heading"></a></h3>
<p>In the <a class="reference internal" href="#document-hvd_intro"><span class="doc">Intoduction to Horovod</span></a>, we discussed how Horovod uses the MPI in conjuction with NCCL
to scale up apps. In this section, we see a simple example of using a similar concept for
running/training an app or a network on GPUs. The main advantage of such scheme is its
possibility of scaling.</p>
<p>In the below CUDA code, a large is divided by the number of available processers. While the
summation over each chunck is done within a GPU, the total sum is calculated using MPI
AllReduce method. Here, we pin (assume) there is one GPU per CPU.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;mpi.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstdio&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;chrono&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>


<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="p">(</span><span class="kt">double</span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// naive atomic reduction kernel</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">atomic_red</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="w">  </span><span class="o">*</span><span class="n">gdata</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">out</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">){</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">atomicAdd</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="w"> </span><span class="n">gdata</span><span class="p">[</span><span class="n">idx</span><span class="p">]);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">rank</span><span class="p">,</span><span class="w"> </span><span class="n">num_ranks</span><span class="p">;</span>

<span class="w">    </span><span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
<span class="w">    </span><span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">num_ranks</span><span class="p">);</span>
<span class="w">    </span><span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">rank</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Binding the cuda device with local MPI rank</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">local_rank</span><span class="p">,</span><span class="w"> </span><span class="n">local_size</span><span class="p">;</span>
<span class="w">    </span><span class="n">MPI_Comm</span><span class="w"> </span><span class="n">local_comm</span><span class="p">;</span>
<span class="w">    </span><span class="n">MPI_Comm_split_type</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_TYPE_SHARED</span><span class="p">,</span><span class="w"> </span><span class="n">rank</span><span class="p">,</span><span class="w">  </span><span class="n">MPI_INFO_NULL</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">local_comm</span><span class="p">);</span>

<span class="w">    </span><span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">local_comm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">local_size</span><span class="p">);</span>
<span class="w">    </span><span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">local_comm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">local_rank</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="n">local_rank</span><span class="o">%</span><span class="n">local_size</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Total problem size</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Problem size per rank (assumes divisibility of N)</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">N_per_rank</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">num_ranks</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Adapt the last mpi_rank if necessary</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="p">(</span><span class="n">num_ranks</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">N_per_rank</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">N_per_rank</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">num_ranks</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Initialize d_local_x to zero on device</span>
<span class="w">    </span><span class="kt">double</span><span class="o">*</span><span class="w"> </span><span class="n">d_local_x</span><span class="p">;</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="n">d_local_x</span><span class="p">,</span><span class="w"> </span><span class="n">N_per_rank</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
<span class="w">    </span><span class="n">cudaMemset</span><span class="p">(</span><span class="n">d_local_x</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="n">N_per_rank</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>

<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">d_local_sum</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">h_local_sum</span><span class="p">;</span>
<span class="w">    </span><span class="n">h_local_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="kt">double</span><span class="p">;</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_local_sum</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>

<span class="w">    </span><span class="c1">// Number of repetitions</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">num_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span>

<span class="w">    </span><span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">std</span><span class="o">::</span><span class="nn">chrono</span><span class="p">;</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">threads_per_block</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">256</span><span class="p">;</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">blocks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">N_per_rank</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threads_per_block</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threads_per_block</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_reps</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads_per_block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_local_x</span><span class="p">,</span><span class="w"> </span><span class="n">N_per_rank</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// summarize the vector of d_x</span>
<span class="w">    </span><span class="n">atomic_red</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads_per_block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_local_x</span><span class="p">,</span><span class="w"> </span><span class="n">d_local_sum</span><span class="p">,</span><span class="w"> </span><span class="n">N_per_rank</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">end</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">duration</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">duration_cast</span><span class="o">&lt;</span><span class="n">milliseconds</span><span class="o">&gt;</span><span class="p">(</span><span class="n">end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Copy vector sums from device to host:</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_local_sum</span><span class="p">,</span><span class="w"> </span><span class="n">d_local_sum</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">),</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Reduce all sums into the global sum</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">h_global_sum</span><span class="p">;</span>
<span class="w">    </span><span class="n">MPI_Allreduce</span><span class="p">(</span><span class="n">h_local_sum</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">h_global_sum</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_SUM</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Time per kernel = &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; ms &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">abs</span><span class="p">(</span><span class="n">h_global_sum</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">N</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">1e-14</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The sum is incorrect!&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The total sum of x = &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">h_global_sum</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">MPI_Finalize</span><span class="p">();</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Please save this as <code class="docutils literal notranslate"><span class="pre">reduction.cu</span></code> and compile the code using the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>add<span class="w"> </span>OpenMPI/4.0.5-gcccuda-2020b
nvcc<span class="w"> </span>-arch<span class="o">=</span>sm_80<span class="w"> </span>-o<span class="w"> </span>reduction.x<span class="w"> </span>reduction.cu<span class="w"> </span>-I/cvmfs/sling.si/modules/el7/software/OpenMPI/4.0.5-gcccuda-2020b/include<span class="w"> </span>-L/cvmfs/sling.si/modules/el7/software/hwloc/2.2.0-GCCcore-10.2.0/lib<span class="w"> </span>-lmpi<span class="w"> </span>-lcudart
</pre></div>
</div>
<p>Afterwards, you can use the definition file given below to create the desirable contianer.
Since we will use a similar container for the last section, more details about the definition
file will be given in below.</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span>BootStrap:<span class="w"> </span>docker
<span class="k">From</span><span class="s">:</span><span class="w"> </span>nvidia/cuda:11.1.1-devel-ubuntu18.04

%files
<span class="w">    </span>reduction.x<span class="w"> </span>/

%environment
<span class="w">    </span>#<span class="w"> </span>Point<span class="w"> </span>to<span class="w"> </span>OMPI<span class="w"> </span>binaries,<span class="w"> </span>libraries,<span class="w"> </span>man<span class="w"> </span>pages
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/lib:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda/lib64:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MANPATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/share/man:</span><span class="nv">$MANPATH</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LC_ALL</span><span class="o">=</span>C
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">HOROVOD_GPU_ALLREDUCE</span><span class="o">=</span>NCCL
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">HOROVOD_GPU_ALLGATHER</span><span class="o">=</span>MPI
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">HOROVOD_GPU_BROADCAST</span><span class="o">=</span>MPI
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">HOROVOD_NCCL_HOME</span><span class="o">=</span>/usr/local/cuda/nccl
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">HOROVOD_NCCL_INCLUDE</span><span class="o">=</span>/usr/local/cuda/nccl/include
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">HOROVOD_NCCL_LIB</span><span class="o">=</span>/usr/local/cuda/nccl/lib
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">PYTHON_VERSION</span><span class="o">=</span><span class="m">3</span>.7
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">TENSORFLOW_VERSION</span><span class="o">=</span><span class="m">2</span>.7.0
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">CUDNN_VERSION</span><span class="o">=</span><span class="m">8</span>.0.4.30-1+cuda11.1
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_VERSION</span><span class="o">=</span><span class="m">2</span>.8.3-1+cuda11.0

%post
<span class="w">    </span>mkdir<span class="w"> </span>/data1<span class="w"> </span>/data2<span class="w"> </span>/data0
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/var/spool/slurm
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/d/hpc
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/ceph/grid
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/ceph/hpc
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/scratch
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/exa5/scratch

<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">PYTHON_VERSION</span><span class="o">=</span><span class="m">3</span>.7
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">TENSORFLOW_VERSION</span><span class="o">=</span><span class="m">2</span>.7
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">CUDNN_VERSION</span><span class="o">=</span><span class="m">8</span>.0.4.30-1+cuda11.1
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_VERSION</span><span class="o">=</span><span class="m">2</span>.8.3-1+cuda11.0

<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /&quot;</span><span class="w"> </span>&gt;<span class="w"> </span>/etc/apt/sources.list.d/nvidia-ml.list

<span class="w">    </span>apt-get<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>--allow-downgrades<span class="w"> </span>--allow-change-held-packages<span class="w"> </span>--no-install-recommends<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build-essential<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>cmake<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>git<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>curl<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>vim<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>wget<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>ca-certificates<span class="w"> </span><span class="se">\</span>
<span class="w">        </span><span class="nv">libcudnn8</span><span class="o">=</span><span class="si">${</span><span class="nv">CUDNN_VERSION</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span><span class="nv">libnccl2</span><span class="o">=</span><span class="si">${</span><span class="nv">NCCL_VERSION</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libnccl-dev<span class="o">=</span><span class="si">${</span><span class="nv">NCCL_VERSION</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libjpeg-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libpng-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python<span class="si">${</span><span class="nv">PYTHON_VERSION</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python<span class="si">${</span><span class="nv">PYTHON_VERSION</span><span class="si">}</span>-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python<span class="si">${</span><span class="nv">PYTHON_VERSION</span><span class="si">}</span>-distutils

<span class="w">    </span>ln<span class="w"> </span>-s<span class="w"> </span>/usr/bin/python<span class="si">${</span><span class="nv">PYTHON_VERSION</span><span class="si">}</span><span class="w"> </span>/usr/bin/python

<span class="w">    </span>curl<span class="w"> </span>-O<span class="w"> </span>https://bootstrap.pypa.io/get-pip.py<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>python<span class="w"> </span>get-pip.py<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>rm<span class="w"> </span>get-pip.py

<span class="c"># Install Open MPI</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing Open MPI&quot;</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_VERSION</span><span class="o">=</span><span class="m">4</span>.0.5
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_URL</span><span class="o">=</span><span class="s2">&quot;https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-</span><span class="nv">$OMPI_VERSION</span><span class="s2">.tar.bz2&quot;</span>
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/tmp/ompi
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/opt
<span class="w">    </span>#<span class="w"> </span>Download
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>wget<span class="w"> </span>-O<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2<span class="w"> </span><span class="nv">$OMPI_URL</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>tar<span class="w"> </span>-xjf<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2
<span class="w">    </span>#<span class="w"> </span>Compile<span class="w"> </span>and<span class="w"> </span>install
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi/openmpi-<span class="nv">$OMPI_VERSION</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./configure<span class="w"> </span>--prefix<span class="o">=</span><span class="nv">$OMPI_DIR</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span>-j8<span class="w"> </span>install

<span class="w">    </span>#<span class="w"> </span>Set<span class="w"> </span>env<span class="w"> </span>variables<span class="w"> </span>so<span class="w"> </span>we<span class="w"> </span>can<span class="w"> </span>compile<span class="w"> </span>our<span class="w"> </span>application
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/bin:<span class="nv">$PATH</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>

<span class="c"># Install TensorFlow, Keras</span>
<span class="w">    </span>pip<span class="w"> </span>install<span class="w"> </span>tensorflow-gpu<span class="o">==</span><span class="si">${</span><span class="nv">TENSORFLOW_VERSION</span><span class="si">}</span><span class="w"> </span>h5py<span class="w"> </span>tensorflow-hub

<span class="c"># Install the IB verbs</span>
<span class="w">    </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>--no-install-recommends<span class="w"> </span>libibverbs*
<span class="w">    </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>--no-install-recommends<span class="w"> </span>ibverbs-utils<span class="w"> </span>librdmacm*<span class="w"> </span>infiniband-diags<span class="w"> </span>libmlx4*<span class="w"> </span>libmlx5*<span class="w"> </span>libnuma*

<span class="c"># Install Horovod, temporarily using CUDA stubs</span>
<span class="w">    </span>ldconfig<span class="w"> </span>/usr/local/cuda-11.1/targets/x86_64-linux/lib/stubs<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">HOROVOD_GPU_ALLREDUCE</span><span class="o">=</span>NCCL<span class="w"> </span><span class="nv">HOROVOD_WITH_TENSORFLOW</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">HOROVOD_WITH_PYTORCH</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>horovod<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>ldconfig

<span class="c"># Configure OpenMPI to run good defaults:</span>
<span class="c">#   --bind-to none --map-by slot --mca btl_tcp_if_exclude lo,docker0</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;hwloc_base_binding_policy = none&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/usr/local/etc/openmpi-mca-params.conf<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;rmaps_base_mapping_policy = slot&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/usr/local/etc/openmpi-mca-params.conf
<span class="w">    </span>#echo<span class="w"> </span><span class="s2">&quot;btl_tcp_if_exclude = lo,docker0&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/usr/local/etc/openmpi-mca-params.conf

<span class="c"># Set default NCCL parameters</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO<span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/nccl.conf<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>^docker0<span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/nccl.conf
</pre></div>
</div>
<p>Saving it as <code class="docutils literal notranslate"><span class="pre">cuda_example.def</span></code>, we can create the <code class="docutils literal notranslate"><span class="pre">cuda_example.sif</span></code> as mentioned above.
Similarly, we can run our example using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span><span class="w"> </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>cuda_example.sif<span class="w"> </span>/reduction.x
</pre></div>
</div>
<p>We should see an output similar to</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Time<span class="w"> </span>per<span class="w"> </span><span class="nv">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1676</span><span class="w"> </span>ms
The<span class="w"> </span>total<span class="w"> </span>sum<span class="w"> </span>of<span class="w"> </span><span class="nv">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>.07374e+11
Time<span class="w"> </span>per<span class="w"> </span><span class="nv">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1691</span><span class="w"> </span>ms
Time<span class="w"> </span>per<span class="w"> </span><span class="nv">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1689</span><span class="w"> </span>ms
Time<span class="w"> </span>per<span class="w"> </span><span class="nv">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1581</span><span class="w"> </span>ms
</pre></div>
</div>
<p>This example shows the simplest way of <em>offloading</em> a job to GPU(s) and using
the MPI AllReduce was used to calculate the final value. The example above can mimic
the calculation of gradient across difference GPUs.</p>
</section>
<section id="training-an-nlp-model-using-horovod">
<h3>Training an NLP model using Horovod<a class="headerlink" href="#training-an-nlp-model-using-horovod" title="Link to this heading"></a></h3>
<p>For the final part, let’s train the NLP model we used in previous chapters using containers.
Since we assume that the cluster does not provide TensorFlow and Horovod
for our training, we don’t need to load these two modules for the rest of our work.
We have the option either copying our code and dataset to the container or binding the current
path to <code class="docutils literal notranslate"><span class="pre">singularity</span></code> so that it can read file and folders. So far, we avoided
the latter because it interferes with building the containers with created above.
To keep the same tradition let’s copy the code and dataset to the container as we did in other section by
adding the <code class="docutils literal notranslate"><span class="pre">Transfer_Learning_NLP_Horovod.py</span></code> code and dataset <code class="docutils literal notranslate"><span class="pre">dataset.pkl</span></code> from <a class="reference internal" href="#document-hvd_intro"><span class="doc">Intoduction to Horovod</span></a>
to a new folder <code class="docutils literal notranslate"><span class="pre">horovod</span></code> and adding that to <code class="docutils literal notranslate"><span class="pre">%files</span></code> section.
After creating the container we are ready to to traino our model on two processers
using the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>-H<span class="w"> </span>localhost:2<span class="w"> </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>horovod.sif<span class="w"> </span>python<span class="w"> </span>horovod/Transfer_Learning_NLP_Horovod.py
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--------------------------------------------------------------------------
By<span class="w"> </span>default,<span class="w"> </span><span class="k">for</span><span class="w"> </span>Open<span class="w"> </span>MPI<span class="w"> </span><span class="m">4</span>.0<span class="w"> </span>and<span class="w"> </span>later,<span class="w"> </span>infiniband<span class="w"> </span>ports<span class="w"> </span>on<span class="w"> </span>a<span class="w"> </span>device
are<span class="w"> </span>not<span class="w"> </span>used<span class="w"> </span>by<span class="w"> </span>default.<span class="w">  </span>The<span class="w"> </span>intent<span class="w"> </span>is<span class="w"> </span>to<span class="w"> </span>use<span class="w"> </span>UCX<span class="w"> </span><span class="k">for</span><span class="w"> </span>these<span class="w"> </span>devices.
You<span class="w"> </span>can<span class="w"> </span>override<span class="w"> </span>this<span class="w"> </span>policy<span class="w"> </span>by<span class="w"> </span>setting<span class="w"> </span>the<span class="w"> </span>btl_openib_allow_ib<span class="w"> </span>MCA<span class="w"> </span>parameter
to<span class="w"> </span>true.

<span class="w">  </span>Local<span class="w"> </span>host:<span class="w">              </span>vglogin0008
<span class="w">  </span>Local<span class="w"> </span>adapter:<span class="w">           </span>mlx5_0
<span class="w">  </span>Local<span class="w"> </span>port:<span class="w">              </span><span class="m">1</span>

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING:<span class="w"> </span>There<span class="w"> </span>was<span class="w"> </span>an<span class="w"> </span>error<span class="w"> </span>initializing<span class="w"> </span>an<span class="w"> </span>OpenFabrics<span class="w"> </span>device.

<span class="w">  </span>Local<span class="w"> </span>host:<span class="w">   </span>vglogin0008
<span class="w">  </span>Local<span class="w"> </span>device:<span class="w"> </span>mlx5_0
--------------------------------------------------------------------------
Version:<span class="w">  </span><span class="m">2</span>.7.0
Hub<span class="w"> </span>version:<span class="w">  </span><span class="m">0</span>.12.0
GPU<span class="w"> </span>is<span class="w"> </span>available
Number<span class="w"> </span>of<span class="w"> </span>GPUs<span class="w"> </span>:<span class="w"> </span><span class="m">1</span>
The<span class="w"> </span>shape<span class="w"> </span>of<span class="w"> </span>training<span class="w"> </span><span class="o">(</span><span class="m">653061</span>,<span class="w"> </span><span class="m">3</span><span class="o">)</span><span class="w"> </span>and<span class="w"> </span>validation<span class="w"> </span><span class="o">(</span><span class="m">653</span>,<span class="w"> </span><span class="m">3</span><span class="o">)</span><span class="w"> </span>datasets.
<span class="c1">##-------------------------##</span>

<span class="c1">##-------------------------##</span>
Training<span class="w"> </span>starts<span class="w"> </span>...
Epoch<span class="w"> </span><span class="m">1</span>/40
<span class="w">    </span><span class="m">1</span>/20408<span class="w"> </span><span class="o">[</span>..............................<span class="o">]</span><span class="w"> </span>-<span class="w"> </span>ETA:<span class="w"> </span><span class="m">18</span>:55:44<span class="w"> </span>-<span class="w"> </span>loss:<span class="w"> </span><span class="m">0</span>.6903<span class="w"> </span>-<span class="w"> </span>accuracy:<span class="w"> </span><span class="m">0</span>.5938
</pre></div>
</div>
<p>There is whole host of flags at our disposal which can be/must be used to successfully train
the network. For example</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span>-H<span class="w"> </span>localhost:4<span class="w"> </span>-x<span class="w"> </span>LD_LIBRARY_PATH<span class="w"> </span>-x<span class="w"> </span>PATH<span class="w"> </span>-x<span class="w"> </span><span class="nv">HOROVOD_MPI_THREADS_DISABLE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>-x<span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>^virbr0,lo<span class="w"> </span>-mca<span class="w"> </span>btl<span class="w"> </span>openib,self<span class="w"> </span>-mca<span class="w"> </span>pml<span class="w"> </span>ob1<span class="w"> </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>horovod.sif<span class="w"> </span>python<span class="w"> </span>/horovod/Transfer_Learning_NLP_Horovod.py
</pre></div>
</div>
<p>It is always recommended to consult with the system admin regarding the usage of such
flags since it all depends on how the MPI and rest of system is setup.</p>
<div class="admonition-what-is-in-the-definition-file exercise important admonition" id="exercise-0">
<p class="admonition-title">What is in the definition file?</p>
<p>The definition file for the CUDA example and Horovod training is
almost the same. Can you go through the file explain what each part does?</p>
</div>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-containers/content/mpi_contain"></span><section id="running-mpi-parallel-jobs-using-singularity-containers">
<span id="mpi-contain"></span><h2>Running MPI parallel jobs using Singularity containers<a class="headerlink" href="#running-mpi-parallel-jobs-using-singularity-containers" title="Link to this heading"></a></h2>
<section id="mpi-overview">
<h3>MPI overview<a class="headerlink" href="#mpi-overview" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI (Message Passing Interface)</a>
is a widely used standard for parallel programming. It is used for
exchanging messages/data between processes in a parallel application.
If you’ve been involved in developing or working with computational
science software, you may already be familiar with MPI and running MPI
applications.</p>
<p>When working with an MPI code on a large-scale cluster, a common
approach is to compile the code yourself, within your own user
directory on the cluster platform, building against the supported MPI
implementation on the cluster.  Alternatively, if the code is widely
used on the cluster, the platform administrators may build and package
the application as a module so that it is easily accessible by all
users of the cluster.</p>
</section>
<section id="mpi-codes-with-singularity-containers">
<h3>MPI codes with Singularity containers<a class="headerlink" href="#mpi-codes-with-singularity-containers" title="Link to this heading"></a></h3>
<p>We’ve already seen that building Singularity containers can be
impractical without root access. Since we’re highly unlikely to have
root access on a large institutional, regional or national cluster,
building a container directly on the target platform is not normally
an option.</p>
<p>If our target platform uses <a class="reference external" href="https://www.open-mpi.org/">OpenMPI</a>,
one of the two widely used source MPI implementations, we can
build/install a compatible OpenMPI version on our local build
platform, or directly within the image as part of the image build
process. We can then build our code that requires MPI, either
interactively in an image sandbox or via a definition file.</p>
<p>If the target platform uses a version of MPI based on <a class="reference external" href="https://www.mpich.org/">MPICH</a>, the other widely used open source MPI
implementation, there is <a class="reference external" href="https://www.mpich.org/abi/">ABI compatibility between MPICH and several
other MPI implementations</a>.  In this
case, you can build MPICH and your code on a local platform, within an
image sandbox or as part of the image build process via a definition
file, and you should be able to successfully run containers based on
this image on your target cluster platform.</p>
<p>As described in Singularity’s <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/mpi.html">MPI documentation</a>, support for both
OpenMPI and MPICH is provided. Instructions are given for building the
relevant MPI version from source via a definition file and we’ll see
this used in an example below.</p>
<p>While building a container on a local system that is intended for use
on a remote HPC platform does provide some level of portability, if
you’re after the best possible performance, it can present some
issues. The version of MPI in the container will need to be built and
configured to support the hardware on your target platform if the best
possible performance is to be achieved. Where a platform has
specialist hardware with proprietary drivers, building on a different
platform with different hardware present means that building with the
right driver support for optimal performance is not likely to be
possible. This is especially true if the version of MPI available is
different (but compatible). Singularity’s <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/mpi.html">MPI documentation</a> highlights two
different models for working with MPI codes.</p>
<p>The <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/mpi.html#hybrid-model">hybrid model</a> that
we’ll be looking at here involves using the MPI executable from the
MPI installation on the host system to launch singularity and run the
application within the container.  The application in the container is
linked against and uses the MPI installation within the container
which, in turn, communicates with the MPI daemon process running on
the host system. In the following section we’ll look at building a
Singularity image containing a small MPI application that can then be
run using the hybrid model.</p>
</section>
<section id="building-and-running-a-singularity-image-for-an-mpi-code">
<h3>Building and running a Singularity image for an MPI code<a class="headerlink" href="#building-and-running-a-singularity-image-for-an-mpi-code" title="Link to this heading"></a></h3>
<section id="building-and-testing-an-image">
<h4>Building and testing an image<a class="headerlink" href="#building-and-testing-an-image" title="Link to this heading"></a></h4>
<p>This example makes the assumption that you’ll be building a container
image on a local platform and then deploying it to a cluster with a
different but compatible MPI implementation.  See <a class="reference external" href="https://sylabs.io/guides/3.7/user-guide/mpi.html#singularity-and-mpi-applications">Singularity and MPI
applications</a>
in the Singularity documentation for further information on how this
works.  We’ll build an image from a definition file. Containers based
on this image will be able to run MPI benchmarks using the <a class="reference external" href="https://mvapich.cse.ohio-state.edu/benchmarks/">OSU
Micro-Benchmarks</a>
software.</p>
<p>In the OSU example, the target platform is a remote HPC cluster that uses
<a class="reference external" href="https://www.mpich.org/">MPICH</a>. However, since the MPI launcher in the Vega system
is OpenMPI, we create a container with proper OpenMPI libraries.
The container can be built via the Singularity Docker image that we
used in the previous episode of the Singularity material.</p>
<p>We can either download OpenMPI and OSU Micro-Benchmarks locally and copy them to
the container for the installation, or we can download them on-the-fly and install
at the same time. Here we take the later approach. Please note the links to the
“tarballs” for version 5.8 of the OSU Micro-Benchmarks from the <a class="reference external" href="https://mvapich.cse.ohio-state.edu/benchmarks/">OSU Micro-Benchmarks page</a> and for OpenMPI
version 4.0.5 from the <a class="reference external" href="https://www.open-mpi.org/software/ompi/v4.0/">OpenMPI downloads page</a>.</p>
<p>Begin by creating a directory and, within that directory save
the following [definition file <code class="docutils literal notranslate"><span class="pre">/my_folder/osu_benchmarks.def</span></code> to a
<code class="docutils literal notranslate"><span class="pre">.def</span></code> file, e.g. <code class="docutils literal notranslate"><span class="pre">osu_benchmarks.def</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Bootstrap:<span class="w"> </span>docker
From:<span class="w"> </span>ubuntu:18.04

%environment
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/lib:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">MANPATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/share/man:</span><span class="nv">$MANPATH</span><span class="s2">&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">LC_ALL</span><span class="o">=</span>C

<span class="w"> </span>%post
<span class="w">  </span>mkdir<span class="w"> </span>/data1<span class="w"> </span>/data2<span class="w"> </span>/data0
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/var/spool/slurm
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/d/hpc
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/ceph/grid
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/ceph/hpc
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/scratch
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/exa5/scratch

<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing required packages...&quot;</span>
<span class="w">  </span>apt-get<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nv">DEBIAN_FRONTEND</span><span class="o">=</span>noninteractive<span class="w"> </span>apt-get<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>build-essential<span class="w"> </span>libfabric-dev<span class="w"> </span>libibverbs-dev<span class="w"> </span>gfortran
<span class="w">  </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>wget<span class="w"> </span>git<span class="w"> </span>bash<span class="w"> </span>gcc<span class="w"> </span>g++<span class="w"> </span>make<span class="w"> </span>file

<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing Open MPI&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_VERSION</span><span class="o">=</span><span class="m">4</span>.0.5
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_URL</span><span class="o">=</span><span class="s2">&quot;https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-</span><span class="nv">$OMPI_VERSION</span><span class="s2">.tar.bz2&quot;</span>
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/tmp/ompi
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/opt
<span class="w">  </span><span class="c1"># Download</span>
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>wget<span class="w"> </span>-O<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2<span class="w"> </span><span class="nv">$OMPI_URL</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>tar<span class="w"> </span>-xjf<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2
<span class="w">  </span><span class="c1"># Compile and install</span>
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi/openmpi-<span class="nv">$OMPI_VERSION</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./configure<span class="w"> </span>--prefix<span class="o">=</span><span class="nv">$OMPI_DIR</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span>-j8<span class="w"> </span>install

<span class="w">  </span><span class="c1"># Set env variables so we can compile our application</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/bin:<span class="nv">$PATH</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>

<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OSU_URL</span><span class="o">=</span><span class="s2">&quot;https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.8.tgz&quot;</span>
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/tmp/osub
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/tmp/osub<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>wget<span class="w"> </span>-O<span class="w"> </span>osu_mic_bench.tar<span class="w"> </span><span class="nv">$OSU_URL</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>tar<span class="w"> </span>-xf<span class="w"> </span>osu_mic_bench.tar
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>osu-micro-benchmarks-5.8/
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Configuring and building OSU Micro-Benchmarks...&quot;</span>
<span class="w">  </span>./configure<span class="w"> </span>--prefix<span class="o">=</span>/usr/local/osu<span class="w"> </span><span class="nv">CC</span><span class="o">=</span>/opt/ompi/bin/mpicc<span class="w"> </span><span class="nv">CXX</span><span class="o">=</span>/opt/ompi/bin/mpicxx
<span class="w">  </span>make<span class="w"> </span>-j2<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span>install

<span class="w"> </span>%runscript
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Rank </span><span class="si">${</span><span class="nv">PMI_RANK</span><span class="si">}</span><span class="s2"> - About to run: /usr/local/osu/libexec/osu-micro-benchmarks/mpi/</span><span class="nv">$*</span><span class="s2">&quot;</span>
<span class="w">  </span><span class="nb">exec</span><span class="w"> </span>/usr/local/osu/libexec/osu-micro-benchmarks/mpi/<span class="nv">$*</span>
</pre></div>
</div>
<p>A quick overview of what the above definition file is doing:</p>
<blockquote>
<div><ul class="simple">
<li><p>The image is being bootstrapped from the <code class="docutils literal notranslate"><span class="pre">ubuntu:18.04</span></code> Docker
image.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">%environment</span></code> section: Set an environment variable that
will be available within all containers run from the generated
image.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section:</p>
<ul>
<li><p>Ubuntu’s <code class="docutils literal notranslate"><span class="pre">apt-get</span></code> package manager is used to update the package
directory and then install the compilers and other libraries
required for the OpenMPI build.</p></li>
<li><p>The OpenMPI <code class="docutils literal notranslate"><span class="pre">.tar.gz</span></code> file is extracted and the configure, build and
install steps are run.</p></li>
<li><p>The OSU Micro-Benchmarks tar.gz file is extracted and the
configure, build and install steps are run to build the benchmark
code from source.</p></li>
</ul>
</li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>In the <code class="docutils literal notranslate"><span class="pre">%runscript</span></code> section: A runscript is set up that will echo
the rank number of the current process and then run the command
provided as a command line argument.</p></li>
</ul>
<p><em>Note that base path of the the executable to run is hardcoded in the
run script</em> so the command line parameter to provide when running a
container based on this image is relative to this base path, for
example, <code class="docutils literal notranslate"><span class="pre">startup/osu_hello</span></code>, <code class="docutils literal notranslate"><span class="pre">collective/osu_allgather</span></code>,
<code class="docutils literal notranslate"><span class="pre">pt2pt/osu_latency</span></code>, <code class="docutils literal notranslate"><span class="pre">one-sided/osu_put_latency</span></code>.</p>
<div class="admonition-build-and-test-the-osu-micro-benchmarks-image exercise important admonition" id="exercise-0">
<p class="admonition-title">Build and test the OSU Micro-Benchmarks image</p>
<p>Using the above definition file, build a Singularity image named
<code class="docutils literal notranslate"><span class="pre">osu_benchmarks.sif</span></code>.  Once you have built the image, use it to
run the <cite>osu_hello</cite> benchmark that is found in the <cite>startup</cite>
benchmark folder.</p>
<p><strong>NOTE</strong>: If you’re not using the Singularity Docker image to build
your Singularity image, you will need to edit the path to the
.tar.gz file in the <code class="docutils literal notranslate"><span class="pre">%files</span></code> section of the definition file.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>You should be able to build an image from the definition file
as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>build<span class="w"> </span>osu_benchmarks.sif<span class="w"> </span>osu_benchmarks.def
</pre></div>
</div>
<p>Note that if you’re running the Singularity Docker container
directly from the command line to undertake your build, you’ll
need to provide the full path to the <code class="docutils literal notranslate"><span class="pre">.def</span></code> file at which it
appears within the container - for example, if you’ve bind
mounted the directory containing the file to
<code class="docutils literal notranslate"><span class="pre">/home/singularity</span></code> within the container, the full path to the
<code class="docutils literal notranslate"><span class="pre">.def</span></code> file will be <code class="docutils literal notranslate"><span class="pre">/home/singularity/osu_benchmarks.def</span></code>.</p>
<p>Assuming the image builds successfully, you can then try
running the container locally and also transfer the SIF file
to a cluster platform that you have access to (that has
Singularity installed) and run it there.</p>
<p>Let’s begin with a single-process run of <code class="docutils literal notranslate"><span class="pre">osu_hello</span></code> on the
local system to ensure that we can run the container as
expected:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity<span class="w"> </span>run<span class="w"> </span>osu_benchmarks.sif<span class="w"> </span>startup/osu_hello
</pre></div>
</div>
<p>You should see output similar to the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Rank  - About to run: /usr/local/osu/libexec/osu-micro-benchmarks/mpi/startup/osu_hello
# OSU MPI Hello World Test v5.6.2
This is a test with 1 processes
</pre></div>
</div>
<p>Note that no rank number is shown since we didn’t run the
container via mpirun and so the <code class="docutils literal notranslate"><span class="pre">${PMI_RANK}</span></code> environment
variable that we’d normally have set in an MPICH run process is
not set.</p>
</div>
</div>
</section>
</section>
<section id="running-singularity-containers-via-mpi">
<h3>Running Singularity containers via MPI<a class="headerlink" href="#running-singularity-containers-via-mpi" title="Link to this heading"></a></h3>
<p>Assuming the above tests worked, we can now try undertaking a parallel run of
one of the OSU benchmarking tools within our container image.</p>
<p>This is where things get interesting and we’ll begin by looking at how Singularity
containers are run within an MPI environment.</p>
<p>If you’re familiar with running MPI codes, you’ll know that you use <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>,
<code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> or a similar MPI executable to start your application. This executable
may be run directly on the local system or cluster platform that you’re using, or
you may need to run it through a job script submitted to a job scheduler.
Your MPI-based application code, which will be linked against the MPI libraries,
will make MPI API calls into these MPI libraries which in turn talk to the MPI
daemon process running on the host system. This daemon process handles the
communication between MPI processes, including talking to the daemons on other
nodes to exchange information between processes running on different machines, as necessary.</p>
<p>When running code within a Singularity container, we don’t use the MPI executables
stored within the container (i.e. we <strong>DO NOT</strong> run <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">exec</span> <span class="pre">mpirun</span> <span class="pre">-np</span> <span class="pre">&lt;numprocs&gt;</span> <span class="pre">/path/to/my/executable</span></code>).
Instead we use the MPI installation on the host system to run Singularity and start
an instance of our executable from within a container for each MPI process.
Without Singularity support in an MPI implementation, this results in starting
a separate Singularity container instance within each process. This can present
some overhead if a large number of processes are being run on a host. Where Singularity
support is built into an MPI implementation this can address this potential issue and reduce
the overhead of running code from within a container as part of an MPI job.</p>
<p>Ultimately, this means that our running MPI code is linking to the MPI libraries
from the MPI install within our container and these are, in turn, communicating
with the MPI daemon on the host system which is part of the host system’s MPI installation.
These two installations of MPI may be different but as long as there is ABI compatibility
between the version of MPI installed in your container image and the version on the host system,
your job should run successfully.</p>
<p>We can now try running a 2-process MPI run of a point to point benchmark <code class="docutils literal notranslate"><span class="pre">osu_latency</span></code>.
If your local system has both MPI and Singularity installed and has multiple cores,
you can run this test on that system. Alternatively you can run on a cluster. Note
that you may need to submit this command via a job submission script submitted
to a job scheduler if you’re running on a cluster.</p>
<div class="admonition-undertake-a-parallel-run-of-the-osu-latency-benchmark-general-example exercise important admonition" id="exercise-1">
<p class="admonition-title">Undertake a parallel run of the <code class="docutils literal notranslate"><span class="pre">osu_latency</span></code> benchmark (general example)</p>
<p>Move the <code class="docutils literal notranslate"><span class="pre">osu_benchmarks.sif</span></code> Singularity image onto the cluster
(or other suitable) platform where you’re going to undertake
your benchmark run.</p>
<p>You should be able to run the benchmark using a command similar
to the one shown below. However, if you are running on a
cluster, you may need to write and submit a job submission
script at this point to initiate running of the benchmark.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">2</span><span class="w"> </span>singularity<span class="w"> </span>run<span class="w"> </span>osu_benchmarks.sif<span class="w"> </span>pt2pt/osu_latency
</pre></div>
</div>
<div class="admonition-expected-output-and-discussion solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Expected output and discussion</p>
<p>As you can see in the mpirun command shown above, we have called
<code class="docutils literal notranslate"><span class="pre">mpirun</span></code> on the host system and are passing to MPI the
<code class="docutils literal notranslate"><span class="pre">singularity</span></code> executable for which the parameters are the image
file and any parameters we want to pass to the image’s run
script, in this case the path/name of the benchmark executable
to run.</p>
<p>The following shows an example of the output you should expect
to see. You should have latency values shown for message sizes
up to 4MB.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Rank 1 - About to run: /.../mpi/pt2pt/osu_latency
Rank 0 - About to run: /.../mpi/pt2pt/osu_latency
# OSU MPI Latency Test v5.6.2
# Size          Latency (us)
0                       0.38
1                       0.34
...
</pre></div>
</div>
</div>
</div>
<div class="admonition-undertake-a-parallel-run-of-the-osu-latency-benchmark-taught-course-cluster-example exercise important admonition" id="exercise-2">
<p class="admonition-title">Undertake a parallel run of the <code class="docutils literal notranslate"><span class="pre">osu_latency</span></code> benchmark (taught course cluster example)</p>
<p>This version of the exercise for undertaking a parallel run of the
osu_latency benchmark with your Singularity container that
contains an MPI build is specific to this run of the course.  The
information provided here is specifically tailored to the HPC
platform that you’ve been given access to for this taught version
of the course.  Move the <cite>osu_benchmarks.sif</cite> Singularity image
onto the cluster where you’re going to undertake your benchmark
run.  You should use <code class="docutils literal notranslate"><span class="pre">scp</span></code> or a similar utility to copy the file.
The platform you’ve been provided with access to uses <cite>Slurm</cite>
schedule jobs to run on the platform. You now need to create a
<code class="docutils literal notranslate"><span class="pre">Slurm</span></code> job submission script to run the benchmark.</p>
<p>Find a template script on <a class="reference external" href="https://doc.vega.izum.si/first-job/">the Vega support website</a>
and edit it to suit your configuration. You can find more details about the Slurm
<a class="reference external" href="https://doc.vega.izum.si/slurm/">here</a>. Create and appropriate bash file and submit
the modified job submission script to the <cite>Slurm</cite> scheduler using the
<code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>osu_latency.slurm
</pre></div>
</div>
<div class="admonition-expected-output-and-discussion solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Expected output and discussion</p>
<p>As you will have seen in the commands using the provided
template job submission script, we have called <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> on the
host system and are passing to MPI the <code class="docutils literal notranslate"><span class="pre">singularity</span></code> executable
for which the parameters are the image file and any parameters
we want to pass to the image’s run script. In this case, the
parameters are the path/name of the benchmark executable to
run.</p>
<p>The following shows an example of the output you should expect
to see. You should have latency values shown for message sizes
up to 4MB.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>INFO:    Convert SIF file to sandbox...
      INFO:    Convert SIF file to sandbox...
      Rank 1 - About to run: /.../mpi/pt2pt/osu_latency
      Rank 0 - About to run: /.../mpi/pt2pt/osu_latency
      # OSU MPI Latency Test v5.6.2
      # Size          Latency (us)
      0                       1.49
      1                       1.50
      2                       1.50
      ...
      4194304               915.44
      INFO:    Cleaning up image...
INFO:    Cleaning up image...
</pre></div>
</div>
</div>
</div>
<p>This has demonstrated that we can successfully run a parallel MPI
executable from within a Singularity container.  However, in this
case, the two processes will almost certainly have run on the same
physical node so this is not testing the performance of the
interconnects between nodes.</p>
<p>You could now try running a larger-scale test. You can also try
running a benchmark that uses multiple processes, for example try
<code class="docutils literal notranslate"><span class="pre">collective/osu_gather</span></code>.</p>
<div class="admonition-investigate-performance-when-using-a-container-image-built-on-a-local-system-and-run-on-a-cluster exercise important admonition" id="exercise-3">
<p class="admonition-title">Investigate performance when using a container image
           built on a local system and run on a cluster</p>
<p>To get an idea of any difference in performance between the code
within your Singularity image and the same code built natively
on the target HPC platform, try building the OSU benchmarks from
source, locally on the cluster. Then try running the same
benchmark(s) that you ran via the singularity container.  Have a
look at the outputs you get when running <code class="docutils literal notranslate"><span class="pre">collective/osu_gather</span></code>
or one of the other collective benchmarks to get an idea of
whether there is a performance difference and how significant it
is.</p>
<p>Try running with enough processes that the processes are spread
across different physical nodes so that you’re making use of the
cluster’s network interconnects.</p>
<p>What do you see?</p>
<div class="admonition-discussion solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Discussion</p>
<p>You may find that performance is significantly better with the
version of the code built directly on the HPC platform.
Alternatively, performance may be similar between the two
versions.</p>
<p>How big is the performance difference between the two builds of
the code?</p>
<p>What might account for any difference in performance between the
two builds of the code?</p>
<p>If performance is an issue for you with codes that you’d like to
run via Singularity, you are advised to take a look at using the
<a class="reference external" href="https://sylabs.io/guides/3.5/user-guide/mpi.html#bind-model">bind model</a>
for building/running MPI applications through Singularity.</p>
</div>
</div>
</section>
<section id="a-simpler-mpi-example">
<h3>A simpler MPI example<a class="headerlink" href="#a-simpler-mpi-example" title="Link to this heading"></a></h3>
<p>While the OSU benchmark is an impressive test that shows how one can use containers
with MPI launcher, it might obscure the structure behind the whole setup. Let’s take a
look at an example in which we can simply see how MPI is run within a container.</p>
<p>Create a new directory and save the <code class="docutils literal notranslate"><span class="pre">mpitest.c</span></code> given below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#include &lt;mpi.h&gt;</span>
<span class="c1">#include &lt;stdio.h&gt;</span>
<span class="c1">#include &lt;stdlib.h&gt;</span>

int<span class="w"> </span>main<span class="w"> </span><span class="o">(</span>int<span class="w"> </span>argc,<span class="w"> </span>char<span class="w"> </span>**argv<span class="o">)</span><span class="w"> </span><span class="o">{</span>
<span class="w">      </span>int<span class="w"> </span>rc<span class="p">;</span>
<span class="w">      </span>int<span class="w"> </span>size<span class="p">;</span>
<span class="w">      </span>int<span class="w"> </span>myrank<span class="p">;</span>

<span class="w">      </span><span class="nv">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>MPI_Init<span class="w"> </span><span class="o">(</span><span class="p">&amp;</span>argc,<span class="w"> </span><span class="p">&amp;</span>argv<span class="o">)</span><span class="p">;</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="o">(</span>rc<span class="w"> </span>!<span class="o">=</span><span class="w"> </span>MPI_SUCCESS<span class="o">)</span><span class="w"> </span><span class="o">{</span>
<span class="w">              </span>fprintf<span class="w"> </span><span class="o">(</span>stderr,<span class="w"> </span><span class="s2">&quot;MPI_Init() failed&quot;</span><span class="o">)</span><span class="p">;</span>
<span class="w">              </span><span class="k">return</span><span class="w"> </span>EXIT_FAILURE<span class="p">;</span>
<span class="w">      </span><span class="o">}</span>

<span class="w">      </span><span class="nv">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>MPI_Comm_size<span class="w"> </span><span class="o">(</span>MPI_COMM_WORLD,<span class="w"> </span><span class="p">&amp;</span>size<span class="o">)</span><span class="p">;</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="o">(</span>rc<span class="w"> </span>!<span class="o">=</span><span class="w"> </span>MPI_SUCCESS<span class="o">)</span><span class="w"> </span><span class="o">{</span>
<span class="w">              </span>fprintf<span class="w"> </span><span class="o">(</span>stderr,<span class="w"> </span><span class="s2">&quot;MPI_Comm_size() failed&quot;</span><span class="o">)</span><span class="p">;</span>
<span class="w">              </span>goto<span class="w"> </span>exit_with_error<span class="p">;</span>
<span class="w">      </span><span class="o">}</span>

<span class="w">      </span><span class="nv">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>MPI_Comm_rank<span class="w"> </span><span class="o">(</span>MPI_COMM_WORLD,<span class="w"> </span><span class="p">&amp;</span>myrank<span class="o">)</span><span class="p">;</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="o">(</span>rc<span class="w"> </span>!<span class="o">=</span><span class="w"> </span>MPI_SUCCESS<span class="o">)</span><span class="w"> </span><span class="o">{</span>
<span class="w">              </span>fprintf<span class="w"> </span><span class="o">(</span>stderr,<span class="w"> </span><span class="s2">&quot;MPI_Comm_rank() failed&quot;</span><span class="o">)</span><span class="p">;</span>
<span class="w">              </span>goto<span class="w"> </span>exit_with_error<span class="p">;</span>
<span class="w">      </span><span class="o">}</span>

<span class="w">      </span>fprintf<span class="w"> </span><span class="o">(</span>stdout,<span class="w"> </span><span class="s2">&quot;Hello, I am rank %d/%d\n&quot;</span>,<span class="w"> </span>myrank,<span class="w"> </span>size<span class="o">)</span><span class="p">;</span>

<span class="w">      </span>MPI_Finalize<span class="o">()</span><span class="p">;</span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span>EXIT_SUCCESS<span class="p">;</span>

exit_with_error:
<span class="w">      </span>MPI_Finalize<span class="o">()</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span>EXIT_FAILURE<span class="p">;</span>
<span class="o">}</span>
</pre></div>
</div>
<p>We can either compile directly, or compile inside the contianer. For learning
purposes, let’s compile the code inside the container. Create a container with the defintion
file given below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Bootstrap:<span class="w"> </span>docker
From:<span class="w"> </span>ubuntu:18.04

%files
<span class="w">  </span>mpitest.c<span class="w"> </span>/opt

%environment
<span class="w">  </span><span class="c1"># Point to OMPI binaries, libraries, man pages</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/lib:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">MANPATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$OMPI_DIR</span><span class="s2">/share/man:</span><span class="nv">$MANPATH</span><span class="s2">&quot;</span>

%post
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing required packages...&quot;</span>
<span class="w">  </span>apt-get<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>wget<span class="w"> </span>git<span class="w"> </span>bash<span class="w"> </span>gcc<span class="w"> </span>gfortran<span class="w"> </span>g++<span class="w"> </span>make<span class="w"> </span>file

<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing Open MPI&quot;</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_VERSION</span><span class="o">=</span><span class="m">4</span>.0.5
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_URL</span><span class="o">=</span><span class="s2">&quot;https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-</span><span class="nv">$OMPI_VERSION</span><span class="s2">.tar.bz2&quot;</span>
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/tmp/ompi
<span class="w">  </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/opt
<span class="w">  </span><span class="c1"># Download</span>
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>wget<span class="w"> </span>-O<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2<span class="w"> </span><span class="nv">$OMPI_URL</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>tar<span class="w"> </span>-xjf<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2
<span class="w">  </span><span class="c1"># Compile and install</span>
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi/openmpi-<span class="nv">$OMPI_VERSION</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./configure<span class="w"> </span>--prefix<span class="o">=</span><span class="nv">$OMPI_DIR</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span>-j8<span class="w"> </span>install

<span class="w">  </span><span class="c1"># Set env variables so we can compile our application</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/bin:<span class="nv">$PATH</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>

<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Compiling the MPI application...&quot;</span>
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span>/opt<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>mpicc<span class="w"> </span>-o<span class="w"> </span>mpitest<span class="w"> </span>mpitest.c
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>singularity<span class="w"> </span>run<span class="w"> </span>mpi_hybrid.sif<span class="w"> </span>/opt/mpitest
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">1</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">2</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">3</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">4</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">5</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">6</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">7</span>/8
Hello,<span class="w"> </span>I<span class="w"> </span>am<span class="w"> </span>rank<span class="w"> </span><span class="m">0</span>/8
</pre></div>
</div>
</section>
</section>
<span id="document-containers/content/rep_gran"></span><section id="containers-in-research-workflows">
<span id="rep-gran"></span><h2>Containers in research workflows<a class="headerlink" href="#containers-in-research-workflows" title="Link to this heading"></a></h2>
<p>Although this workshop is titled “Reproducible computational
environments using containers”, so far we have mostly covered the
mechanics of using Docker with only passing reference to the
reproducibility aspects. In this section, we discuss these aspects in
more detail.</p>
<div class="admonition-work-in-progress callout admonition" id="callout-0">
<p class="admonition-title">Work in progress</p>
<p>Note that reproducibility aspects of software and containers are an
active area of research, discussion and development so are subject
to many changes. We will present some ideas and approaches here but
best practices will likely evolve in the near future.</p>
</div>
<section id="reproducibility">
<h3>Reproducibility<a class="headerlink" href="#reproducibility" title="Link to this heading"></a></h3>
<p>By <em>reproducibility</em> here we mean the ability of someone else (or your
future self) being able to reproduce what you did computationally at a
particular time (be this in research, analysis or something else) as
closely as possible even if they do not have access to exactly the
same hardware resources # that you had when you did the original work.</p>
<p>Some examples of why containers are an attractive technology to help
with reproducibility include:</p>
<ul class="simple">
<li><p>The same computational work can be run across multiple different
technologies seamlessly (e.g. Windows, macOS, Linux).</p></li>
<li><p>You can save the exact process that you used for your computational
work (rather than relying on potentially incomplete notes).</p></li>
<li><p>You can save the exact versions of software and their dependencies
in the image.</p></li>
<li><p>You can access legacy versions of software and underlying
dependencies which may not be generally available any more.</p></li>
<li><p>Depending on their size, you can also potentially store a copy of
key data within the image.</p></li>
<li><p>You can archive and share the image as well as associating a
persistent identifier with an image to allow other researchers to
reproduce and build on your work.</p></li>
</ul>
</section>
<section id="sharing-images">
<h3>Sharing images<a class="headerlink" href="#sharing-images" title="Link to this heading"></a></h3>
<p>As we have already seen, the Docker Hub provides a platform for
sharing images publicly. Once you have uploaded an image, you can
point people to its public location and they can download and build
upon it.</p>
<p>This is fine for working collaboratively with images on a day-to-day
basis but the Docker Hub is not a good option for long time archive of
images in support of research and publications as:</p>
<ul class="simple">
<li><p>free accounts have a limit on how long an image will be hosted if it
is not updated</p></li>
<li><p>it does not support adding persistent identifiers to images</p></li>
<li><p>it is easy to overwrite tagged images with newer versions by mistake.</p></li>
</ul>
</section>
<section id="archiving-images">
<h3>Archiving images<a class="headerlink" href="#archiving-images" title="Link to this heading"></a></h3>
<p>If for any reason you decided to archive an image, you can use the
command below to take a snapshot of the image.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>save<span class="w"> </span>alice/alpine-python:v1<span class="w"> </span>-o<span class="w"> </span>alpine-python.tar
</pre></div>
</div>
<div class="admonition-restoring-the-image-from-a-save callout admonition" id="callout-1">
<p class="admonition-title">Restoring the image from a save</p>
<p>Unsurprisingly, the command <cite>docker load alpine-python.tar.gz</cite> would
be used to load the saved container and make it available to be used
on your system. Note that the command can restore the compressed
container directly without the need to uncompress first.</p>
</div>
</section>
<section id="reproducibility-good-practice">
<h3>Reproducibility good practice<a class="headerlink" href="#reproducibility-good-practice" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Make use of images to capture the computational environment
required for your work.</p></li>
<li><p>Decide on the appropriate granularity for the images you will use
for your computational work - this will be different for each
project/area. Take note of accepted practice from contemporary work
in the same area.  What are the right building blocks for
individual images in your work?</p></li>
<li><p>Document what you have done and why - this can be put in comments
in the Dockerfile and the use of the image described in associated
documentation and/or publications.  Make sure that references are
made in both directions so that the image and the documentation are
appropriately linked.</p></li>
</ul>
</section>
<section id="container-granularity">
<h3>Container Granularity<a class="headerlink" href="#container-granularity" title="Link to this heading"></a></h3>
<p>As mentioned above, one of the decisions you may need to make when
containerising your research workflows is what level of <em>granularity</em>
you wish to employ. The two extremes of this decision could be
characterised as:</p>
<ul class="simple">
<li><p>Create a single container image with all the tools you require for
your research or analysis workflow</p></li>
<li><p>Create many container images each running a single command (or step)
of the workflow and use them in sequence</p></li>
</ul>
<p>Of course, many real applications will sit somewhere between these two
extremes.</p>
<div class="admonition-positives-and-negatives callout admonition" id="callout-2">
<p class="admonition-title">Positives and negatives</p>
<p>What are the advantages and disadvantages of the two approaches to
container granularity for research workflows described above? Think
about this and write a few bullet points for advantages and
disadvantages for each approach in the course Etherpad.</p>
<p><strong>Single large container</strong></p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Advantages</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Disadvantages</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><ul class="simple">
<li><p>Simpler to document</p></li>
<li><p>Full set of requirements packaged in one place</p></li>
<li><p>Potentially easier to maintain (though could be opposite if
working with large, distributed group)</p></li>
</ul>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><ul class="simple">
<li><p>Could get very large in size, making it more difficult to
distribute</p></li>
<li><p>Could use Docker multi-stage build
docs.docker.com/develop/develop-images/multistage-build to
reduce size</p></li>
<li><p>Singularity also has a multistage build feature:
sylabs.io/guides/3.2/user-guide/definition_files.html#multi-stage-builds</p></li>
<li><p>May end up with same dependency issues within the container
from different software requirements</p></li>
<li><p>Potentially more complex to test</p></li>
<li><p>Less re-useable for different, but related, work</p></li>
</ul>
</div></div>
<p><strong>Multiple smaller containers</strong></p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">Advantages</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">Disadvantage</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><ul class="simple">
<li><p>Individual components can be re-used for different, but
related, work</p></li>
<li><p>Individual parts are smaller in size making them easier to
distribute</p></li>
<li><p>Avoid dependency issues between different softwares</p></li>
<li><p>Easier to test</p></li>
</ul>
</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><ul class="simple">
<li><p>More difficult to document</p></li>
<li><p>Potentially more difficult to maintain (though could be
easier if working with large, distributed group)</p></li>
<li><p>May end up with dependency issues between component
containers if they get out of sync</p></li>
</ul>
</div></div>
</div>
</section>
<section id="container-orchestration">
<h3>Container Orchestration<a class="headerlink" href="#container-orchestration" title="Link to this heading"></a></h3>
<p>Although you can certainly manage research workflows that use multiple
containers manually, there are a number of container orchestration
tools that you may find useful when managing workflows that use
multiple containers.  We won’t go in depth on using these tools in
this lesson but instead briefly describe a few options and point to
useful resources on using these tools to allow you to explore them
yourself.</p>
<ul class="simple">
<li><p>Docker Compose</p></li>
<li><p>Kubernetes</p></li>
<li><p>Docker Swarm</p></li>
</ul>
<div class="admonition-the-wild-west callout admonition" id="callout-3">
<p class="admonition-title">The Wild West</p>
<p>Use of container orchestration tools for research workflows is a
relatively new concept and so there is not a huge amount of
documentation and experience out there at the moment. You may need
to search around for useful information or, better still, contact
your friendly neighbourhood to discuss what you want to do.</p>
</div>
<p><a class="reference external" href="https://docs.docker.com/compose/">Docker Compose</a> provides a
way of constructing a unified workflow (or service) made up of
multiple individual Docker containers. In addition to the individual
Dockerfiles for each container, you provide a higher-level
configuration file which describes the different containers and how
they link together along with shared storage definitions between the
containers. Once this high-level configuration has been defined, you
can use single commands to start and stop the orchestrated set of
containers.</p>
<p><a class="reference external" href="https://kubernetes.io">Kubernetes</a> is an open source framework
that provides similar functionality to Docker Compose. Its particular
strengths are that is platform independent and can be used with many
different container technologies and that it is widely available on
cloud platforms so once you have implemented your workflow in
Kubernetes it can be deployed in different locations as required. It
has become the de facto standard for container orchestration.</p>
<p><a class="reference external" href="https://docs.docker.com/engine/swarm/">Docker Swarm</a> provides
a way to scale out to multiple copies of similar containers. This
potentially allows you to parallelise and scale out your research
workflow so that you can run multiple copies and increase
throughput. This would allow you, for example, to take advantage of
multiple cores on a local system or run your workflow in the cloud to
access more resources. Docker Swarm uses the concept of a manager
container and worker containers to implement this distribution.</p>
</section>
</section>
<span id="document-containers/content/pwd_exmps"></span><section id="pwd-exercises">
<span id="pwd-exmps"></span><h2>PWD exercises<a class="headerlink" href="#pwd-exercises" title="Link to this heading"></a></h2>
<section id="clone-the-labs-github-repo">
<h3>Clone the Lab’s GitHub Repo<a class="headerlink" href="#clone-the-labs-github-repo" title="Link to this heading"></a></h3>
<p>Use the following command to clone the lab’s repo from GitHub (you can click the
command or manually type it). This will make a copy of the lab’s repo in a new
sub-directory called linux_tweet_app.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/dockersamples/linux_tweet_app
</pre></div>
</div>
<section id="task-1-run-some-simple-docker-containers">
<h4>Task 1: Run some simple Docker containers<a class="headerlink" href="#task-1-run-some-simple-docker-containers" title="Link to this heading"></a></h4>
<p>There are different ways to use containers. These include:</p>
<ul class="simple">
<li><p><strong>To run a single task</strong>: This could be a shell script or a custom app.</p></li>
<li><p><strong>Interactively</strong>: This connects you to the container similar to the way you SSH into a remote server.</p></li>
<li><p><strong>In the background</strong>: For long-running services like websites and databases.</p></li>
</ul>
<p>In this section you’ll try each of those options and see how Docker manages the workload.</p>
</section>
</section>
<section id="run-a-single-task-in-an-alpine-linux-container">
<h3>Run a single task in an Alpine Linux container<a class="headerlink" href="#run-a-single-task-in-an-alpine-linux-container" title="Link to this heading"></a></h3>
<p>In this step we’re going to start a new container and tell it to run the <code class="docutils literal notranslate"><span class="pre">hostname</span></code> command.
The container will start, execute the hostname command, then exit.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Run the following command in your Linux console.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span>alpine<span class="w"> </span>hostname
</pre></div>
</div>
<p>The output below shows that the <code class="docutils literal notranslate"><span class="pre">alpine:latest</span></code> image could not be found locally.
When this happens, Docker automatically pulls it from Docker Hub.</p>
<p>After the image is pulled, the container’s hostname is displayed
(<code class="docutils literal notranslate"><span class="pre">888e89a3b36b</span></code> in the example below).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="n">Unable</span> <span class="n">to</span> <span class="n">find</span> <span class="n">image</span> <span class="s1">&#39;alpine:latest&#39;</span> <span class="n">locally</span>
<span class="n">latest</span><span class="p">:</span> <span class="n">Pulling</span> <span class="kn">from</span><span class="w"> </span><span class="nn">library</span><span class="o">/</span><span class="n">alpine</span>
<span class="mi">88286</span><span class="n">f41530e</span><span class="p">:</span> <span class="n">Pull</span> <span class="n">complete</span>
<span class="n">Digest</span><span class="p">:</span> <span class="n">sha256</span><span class="p">:</span><span class="n">f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d</span>
<span class="n">Status</span><span class="p">:</span> <span class="n">Downloaded</span> <span class="n">newer</span> <span class="n">image</span> <span class="k">for</span> <span class="n">alpine</span><span class="p">:</span><span class="n">latest</span>
<span class="mf">888e89</span><span class="n">a3b36b</span>
</pre></div>
</div>
<p>2. Docker keeps a container running as long as the process it started inside
the container is still running. In this case the <code class="docutils literal notranslate"><span class="pre">hostname</span></code> process exits
as soon as the output is written. This means the container stops. However,
Docker doesn’t delete resources by default, so the container still exists
in the <code class="docutils literal notranslate"><span class="pre">Exited</span></code> state.</p>
<p>List all containers.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>ls<span class="w"> </span>--all
</pre></div>
</div>
<p>Notice that your Alpine Linux container is in the <code class="docutils literal notranslate"><span class="pre">Exited</span></code> state.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>CONTAINER<span class="w"> </span>ID<span class="w">        </span>IMAGE<span class="w">               </span>COMMAND<span class="w">             </span>CREATED<span class="w">             </span>STATUS<span class="w">            </span>PORTS<span class="w">               </span>NAMES
888e89a3b36b<span class="w">        </span>alpine<span class="w">              </span><span class="s2">&quot;hostname&quot;</span><span class="w">          </span><span class="m">50</span><span class="w"> </span>seconds<span class="w"> </span>ago<span class="w">      </span>Exited<span class="w"> </span><span class="o">(</span><span class="m">0</span><span class="o">)</span><span class="w"> </span><span class="m">49</span><span class="w"> </span>seconds<span class="w"> </span>ago<span class="w">                       </span>awesome_elion
</pre></div>
</div>
<p><strong>Note</strong>: The container ID is the hostname that the container displayed. In the example above it’s <code class="docutils literal notranslate"><span class="pre">888e89a3b36b</span></code>.</p>
</div></blockquote>
<p>Containers which do one task and then exit can be very useful. You could build a
Docker image that executes a script to configure something. Anyone can execute
that task just by running the container - they don’t need the actual scripts or configuration information.</p>
<section id="run-an-interactive-ubuntu-container">
<h4>Run an interactive Ubuntu container<a class="headerlink" href="#run-an-interactive-ubuntu-container" title="Link to this heading"></a></h4>
<p>You can run a container based on a different version of Linux than is running on
your Docker host.</p>
<p>In the next example, we are going to run an Ubuntu Linux container on top of an
Alpine Linux Docker host (Play With Docker uses Alpine Linux for its nodes).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Run a Docker container and access its shell.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span>--interactive<span class="w"> </span>--tty<span class="w"> </span>--rm<span class="w"> </span>ubuntu<span class="w"> </span>bash
</pre></div>
</div>
<p>In this example, we’re giving Docker three parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--interactive</span></code> says you want an interactive session.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--tty</span></code> allocates a pseudo-tty.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--rm</span></code> tells Docker to go ahead and remove the container when it’s done executing.</p></li>
</ul>
<p>The first two parameters allow you to interact with the Docker container.</p>
<p>We’re also telling the container to run <code class="docutils literal notranslate"><span class="pre">bash</span></code> as its main process (PID 1).</p>
<p>When the container starts you’ll drop into the bash shell with the default prompt
<code class="docutils literal notranslate"><span class="pre">root&#64;&lt;container</span> <span class="pre">id&gt;:/#</span></code>. Docker has attached to the shell in the container,
relaying input and output between your local session and the shell session in the container.</p>
<ol class="arabic simple" start="2">
<li><p>Run the following commands in the container.</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">/</span></code> will list the contents of the root directory in the container,
<code class="docutils literal notranslate"><span class="pre">ps</span> <span class="pre">aux</span></code> will show running processes in the container, <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/etc/issue</span></code> will
show which Linux distro the container is running, in this case Ubuntu 20.04.3 LTS.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ls<span class="w"> </span>/
ps<span class="w"> </span>aux
cat<span class="w"> </span>/etc/issue
</pre></div>
</div>
<p>3. Type exit to leave the shell session. This will terminate the bash process,
causing the container to exit.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">exit</span>
</pre></div>
</div>
<p><strong>Note</strong>: As we used the <code class="docutils literal notranslate"><span class="pre">--rm</span></code> flag when we started the container, Docker
removed the container when it stopped. This means if you run another <code class="docutils literal notranslate"><span class="pre">docker</span>
<span class="pre">container</span> <span class="pre">ls</span> <span class="pre">--all</span></code> you won’t see the Ubuntu container.</p>
<ol class="arabic simple" start="3">
<li><p>For fun, let’s check the version of our host VM.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>/etc/issue
</pre></div>
</div>
<p>You should see:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Welcome<span class="w"> </span>to<span class="w"> </span>Alpine<span class="w"> </span>Linux<span class="w"> </span><span class="m">3</span>.8
Kernel<span class="w"> </span><span class="se">\r</span><span class="w"> </span>on<span class="w"> </span>an<span class="w"> </span><span class="se">\m</span><span class="w"> </span><span class="o">(</span><span class="se">\l</span><span class="o">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Notice that our host VM is running Alpine Linux, yet we were able to run an
Ubuntu container. As previously mentioned, the distribution of Linux inside
the container does not need to match the distribution of Linux running on the Docker host.</p>
<p>However, Linux containers require the Docker host to be running a Linux kernel.
For example, Linux containers cannot run directly on Windows Docker hosts.
The same is true of Windows containers - they need to run on a Docker host with a Windows kernel.</p>
<p>Interactive containers are useful when you are putting together your own image.
You can run a container and verify all the steps you need to deploy your app,
and capture them in a Dockerfile.</p>
</section>
<section id="run-a-background-mysql-container">
<h4>Run a background MySQL container<a class="headerlink" href="#run-a-background-mysql-container" title="Link to this heading"></a></h4>
<p>Background containers are how you’ll run most applications. Here’s a simple example using MySQL.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Run a new MySQL container with the following command.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
--detach<span class="w"> </span><span class="se">\</span>
--name<span class="w"> </span>mydb<span class="w"> </span><span class="se">\</span>
-e<span class="w"> </span><span class="nv">MYSQL_ROOT_PASSWORD</span><span class="o">=</span>my-secret-pw<span class="w"> </span><span class="se">\</span>
mysql:latest
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--detach</span></code> will run the container in the background.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--name</span></code> will name it mydb.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-e</span></code> will use an environment variable to specify the root password
(NOTE: This should never be done in production).</p></li>
</ul>
<p>As the MySQL image was not available locally, Docker automatically pulled it from Docker Hub.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Unable<span class="w"> </span>to<span class="w"> </span>find<span class="w"> </span>image<span class="w"> </span><span class="s1">&#39;mysql:latest&#39;</span><span class="w"> </span>locallylatest:<span class="w"> </span>Pulling<span class="w"> </span>from<span class="w"> </span>library/mysql
aa18ad1a0d33:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
fdb8d83dece3:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
75b6ce7b50d3:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
ed1d0a3a64e4:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
8eb36a82c85b:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
41be6f1a1c40:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
0e1b414eac71:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
914c28654a91:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
587693eb988c:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
b183c3585729:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
315e21657aa4:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
Digest:<span class="w"> </span>sha256:0dc3dacb751ef46a6647234abdec2d47400f0dfbe77ab490b02bffdae57846ed
Status:<span class="w"> </span>Downloaded<span class="w"> </span>newer<span class="w"> </span>image<span class="w"> </span><span class="k">for</span><span class="w"> </span>mysql:latest
41d6157c9f7d1529a6c922acb8167ca66f167119df0fe3d86964db6c0d7ba4e0
</pre></div>
</div>
<p>As long as the MySQL process is running, Docker will keep the container running in the background.</p>
<ol class="arabic simple" start="2">
<li><p>List the running containers.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>ls
</pre></div>
</div>
<p>Notice your container is running.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>CONTAINER<span class="w"> </span>ID<span class="w">        </span>IMAGE<span class="w">               </span>COMMAND<span class="w">                  </span>CREATED<span class="w">             </span>STATUS<span class="w">              </span>PORTS<span class="w">            </span>NAMES
3f4e8da0caf7<span class="w">        </span>mysql:latest<span class="w">        </span><span class="s2">&quot;docker-entrypoint...&quot;</span><span class="w">   </span><span class="m">52</span><span class="w"> </span>seconds<span class="w"> </span>ago<span class="w">      </span>Up<span class="w"> </span><span class="m">51</span><span class="w"> </span>seconds<span class="w">       </span><span class="m">3306</span>/tcp<span class="w">            </span>mydb
</pre></div>
</div>
<p>3. You can check what’s happening in your containers by using a couple of built-in
Docker commands: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">logs</span></code> and <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">top</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>logs<span class="w"> </span>mydb
</pre></div>
</div>
<p>This shows the logs from the MySQL Docker container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&lt;output<span class="w"> </span>truncated&gt;
<span class="m">2017</span>-09-29T16:02:58.605004Z<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">[</span>Note<span class="o">]</span><span class="w"> </span>Executing<span class="w"> </span><span class="s1">&#39;SELECT * FROM INFORMATION_SCHEMA.TABLES;&#39;</span><span class="w"> </span>to<span class="w"> </span>get<span class="w"> </span>a<span class="w"> </span>list<span class="w"> </span>of<span class="w"> </span>tables<span class="w"> </span>using<span class="w"> </span>the<span class="w"> </span>deprecated<span class="w"> </span>partition<span class="w"> </span>engine.<span class="w"> </span>You<span class="w"> </span>may<span class="w"> </span>use<span class="w"> </span>the<span class="w"> </span>startup<span class="w"> </span>option<span class="w"> </span><span class="s1">&#39;--disable-partition-engine-check&#39;</span><span class="w"> </span>to<span class="w"> </span>skip<span class="w"> </span>this<span class="w"> </span>check.
<span class="m">2017</span>-09-29T16:02:58.605026Z<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">[</span>Note<span class="o">]</span><span class="w"> </span>Beginning<span class="w"> </span>of<span class="w"> </span>list<span class="w"> </span>of<span class="w"> </span>non-natively<span class="w"> </span>partitioned<span class="w"> </span>tables
<span class="m">2017</span>-09-29T16:02:58.616575Z<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">[</span>Note<span class="o">]</span><span class="w"> </span>End<span class="w"> </span>of<span class="w"> </span>list<span class="w"> </span>of<span class="w"> </span>non-natively<span class="w"> </span>partitioned<span class="w"> </span>tables
</pre></div>
</div>
<p>Let’s look at the processes running inside the container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>top<span class="w"> </span>mydb
</pre></div>
</div>
<p>You should see the MySQL daemon (<code class="docutils literal notranslate"><span class="pre">mysqld</span></code>) is running in the container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>PID<span class="w">                 </span>USER<span class="w">                </span>TIME<span class="w">                </span>COMMAND
<span class="m">2876</span><span class="w">                </span><span class="m">999</span><span class="w">                 </span><span class="m">0</span>:00<span class="w">                </span>mysqld
</pre></div>
</div>
<p>Although MySQL is running, it is isolated within the container because no network
ports have been published to the host. Network traffic cannot reach containers from
the host unless ports are explicitly published.</p>
<ol class="arabic simple" start="4">
<li><p>List the MySQL version using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">exec</span></code>.</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">exec</span></code> allows you to run a command inside a container.
In this example, we’ll use <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">exec</span></code> to run the command-line
equivalent of <code class="docutils literal notranslate"><span class="pre">mysql</span> <span class="pre">--user=root</span> <span class="pre">--password=$MYSQL_ROOT_PASSWORD</span> <span class="pre">--version</span></code>
inside our MySQL container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>mydb<span class="w"> </span><span class="se">\</span>
mysql<span class="w"> </span>--user<span class="o">=</span>root<span class="w"> </span>--password<span class="o">=</span><span class="nv">$MYSQL_ROOT_PASSWORD</span><span class="w"> </span>--version
</pre></div>
</div>
<p>You will see the MySQL version number, as well as a handy warning.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mysql:<span class="w"> </span><span class="o">[</span>Warning<span class="o">]</span><span class="w"> </span>Using<span class="w"> </span>a<span class="w"> </span>password<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span><span class="nb">command</span><span class="w"> </span>line<span class="w"> </span>interface<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>insecure.
mysql<span class="w">  </span>Ver<span class="w"> </span><span class="m">14</span>.14<span class="w"> </span>Distrib<span class="w"> </span><span class="m">5</span>.7.19,<span class="w"> </span><span class="k">for</span><span class="w"> </span>Linux<span class="w"> </span><span class="o">(</span>x86_64<span class="o">)</span><span class="w"> </span>using<span class="w">  </span>EditLine<span class="w"> </span>wrapper
</pre></div>
</div>
<p>5. You can also use <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">exec</span></code> to connect to a new shell process
inside an already-running container. Executing the command below will give you
an interactive shell (<code class="docutils literal notranslate"><span class="pre">sh</span></code>) inside your MySQL container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>mydb<span class="w"> </span>sh
</pre></div>
</div>
<p>Notice that your shell prompt has changed. This is because your shell is now
connected to the sh process running inside of your container.</p>
<p>Let’s check the version number by running the same command again, only this
time from within the new shell session in the container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mysql<span class="w"> </span>--user<span class="o">=</span>root<span class="w"> </span>--password<span class="o">=</span><span class="nv">$MYSQL_ROOT_PASSWORD</span><span class="w"> </span>--version
</pre></div>
</div>
<p>Notice the output is the same as before.</p>
<p>Type exit to leave the interactive shell session.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">exit</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="task-2-package-and-run-a-custom-app-using-docker">
<h4>Task 2: Package and run a custom app using Docker<a class="headerlink" href="#task-2-package-and-run-a-custom-app-using-docker" title="Link to this heading"></a></h4>
<p>In this step you’ll learn how to package your own apps as Docker images using a Dockerfile.</p>
<p>The Dockerfile syntax is straightforward. In this task, we’re going to create a
simple NGINX website from a Dockerfile.</p>
</section>
</section>
<section id="build-a-simple-website-image">
<h3>Build a simple website image<a class="headerlink" href="#build-a-simple-website-image" title="Link to this heading"></a></h3>
<p>Let’s have a look at the Dockerfile we’ll be using, which builds a simple website
that allows you to send a tweet.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Make sure you’re in the linux_tweet_app directory.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>~/linux_tweet_app
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Display the contents of the Dockerfile.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>Dockerfile
FROM<span class="w"> </span>nginx:latest

COPY<span class="w"> </span>index.html<span class="w"> </span>/usr/share/nginx/html
COPY<span class="w"> </span>linux.png<span class="w"> </span>/usr/share/nginx/html

EXPOSE<span class="w"> </span><span class="m">80</span><span class="w"> </span><span class="m">443</span>

CMD<span class="w"> </span><span class="o">[</span><span class="s2">&quot;nginx&quot;</span>,<span class="w"> </span><span class="s2">&quot;-g&quot;</span>,<span class="w"> </span><span class="s2">&quot;daemon off;&quot;</span><span class="o">]</span>
</pre></div>
</div>
<p>Let’s see what each of these lines in the Dockerfile do.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">FROM</span></code> specifies the base image to use as the starting point for this new
image you’re creating. For this example we’re starting from <code class="docutils literal notranslate"><span class="pre">nginx:latest</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">COPY</span></code> copies files from the Docker host into the image, at a known location.
In this example, <code class="docutils literal notranslate"><span class="pre">COPY</span></code> is used to copy two files into the image: <code class="docutils literal notranslate"><span class="pre">index.html</span></code>.
and a graphic that will be used on our webpage.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">EXPOSE</span></code> documents which ports the application uses.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CMD</span></code> specifies what command to run when a container is started from the image.
Notice that we can specify the command, as well as run-time arguments.</p></li>
</ul>
<p>3. In order to make the following commands more copy/paste friendly, export an environment
variable containing your DockerID (if you don’t have a DockerID you can get one for
free via Docker Hub).</p>
<p>You will have to manually type this command as it requires your unique DockerID.</p>
<p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">DOCKERID=&lt;your</span> <span class="pre">docker</span> <span class="pre">id&gt;</span></code></p>
<p>Echo the value of the variable back to the terminal to ensure it was stored correctly.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span><span class="w"> </span><span class="nv">$DOCKERID</span>
</pre></div>
</div>
<p>Use the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span> <span class="pre">build</span></code> command to create a new Docker image using the instructions
in the Dockerfile.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--tag</span></code> allows us to give the image a custom name. In this case it’s comprised of our
DockerID, the application name, and a version. Having the Docker ID attached to the name
will allow us to store it on Docker Hub in a later step</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.</span></code> tells Docker to use the current directory as the build context</p></li>
</ul>
<p>Be sure to include period (.) at the end of the command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>build<span class="w"> </span>--tag<span class="w"> </span><span class="nv">$DOCKERID</span>/linux_tweet_app:1.0<span class="w"> </span>.
</pre></div>
</div>
<p>The output below shows the Docker daemon executing each line in the Dockerfile</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Sending<span class="w"> </span>build<span class="w"> </span>context<span class="w"> </span>to<span class="w"> </span>Docker<span class="w"> </span>daemon<span class="w">  </span><span class="m">32</span>.77kB
Step<span class="w"> </span><span class="m">1</span>/5<span class="w"> </span>:<span class="w"> </span>FROM<span class="w"> </span>nginx:latest
latest:<span class="w"> </span>Pulling<span class="w"> </span>from<span class="w"> </span>library/nginx
afeb2bfd31c0:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
7ff5d10493db:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
d2562f1ae1d0:<span class="w"> </span>Pull<span class="w"> </span><span class="nb">complete</span>
Digest:<span class="w"> </span>sha256:af32e714a9cc3157157374e68c818b05ebe9e0737aac06b55a09da374209a8f9
Status:<span class="w"> </span>Downloaded<span class="w"> </span>newer<span class="w"> </span>image<span class="w"> </span><span class="k">for</span><span class="w"> </span>nginx:latest
---&gt;<span class="w"> </span>da5939581ac8
Step<span class="w"> </span><span class="m">2</span>/5<span class="w"> </span>:<span class="w"> </span>COPY<span class="w"> </span>index.html<span class="w"> </span>/usr/share/nginx/html
---&gt;<span class="w"> </span>eba2eec2bea9
Step<span class="w"> </span><span class="m">3</span>/5<span class="w"> </span>:<span class="w"> </span>COPY<span class="w"> </span>linux.png<span class="w"> </span>/usr/share/nginx/html
---&gt;<span class="w"> </span>4d080f499b53
Step<span class="w"> </span><span class="m">4</span>/5<span class="w"> </span>:<span class="w"> </span>EXPOSE<span class="w"> </span><span class="m">80</span><span class="w"> </span><span class="m">443</span>
---&gt;<span class="w"> </span>Running<span class="w"> </span><span class="k">in</span><span class="w"> </span>47232cb5699f
---&gt;<span class="w"> </span>74c968a9165f
Removing<span class="w"> </span>intermediate<span class="w"> </span>container<span class="w"> </span>47232cb5699f
Step<span class="w"> </span><span class="m">5</span>/5<span class="w"> </span>:<span class="w"> </span>CMD<span class="w"> </span>nginx<span class="w"> </span>-g<span class="w"> </span>daemon<span class="w"> </span>off<span class="p">;</span>
---&gt;<span class="w"> </span>Running<span class="w"> </span><span class="k">in</span><span class="w"> </span>4623761274ac
---&gt;<span class="w"> </span>12045a0df899
Removing<span class="w"> </span>intermediate<span class="w"> </span>container<span class="w"> </span>4623761274ac
Successfully<span class="w"> </span>built<span class="w"> </span>12045a0df899
Successfully<span class="w"> </span>tagged<span class="w"> </span>&lt;your<span class="w"> </span>docker<span class="w"> </span>ID&gt;/linux_tweet_app:latest
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">run</span></code> command to start a new container from the image you created.</p></li>
</ol>
<p>As this container will be running an NGINX web server, we’ll use the <code class="docutils literal notranslate"><span class="pre">--publish</span></code> flag to publish
port 80 inside the container onto port 80 on the host. This will allow traffic coming in to
the Docker host on port 80 to be directed to port 80 in the container.
The format of the <code class="docutils literal notranslate"><span class="pre">--publish</span></code> flag is <code class="docutils literal notranslate"><span class="pre">host_port:container_port</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
--detach<span class="w"> </span><span class="se">\</span>
--publish<span class="w"> </span><span class="m">80</span>:80<span class="w"> </span><span class="se">\</span>
--name<span class="w"> </span>linux_tweet_app<span class="w"> </span><span class="se">\</span>
<span class="nv">$DOCKERID</span>/linux_tweet_app:1.0
</pre></div>
</div>
<p>Any external traffic coming into the server on port 80 will now be directed
into the container on port 80.</p>
<p>In a later step you will see how to map traffic from two different ports - this
is necessary when two containers use the same port to communicate since you can
only expose the port once on the host.</p>
<ol class="arabic simple" start="7">
<li><p>Load the website which should be running.</p></li>
<li><p>Once you’ve accessed your website, shut it down and remove it.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>rm<span class="w"> </span>--force<span class="w"> </span>linux_tweet_app
</pre></div>
</div>
</div></blockquote>
<section id="task-3-modify-a-running-website">
<h4>Task 3: Modify a running website<a class="headerlink" href="#task-3-modify-a-running-website" title="Link to this heading"></a></h4>
<p>When you’re actively working on an application it is inconvenient to have to stop
the container, rebuild the image, and run a new version every time you make a change
to your source code.</p>
<p>One way to streamline this process is to mount the source code directory on the
local machine into the running container. This will allow any changes made to
the files on the host to be immediately reflected in the container.</p>
<p>We do this using something called a <a class="reference external" href="https://docs.docker.com/engine/admin/volumes/bind-mounts/">bind mount</a>.</p>
<p>When you use a bind mount, a file or directory on the host machine is mounted
into a container running on the same host.</p>
</section>
</section>
<section id="start-our-web-app-with-a-bind-mount">
<h3>Start our web app with a bind mount<a class="headerlink" href="#start-our-web-app-with-a-bind-mount" title="Link to this heading"></a></h3>
<blockquote>
<div><ol class="arabic simple">
<li><p>Let’s start the web app and mount the current directory into the container.</p></li>
</ol>
<p>In this example we’ll use the <code class="docutils literal notranslate"><span class="pre">--mount</span></code> flag to mount the current directory
on the host into <code class="docutils literal notranslate"><span class="pre">/usr/share/nginx/html</span></code> inside the container.</p>
<p>Be sure to run this command from within the <code class="docutils literal notranslate"><span class="pre">linux_tweet_app</span></code> directory on your Docker host.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
--detach<span class="w"> </span><span class="se">\</span>
--publish<span class="w"> </span><span class="m">80</span>:80<span class="w"> </span><span class="se">\</span>
--name<span class="w"> </span>linux_tweet_app<span class="w"> </span><span class="se">\</span>
--mount<span class="w"> </span><span class="nv">type</span><span class="o">=</span>bind,source<span class="o">=</span><span class="s2">&quot;</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span><span class="s2">&quot;</span>,target<span class="o">=</span>/usr/share/nginx/html<span class="w"> </span><span class="se">\</span>
<span class="nv">$DOCKERID</span>/linux_tweet_app:1.0
</pre></div>
</div>
<p><strong>Remember</strong> from the Dockerfile, <code class="docutils literal notranslate"><span class="pre">usr/share/nginx/html</span></code> is where the html
files are stored for the web app.</p>
<ol class="arabic simple" start="2">
<li><p>The website should be running.</p></li>
</ol>
</div></blockquote>
</section>
<section id="modify-the-running-website">
<h3>Modify the running website<a class="headerlink" href="#modify-the-running-website" title="Link to this heading"></a></h3>
<p>Bind mounts mean that any changes made to the local file system are immediately
reflected in the running container.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Copy a new <code class="docutils literal notranslate"><span class="pre">index.html</span></code> into the container.</p></li>
</ol>
<p>The Git repo that you pulled earlier contains several different versions of an
index.html file. You can manually run an ls command from within the <code class="docutils literal notranslate"><span class="pre">~/linux_tweet_app</span></code>
directory to see a list of them. In this step we’ll replace <code class="docutils literal notranslate"><span class="pre">index.html</span></code>
with <code class="docutils literal notranslate"><span class="pre">index-new.html</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">cp</span> <span class="pre">index-new.html</span> <span class="pre">index.html</span></code></p>
<ol class="arabic simple" start="2">
<li><p>Go to the running website and refresh the page. Notice that the site has changed.</p></li>
</ol>
<div class="admonition-trick callout admonition" id="callout-0">
<p class="admonition-title">Trick</p>
<p>If you are comfortable with vi you can use it to load the local index.html file
and make additional changes. Those too would be reflected when you reload the webpage.
If you are really adventurous, why not try using exec to access the running container
and modify the files stored there.</p>
</div>
</div></blockquote>
<p>Even though we’ve modified the index.html local filesystem and seen it reflected in
the running container, we’ve not actually changed the Docker image that the container was started from.</p>
<p>To show this, stop the current container and re-run the <code class="docutils literal notranslate"><span class="pre">1.0</span></code> image without a bind mount.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Stop and remove the currently running container.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>rm<span class="w"> </span>--force<span class="w"> </span>linux_tweet_app
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Rerun the current version without a bind mount.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
--detach<span class="w"> </span><span class="se">\</span>
--publish<span class="w"> </span><span class="m">80</span>:80<span class="w"> </span><span class="se">\</span>
--name<span class="w"> </span>linux_tweet_app<span class="w"> </span><span class="se">\</span>
<span class="nv">$DOCKERID</span>/linux_tweet_app:1.0
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Notice the website is back to the original version.</p></li>
<li><p>Stop and remove the current container</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>rm<span class="w"> </span>--force<span class="w"> </span>linux_tweet_app
</pre></div>
</div>
</div></blockquote>
<section id="update-the-image">
<h4>Update the image<a class="headerlink" href="#update-the-image" title="Link to this heading"></a></h4>
<p>To persist the changes you made to the index.html file into the image,
you need to build a new version of the image.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Build a new image and tag it as <code class="docutils literal notranslate"><span class="pre">2.0</span></code></p></li>
</ol>
<p>Remember that you previously modified the <code class="docutils literal notranslate"><span class="pre">index.html</span></code> file on the Docker hosts
local filesystem. This means that running another docker image build command
will build a new image with the updated <code class="docutils literal notranslate"><span class="pre">index.html</span></code></p>
<p>Be sure to include the period (<code class="docutils literal notranslate"><span class="pre">.</span></code>) at the end of the command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>build<span class="w"> </span>--tag<span class="w"> </span><span class="nv">$DOCKERID</span>/linux_tweet_app:2.0<span class="w"> </span>.
</pre></div>
</div>
<p>Notice how fast that built! This is because Docker only modified the portion
of the image that changed vs. rebuilding the whole image.</p>
<ol class="arabic simple" start="2">
<li><p>Let’s look at the images on the system.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>ls
</pre></div>
</div>
<p>You now have both versions of the web app on your host.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>REPOSITORY<span class="w">                     </span>TAG<span class="w">                 </span>IMAGE<span class="w"> </span>ID<span class="w">            </span>CREATED<span class="w">             </span>SIZE
&lt;docker<span class="w"> </span>id&gt;/linux_tweet_app<span class="w">    </span><span class="m">2</span>.0<span class="w">                 </span>01612e05312b<span class="w">        </span><span class="m">16</span><span class="w"> </span>seconds<span class="w"> </span>ago<span class="w">      </span>108MB
&lt;docker<span class="w"> </span>id&gt;/linux_tweet_app<span class="w">    </span><span class="m">1</span>.0<span class="w">                 </span>bb32b5783cd3<span class="w">        </span><span class="m">4</span><span class="w"> </span>minutes<span class="w"> </span>ago<span class="w">       </span>108MB
mysql<span class="w">                          </span>latest<span class="w">              </span>b4e78b89bcf3<span class="w">        </span><span class="m">2</span><span class="w"> </span>weeks<span class="w"> </span>ago<span class="w">         </span>412MB
ubuntu<span class="w">                         </span>latest<span class="w">              </span>2d696327ab2e<span class="w">        </span><span class="m">2</span><span class="w"> </span>weeks<span class="w"> </span>ago<span class="w">         </span>122MB
nginx<span class="w">                          </span>latest<span class="w">              </span>da5939581ac8<span class="w">        </span><span class="m">3</span><span class="w"> </span>weeks<span class="w"> </span>ago<span class="w">         </span>108MB
alpine<span class="w">                         </span>latest<span class="w">              </span>76da55c8019d<span class="w">        </span><span class="m">3</span><span class="w"> </span>weeks<span class="w"> </span>ago<span class="w">         </span><span class="m">3</span>.97MB
</pre></div>
</div>
</div></blockquote>
</section>
<section id="test-the-new-version">
<h4>Test the new version<a class="headerlink" href="#test-the-new-version" title="Link to this heading"></a></h4>
<blockquote>
<div><ol class="arabic simple">
<li><p>Run a new container from the new version of the image.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
--detach<span class="w"> </span><span class="se">\</span>
--publish<span class="w"> </span><span class="m">80</span>:80<span class="w"> </span><span class="se">\</span>
--name<span class="w"> </span>linux_tweet_app<span class="w"> </span><span class="se">\</span>
<span class="nv">$DOCKERID</span>/linux_tweet_app:2.0
</pre></div>
</div>
<p>2. Check the new version of the website (You may need to refresh your browser
to get the new version to load).</p>
<p>The web page will have an orange background.</p>
<p>We can run both versions side by side. The only thing we need to be aware of
is that we cannot have two containers using port 80 on the same host.</p>
<p>As we’re already using port 80 for the container running from the <code class="docutils literal notranslate"><span class="pre">2.0</span></code> version
of the image, we will start a new container and publish it on port 8080.
Additionally, we need to give our container a unique name (<code class="docutils literal notranslate"><span class="pre">old_linux_tweet_app</span></code>)</p>
<ol class="arabic simple" start="3">
<li><p>Run another new container, this time from the old version of the image.</p></li>
</ol>
<p>Notice that this command maps the new container to port 8080 on the host.
This is because two containers cannot map to the same port on a single Docker host.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
--detach<span class="w"> </span><span class="se">\</span>
--publish<span class="w"> </span><span class="m">8080</span>:80<span class="w"> </span><span class="se">\</span>
--name<span class="w"> </span>old_linux_tweet_app<span class="w"> </span><span class="se">\</span>
<span class="nv">$DOCKERID</span>/linux_tweet_app:1.0
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>View the old version of the website.</p></li>
</ol>
</div></blockquote>
</section>
<section id="push-your-images-to-docker-hub">
<h4>Push your images to Docker Hub<a class="headerlink" href="#push-your-images-to-docker-hub" title="Link to this heading"></a></h4>
<blockquote>
<div><ol class="arabic simple">
<li><p>List the images on your Docker host.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>ls<span class="w"> </span>-f<span class="w"> </span><span class="nv">reference</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$DOCKERID</span><span class="s2">/*&quot;</span>
</pre></div>
</div>
<p>You will see that you now have two <code class="docutils literal notranslate"><span class="pre">linux_tweet_app</span></code> images - one tagged
as 1.0 and the other as 2.0.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>REPOSITORY<span class="w">                     </span>TAG<span class="w">                 </span>IMAGE<span class="w"> </span>ID<span class="w">            </span>CREATED<span class="w">             </span>SIZE
&lt;docker<span class="w"> </span>id&gt;/linux_tweet_app<span class="w">    </span><span class="m">2</span>.0<span class="w">                 </span>01612e05312b<span class="w">        </span><span class="m">3</span><span class="w"> </span>minutes<span class="w"> </span>ago<span class="w">       </span>108MB
&lt;docker<span class="w"> </span>id&gt;/linux_tweet_app<span class="w">    </span><span class="m">1</span>.0<span class="w">                 </span>bb32b5783cd3<span class="w">        </span><span class="m">7</span><span class="w"> </span>minutes<span class="w"> </span>ago<span class="w">       </span>108MB
</pre></div>
</div>
<p>These images are only stored in your Docker hosts local repository. Your Docker
host will be deleted after the workshop. In this step we’ll push the images to
a public repository so you can run them from any Linux machine with Docker.</p>
<p>Distribution is built into the Docker platform. You can build images locally
and push them to a public or private registry, making them available to other users.
Anyone with access can pull that image and run a container from it. The behavior
of the app in the container will be the same for everyone, because the image
contains the fully-configured app - the only requirements to run it are Linux and Docker.</p>
<p><a class="reference external" href="https://hub.docker.com/">Docker Hub</a> is the default public registry for Docker images.</p>
<ol class="arabic simple" start="2">
<li><p>Before you can push your images, you will need to log into Docker Hub.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>login
</pre></div>
</div>
<p>You will need to supply your Docker ID credentials when prompted.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Username:<span class="w"> </span>&lt;your<span class="w"> </span>docker<span class="w"> </span>id&gt;
Password:<span class="w"> </span>&lt;your<span class="w"> </span>docker<span class="w"> </span>id<span class="w"> </span>password&gt;
Login<span class="w"> </span>Succeeded
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Push version <code class="docutils literal notranslate"><span class="pre">1.0</span></code> of your web app using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span> <span class="pre">push</span></code>.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>push<span class="w"> </span><span class="nv">$DOCKERID</span>/linux_tweet_app:1.0
</pre></div>
</div>
<p>You’ll see the progress as the image is pushed up to Docker Hub.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>The<span class="w"> </span>push<span class="w"> </span>refers<span class="w"> </span>to<span class="w"> </span>a<span class="w"> </span>repository<span class="w"> </span><span class="o">[</span>docker.io/&lt;your<span class="w"> </span>docker<span class="w"> </span>id&gt;/linux_tweet_app<span class="o">]</span>
910e84bcef7a:<span class="w"> </span>Pushed
1dee161c8ba4:<span class="w"> </span>Pushed
110566462efa:<span class="w"> </span>Pushed
305e2b6ef454:<span class="w"> </span>Pushed
24e065a5f328:<span class="w"> </span>Pushed
<span class="m">1</span>.0:<span class="w"> </span>digest:<span class="w"> </span>sha256:51e937ec18c7757879722f15fa1044cbfbf2f6b7eaeeb578c7c352baba9aa6dc<span class="w"> </span>size:<span class="w"> </span><span class="m">1363</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Now push version <code class="docutils literal notranslate"><span class="pre">2.0</span></code>.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>image<span class="w"> </span>push<span class="w"> </span><span class="nv">$DOCKERID</span>/linux_tweet_app:2.0
</pre></div>
</div>
<p>Notice that several lines of the output say <code class="docutils literal notranslate"><span class="pre">Layer</span> <span class="pre">already</span> <span class="pre">exists</span></code>. This is because
Docker will leverage read-only layers that are the same as any previously uploaded image layers.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>The<span class="w"> </span>push<span class="w"> </span>refers<span class="w"> </span>to<span class="w"> </span>a<span class="w"> </span>repository<span class="w"> </span><span class="o">[</span>docker.io/&lt;your<span class="w"> </span>docker<span class="w"> </span>id&gt;/linux_tweet_app<span class="o">]</span>
0b171f8fbe22:<span class="w"> </span>Pushed
70d38c767c00:<span class="w"> </span>Pushed
110566462efa:<span class="w"> </span>Layer<span class="w"> </span>already<span class="w"> </span>exists
305e2b6ef454:<span class="w"> </span>Layer<span class="w"> </span>already<span class="w"> </span>exists
24e065a5f328:<span class="w"> </span>Layer<span class="w"> </span>already<span class="w"> </span>exists
<span class="m">2</span>.0:<span class="w"> </span>digest:<span class="w"> </span>sha256:7c51f77f90b81e5a598a13f129c95543172bae8f5850537225eae0c78e4f3add<span class="w"> </span>size:<span class="w"> </span><span class="m">1363</span>
</pre></div>
</div>
</div></blockquote>
<p>You can browse to <code class="docutils literal notranslate"><span class="pre">https://hub.docker.com/r/&lt;your</span> <span class="pre">docker</span> <span class="pre">id&gt;/</span></code> and see your
newly-pushed Docker images. These are public repositories, so anyone can pull
the image - you don’t even need a Docker ID to pull public images.
Docker Hub also supports private repositories.</p>
</section>
</section>
</section>
<span id="document-upscalingAIcontainer/content/namespc-cgroup"></span><section id="namespaces-and-cgroups">
<span id="namespc-cgroup"></span><h2>Namespaces and cgroups<a class="headerlink" href="#namespaces-and-cgroups" title="Link to this heading"></a></h2>
<section id="what-are-namespaces">
<h3>What Are Namespaces?<a class="headerlink" href="#what-are-namespaces" title="Link to this heading"></a></h3>
<p>Namespaces have been part of the Linux kernel since about 2002, and over time more
tooling and namespace types have been added. Real container support was added to
the Linux kernel only in 2013, however. This is what made namespaces really useful
and brought them to the masses.</p>
<div class="admonition-definition-according-to-wikipedia-https-en-wikipedia-org-wiki-linux-namespaces callout admonition" id="callout-0">
<p class="admonition-title">Definition according to <a class="reference external" href="https://en.wikipedia.org/wiki/Linux_namespaces">Wikipedia</a></p>
<p><cite>Namespaces are a feature of the Linux kernel that partitions kernel resources
such that one set of processes sees one set of resources while another set of
processes sees a different set of resources.</cite></p>
</div>
<p>In other words, the key feature of namespaces is that they isolate processes from
each other. On a server where you are running many different services, isolating
each service and its associated processes from other services means that there is
a smaller blast radius for changes, as well as a smaller footprint for security‑related
concerns.</p>
<p>Using containers during the development process gives the developer an isolated
environment that looks and feels like a complete VM. It’s not a VM, though – it’s
a process running on a server somewhere. If the developer starts two containers,
there are two processes running on a single server somewhere – but they are isolated
from each other.</p>
<section id="types-of-namespaces">
<h4>Types of Namespaces<a class="headerlink" href="#types-of-namespaces" title="Link to this heading"></a></h4>
<p>Within the Linux kernel, there are different types of namespaces. Each namespace
has its own unique properties:</p>
<ul class="simple">
<li><p>A <a class="reference external" href="https://man7.org/linux/man-pages/man7/user_namespaces.7.html">user namespace</a>
has its own set of user IDs and group IDs for assignment to processes. In particular,
this means that a process can have root privilege within its user namespace without
having it in other user namespaces.</p></li>
<li><p>A <a class="reference external" href="https://man7.org/linux/man-pages/man7/pid_namespaces.7.html">process ID (PID) namespace</a>
assigns a set of PIDs to processes that are independent from the set of PIDs in other namespaces.
The first process created in a new namespace has PID 1 and child processes are assigned subsequent PIDs.
If a child process is created with its own PID namespace, it has PID 1 in that namespace
as well as its PID in the parent process’ namespace. See below for an example.</p></li>
<li><p>A <a class="reference external" href="https://man7.org/linux/man-pages/man7/network_namespaces.7.html">network namespace</a>
has an independent network stack: its own private routing table, set of IP addresses,
socket listing, connection tracking table, firewall, and other network‑related resources.</p></li>
<li><p>A <a class="reference external" href="https://man7.org/linux/man-pages/man7/mount_namespaces.7.html">mount namespace</a>
has an independent list of mount points seen by the processes in the namespace. This means
that you can mount and unmount filesystems in a mount namespace without affecting the host filesystem.</p></li>
<li><p>An <a class="reference external" href="https://man7.org/linux/man-pages/man7/ipc_namespaces.7.html">interprocess communication (IPC) namespace</a>
has its own IPC resources.</p></li>
<li><p>A <a class="reference external" href="https://man7.org/linux/man-pages/man7/uts_namespaces.7.html">UNIX Time‑Sharing (UTS) namespace</a>
allows a single system to appear to have different host and domain names to different processes.</p></li>
</ul>
</section>
<section id="an-example-of-parent-and-child-pid-namespaces">
<h4>An Example of Parent and Child PID Namespaces<a class="headerlink" href="#an-example-of-parent-and-child-pid-namespaces" title="Link to this heading"></a></h4>
<p>In the diagram below, there are three PID namespaces – a parent namespace and
two child namespaces. Within the parent namespace, there are four processes,
named PID1 through PID4. These are normal processes which can all see each
other and share resources.</p>
<p>The child processes with PID2 and PID3 in the parent namespace also belong to
their own PID namespaces in which their PID is 1. From within a child namespace,
the PID1 process cannot see anything outside. For example, PID1 in both child
namespaces cannot see PID4 in the parent namespace.</p>
<p>This provides isolation between (in this case) processes within different namespaces.</p>
<figure class="align-default" id="id3">
<img alt="https://www.nginx.com/wp-content/uploads/2021/07/Namespaces-cgroups_PID-namespaces.svg" src="https://www.nginx.com/wp-content/uploads/2021/07/Namespaces-cgroups_PID-namespaces.svg" />
<figcaption>
<p><span class="caption-text"><a class="reference external" href="https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work">(Image Source)</a></span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="creating-a-namespace">
<h4>Creating a Namespace<a class="headerlink" href="#creating-a-namespace" title="Link to this heading"></a></h4>
<p>With all that theory under our belts, let’s cement our understanding by actually
creating a new namespace. The Linux <a class="reference external" href="https://man7.org/linux/man-pages/man1/unshare.1.html">unshare</a>
command is a good place to start. The manual page indicates that it does exactly what we want:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>NAME
<span class="w">        </span>unshare<span class="w"> </span>-<span class="w"> </span>run<span class="w"> </span>program<span class="w"> </span><span class="k">in</span><span class="w"> </span>new<span class="w"> </span>name<span class="w"> </span>namespaces
</pre></div>
</div>
<p>Let’s see our user ID, group, and so on. While in PWD we have root privileges,
to use this command we don’t have to have root privileges:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>id
<span class="nv">uid</span><span class="o">=</span><span class="m">0</span><span class="o">(</span>root<span class="o">)</span><span class="w"> </span><span class="nv">gid</span><span class="o">=</span><span class="m">0</span><span class="o">(</span>root<span class="o">)</span><span class="w"> </span><span class="nv">groups</span><span class="o">=</span><span class="m">0</span><span class="o">(</span>root<span class="o">)</span>,1<span class="o">(</span>bin<span class="o">)</span>,2<span class="o">(</span>daemon<span class="o">)</span>,3<span class="o">(</span>sys<span class="o">)</span>,4<span class="o">(</span>adm<span class="o">)</span>,6<span class="o">(</span>disk<span class="o">)</span>,10<span class="o">(</span>wheel<span class="o">)</span>,11<span class="o">(</span>floppy<span class="o">)</span>,20<span class="o">(</span>dialout<span class="o">)</span>,26<span class="o">(</span>tape<span class="o">)</span>,27<span class="o">(</span>video<span class="o">)</span>
</pre></div>
</div>
<p>Now we run the following unshare command to create a new namespace with its own
user and PID namespaces. We map the root user to the new namespace (in other words,
we have root privilege within the new namespace), mount a new proc filesystem,
and fork my process (in this case, bash) in the newly created namespace.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>unshare<span class="w"> </span>--user<span class="w"> </span>--pid<span class="w"> </span>--map-root-user<span class="w"> </span>--mount-proc<span class="w"> </span>--fork<span class="w"> </span>bash
</pre></div>
</div>
<p>The command above accomplishes the same thing as issuing
the <code class="docutils literal notranslate"><span class="pre">&lt;runtime&gt;</span> <span class="pre">exec</span> <span class="pre">-it</span> <span class="pre">&lt;image&gt;</span> <span class="pre">/bin/bash</span></code> command in a running container.</p>
<div class="admonition-ps-process-status-command-in-pwd callout admonition" id="callout-1">
<p class="admonition-title"><code class="docutils literal notranslate"><span class="pre">ps</span></code> (process status) command in PWD</p>
<p>The avaiable <code class="docutils literal notranslate"><span class="pre">ps</span></code> command in PWD doesn’t show the output in a desired state.
We need to install it manually using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apk<span class="w"> </span>add<span class="w"> </span>--no-cache<span class="w"> </span>procps
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">ps</span> <span class="pre">-ef</span></code> command shows there are two processes running – <strong>bash</strong> and
the <strong>ps</strong> command itself – and the id command confirms that I’m <strong>root</strong> in the new
namespace (which is also indicated by the changed command prompt):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 15:46 pts/1    00:00:00 bash
root        21     1  0 15:56 pts/1    00:00:00 ps -ef

$ id
uid=0(root) gid=0(root) groups=0(root),65534(nobody),65534(nobody)
</pre></div>
</div>
<p>The crucial thing to notice is that I can see only the two processes in my namespace,
not any other processes running on the system. I am completely isolated within my own namespace.</p>
<p>(** Above exercise can be also done on the Vega.)</p>
</section>
<section id="looking-at-a-namespace-from-the-outside">
<h4>Looking at a Namespace from the Outside<a class="headerlink" href="#looking-at-a-namespace-from-the-outside" title="Link to this heading"></a></h4>
<p>Although we can’t see other processes from within the namespace, with the lsns (list namespaces)
command we can list all available namespaces and display information about them,
from the perspective of the parent namespace (outside the new namespace).</p>
<p>The output shows the namespaces – of types user, mnt, and pid – which correspond
to the arguments on the unshare command we ran above. From this external perspective,
each namespace is running as user $USER, not root, whereas inside the namespace processes run as root,
with access to all of the expected resources.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>lsns<span class="w"> </span>--output-all
<span class="w">        </span>NS<span class="w"> </span>TYPE<span class="w">   </span>PATH<span class="w">              </span>NPROCS<span class="w"> </span>PID<span class="w"> </span>PPID<span class="w"> </span>COMMAND<span class="w"> </span>UID<span class="w"> </span>USER<span class="w">    </span>NETNSID<span class="w"> </span>NSFS
<span class="m">4026531835</span><span class="w"> </span>cgroup<span class="w"> </span>/proc/1/ns/cgroup<span class="w">      </span><span class="m">2</span><span class="w">   </span><span class="m">1</span><span class="w">    </span><span class="m">0</span><span class="w"> </span>bash<span class="w">      </span><span class="m">0</span><span class="w"> </span>root
<span class="m">4026533087</span><span class="w"> </span>uts<span class="w">    </span>/proc/1/ns/uts<span class="w">         </span><span class="m">2</span><span class="w">   </span><span class="m">1</span><span class="w">    </span><span class="m">0</span><span class="w"> </span>bash<span class="w">      </span><span class="m">0</span><span class="w"> </span>root
<span class="m">4026533090</span><span class="w"> </span>ipc<span class="w">    </span>/proc/1/ns/ipc<span class="w">         </span><span class="m">2</span><span class="w">   </span><span class="m">1</span><span class="w">    </span><span class="m">0</span><span class="w"> </span>bash<span class="w">      </span><span class="m">0</span><span class="w"> </span>root
<span class="m">4026533093</span><span class="w"> </span>net<span class="w">    </span>/proc/1/ns/net<span class="w">         </span><span class="m">2</span><span class="w">   </span><span class="m">1</span><span class="w">    </span><span class="m">0</span><span class="w"> </span>bash<span class="w">      </span><span class="m">0</span><span class="w"> </span>root<span class="w"> </span>unassigned
<span class="m">4026537060</span><span class="w"> </span>pid<span class="w">    </span>/proc/1/ns/pid<span class="w">         </span><span class="m">2</span><span class="w">   </span><span class="m">1</span><span class="w">    </span><span class="m">0</span><span class="w"> </span>bash<span class="w">      </span><span class="m">0</span><span class="w"> </span>root
<span class="m">4026537071</span><span class="w"> </span>user<span class="w">   </span>/proc/1/ns/user<span class="w">        </span><span class="m">2</span><span class="w">   </span><span class="m">1</span><span class="w">    </span><span class="m">0</span><span class="w"> </span>bash<span class="w">      </span><span class="m">0</span><span class="w"> </span>root
<span class="m">4026537072</span><span class="w"> </span>mnt<span class="w">    </span>/proc/1/ns/mnt<span class="w">         </span><span class="m">2</span><span class="w">   </span><span class="m">1</span><span class="w">    </span><span class="m">0</span><span class="w"> </span>bash<span class="w">      </span><span class="m">0</span><span class="w"> </span>root
</pre></div>
</div>
</section>
<section id="namespaces-and-containers">
<h4>Namespaces and Containers<a class="headerlink" href="#namespaces-and-containers" title="Link to this heading"></a></h4>
<p>Namespaces are one of the technologies that containers are built on, used to enforce
segregation of resources. We’ve shown how to create namespaces manually, but container
runtimes like Docker makes things easier by creating namespaces on your behalf.</p>
</section>
</section>
<section id="what-are-cgroups">
<h3>What Are cgroups?<a class="headerlink" href="#what-are-cgroups" title="Link to this heading"></a></h3>
<p>A control group (cgroup) is a Linux kernel feature that limits, accounts for,
and isolates the resource usage (CPU, memory, disk I/O, network, and so on) of a collection of processes.</p>
<p>Cgroups provide the following features:</p>
<ul class="simple">
<li><p><strong>Resource limits</strong>: You can configure a cgroup to limit how much of a particular
resource (memory or CPU, for example) a process can use.</p></li>
<li><p><strong>Prioritization</strong>: You can control how much of a resource (CPU, disk, or network)
a process can use compared to processes in another cgroup when there is resource contention.</p></li>
<li><p><strong>Accounting</strong>: Resource limits are monitored and reported at the cgroup level.</p></li>
<li><p><strong>Control</strong>: You can change the status (frozen, stopped, or restarted) of all
processes in a cgroup with a single command.</p></li>
</ul>
<p>So basically you use cgroups to control how much of a given key resource (CPU, memory, network, and disk I/O)
can be accessed or used by a process or set of processes. Cgroups are a key component
of containers because there are often multiple processes running in a container
that you need to control together. In a Kubernetes environment, cgroups can be
used to implement resource requests and limits and corresponding QoS classes at the pod level.</p>
<p>The following diagram illustrates how when you allocate a particular percentage
of available system resources to a cgroup (in this case <strong>cgroup‑1</strong>),
he remaining percentage is available to other cgroups (and individual processes) on the system.</p>
<section id="cgroup-versions">
<h4>Cgroup Versions<a class="headerlink" href="#cgroup-versions" title="Link to this heading"></a></h4>
<p>According to <a class="reference external" href="https://en.wikipedia.org/wiki/Cgroups">Wikipedia</a>, the first version
of cgroups was merged into the Linux kernel mainline in late 2007 or early 2008,
and “the documentation of cgroups‑v2 first appeared in [the] Linux kernel … [in] 2016”.
Among the many changes in version 2, the big ones are a much simplified tree architecture,
new features and interfaces in the cgroup hierarchy, and better
accommodation of “rootless” containers (with non‑zero UIDs).</p>
<figure class="align-default">
<img alt="https://www.nginx.com/wp-content/uploads/2021/07/Namespaces-cgroups_resource-limits.svg" src="https://www.nginx.com/wp-content/uploads/2021/07/Namespaces-cgroups_resource-limits.svg" />
</figure>
<p><a class="reference external" href="https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work">(Image Source)</a></p>
</section>
<section id="creating-a-cgroup">
<h4>Creating a cgroup<a class="headerlink" href="#creating-a-cgroup" title="Link to this heading"></a></h4>
<p>The following command creates a v1 cgroup (you can tell by pathname format)
called foo and sets the memory limit for it to 50,000,000 bytes (50 MB).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/sys/fs/cgroup/memory/foo
$<span class="w"> </span>sudo<span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="m">50000000</span><span class="w"> </span>&gt;<span class="w"> </span>/sys/fs/cgroup/memory/foo/memory.limit_in_bytes
</pre></div>
</div>
<p>If we know check the mem limits, we get</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>cat<span class="w"> </span>/sys/fs/cgroup/memory/foo/memory.limit_in_bytes
<span class="m">49999872</span>
</pre></div>
</div>
<p>Now, let’s create a test bash file to check cgroup functionality. A simple example
of such shell is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>vim<span class="w"> </span>test.sh

<span class="c1">#!/bin/sh</span>
<span class="k">while</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;hellp world&quot;</span>
<span class="w">    </span>sleep<span class="w"> </span><span class="m">60</span>
<span class="k">done</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">test.sh</span></code> is a shell script, which prints a message to the screen
and then sleeps for 60 seconds. It is fine for our purposes because it is in
an infinite loop.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>sh<span class="w"> </span>./test.sh<span class="w"> </span><span class="p">&amp;</span>
<span class="o">[</span><span class="m">1</span><span class="o">]</span><span class="w"> </span><span class="m">31344</span>
hello<span class="w"> </span>world
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">test.sh</span></code> is started in the background and its PID is reported as 31344.
The script produces its output and then we assign the process to the cgroup
by piping its PID into the cgroup file <code class="docutils literal notranslate"><span class="pre">/sys/fs/cgroup/memory/foo/cgroup.procs</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>sudo<span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="m">31344</span><span class="w"> </span>&gt;<span class="w"> </span>/sys/fs/cgroup/memory/foo/cgroup.procs
</pre></div>
</div>
<p>To validate that my process is in fact subject to the memory limits that we defined
for cgroup foo, we run the following ps command. The -o cgroup flag displays
the cgroups to which the specified process (31344) belongs. The output confirms
that its memory cgroup is foo.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ps<span class="w"> </span>-o<span class="w"> </span>cgroup<span class="w"> </span><span class="m">31344</span>
CGROUP
<span class="m">11</span>:memory:/docker/874edaaa7ef8e61e283b438077e82c3435e53c5bedc91ba63ea84eca0993678f/foo,10:blkio:/docker/874eda
</pre></div>
</div>
<p>We can also check the amount of memory currently <code class="docutils literal notranslate"><span class="pre">test.sh</span></code> is using with the command below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>sudo<span class="w"> </span>cat<span class="w"> </span>/sys/fs/cgroup/memory/foo/memory.usage_in_bytes
<span class="m">1712128</span>
</pre></div>
</div>
<p>Namespaces and cgroups are the building blocks for containers and modern applications.
Having an understanding of how they work is important as we refactor applications
to more modern architectures.</p>
<p>Namespaces provide isolation of system resources, and cgroups allow for fine‑grained
control and enforcement of limits for those resources.</p>
<p>Containers are not the only way that you can use namespaces and cgroups.
Namespaces and cgroup interfaces are built into the Linux kernel, which means that other applications can use them to provide separation and resource constraints.</p>
</section>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-quick-reference"></span><section id="quick-reference">
<h2>Quick Reference<a class="headerlink" href="#quick-reference" title="Link to this heading"></a></h2>
</section>
<span id="document-guide"></span><section id="instructor-s-guide">
<h2>Instructor’s guide<a class="headerlink" href="#instructor-s-guide" title="Link to this heading"></a></h2>
</section>
</div>
<section id="who-is-the-course-for">
<span id="learner-personas"></span><h2>Who is the course for?<a class="headerlink" href="#who-is-the-course-for" title="Link to this heading"></a></h2>
</section>
<section id="about-the-course">
<h2>About the course<a class="headerlink" href="#about-the-course" title="Link to this heading"></a></h2>
<p>This lesson material is developed by the <a class="reference external" href="https://enccs.se/">EuroCC National Competence Center
Sweden (ENCCS)</a> and taught in ENCCS workshops. It aims
at researchers and developers who have experience working with AI and wish to train their applications on supercomputers.
The lesson material is licensed under <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0</a> and can be reused in any form
(with appropriate credit) in other courses and workshops.
Instructors who wish to teach this lesson can refer to the <a class="reference internal" href="#document-guide"><span class="doc">Instructor’s guide</span></a> for
practical advice.</p>
</section>
<section id="see-also">
<h2>See also<a class="headerlink" href="#see-also" title="Link to this heading"></a></h2>
<p>Docker provides plenty of educational materials for users. Therefore, checking <a class="reference external" href="https://docs.docker.com/get-started/">Docker official website</a>
is highly recommended. The same can be stated about <a class="reference external" href="https://sylabs.io/guides/latest/user-guide/">Singularity</a>
, where one can find many compelling examples with relevant details.</p>
<p>TensorFlow and Horovorod documentation are also good sources of learning about commands
and their proper use.</p>
</section>
<section id="credits">
<h2>Credits<a class="headerlink" href="#credits" title="Link to this heading"></a></h2>
<p>The lesson file structure and browsing layout is inspired by and
derived from <a class="reference external" href="https://github.com/coderefinery/sphinx-lesson">work</a>
by <a class="reference external" href="https://coderefinery.org/">CodeRefinery</a> licensed under the <a class="reference external" href="http://opensource.org/licenses/mit-license.html">MIT
license</a>. We have
copied and adapted most of their license text.</p>
<dl class="simple">
<dt>Materials from the below references have been used in various parts of this course.</dt><dd><ul class="simple">
<li><p>The Carpentries lesson on <a class="reference external" href="https://epcced.github.io/2020-12-08-Containers-Online/">“Reproducible Computational Environments Using Containers: Introduction to Docker and Singularity”</a></p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/guide/distributed_training">TensorFlow documentation</a></p></li>
<li><p><a class="reference external" href="https://horovod.readthedocs.io/en/stable/">Horovod documentation</a></p></li>
</ul>
</dd>
</dl>
<section id="instructional-material">
<h3>Instructional Material<a class="headerlink" href="#instructional-material" title="Link to this heading"></a></h3>
<p>This instructional material is made available under the <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">Creative
Commons Attribution license (CC-BY-4.0)</a>.  The following is a
human-readable summary of (and not a substitute for) the <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/legalcode">full legal
text of the CC-BY-4.0 license</a>.  You are
free to:</p>
<ul class="simple">
<li><p><strong>share</strong> - copy and redistribute the material in any medium or format</p></li>
<li><p><strong>adapt</strong> - remix, transform, and build upon the material for any purpose,
even commercially.</p></li>
</ul>
<p>The licensor cannot revoke these freedoms as long as you follow these
license terms:</p>
<ul class="simple">
<li><p><strong>Attribution</strong> - You must give appropriate credit (mentioning that your work
is derived from work that is Copyright (c) Hossein Ehteshami and individual contributors and, where practical, linking
to <a class="reference external" href="https://enccs.se">https://enccs.se</a>), provide a <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">link to the license</a>, and indicate if changes were
made. You may do so in any reasonable manner, but not in any way that suggests
the licensor endorses you or your use.</p></li>
<li><p><strong>No additional restrictions</strong> - You may not apply legal terms or
technological measures that legally restrict others from doing anything the
license permits.</p></li>
</ul>
<p>With the understanding that:</p>
<ul class="simple">
<li><p>You do not have to comply with the license for elements of the material in
the public domain or where your use is permitted by an applicable exception
or limitation.</p></li>
<li><p>No warranties are given. The license may not give you all of the permissions
necessary for your intended use. For example, other rights such as
publicity, privacy, or moral rights may limit how you use the material.</p></li>
</ul>
</section>
<section id="software">
<h3>Software<a class="headerlink" href="#software" title="Link to this heading"></a></h3>
<p>Except where otherwise noted, the example programs and other software
provided with this repository are made available under the <a class="reference external" href="http://opensource.org/">OSI</a>-approved
<a class="reference external" href="https://opensource.org/licenses/mit-license.html">MIT license</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, ENCCS, and individual contributors..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>